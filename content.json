{"meta":{"title":"努力，奋斗","subtitle":"记录学习","description":null,"author":"王尚","url":"https://shang.at","root":"/"},"pages":[{"title":"About","date":"2019-03-24T01:33:47.232Z","updated":"2019-03-24T01:33:47.232Z","comments":true,"path":"about/index.html","permalink":"https://shang.at/about/index.html","excerpt":"","text":"大数据工程师一枚 邮箱：2823147532@qq.com 此博客仅为个人学习数据结构和算法的一个笔记，就是想督促自己坚持学习。笔记内容主要来源于其他的博客和读完博客后的自己的一点思考"},{"title":"Categories","date":"2020-06-29T17:14:30.000Z","updated":"2020-06-29T17:15:11.496Z","comments":true,"path":"categories/index.html","permalink":"https://shang.at/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2020-06-29T17:14:30.000Z","updated":"2020-06-29T17:16:06.172Z","comments":true,"path":"tags/index.html","permalink":"https://shang.at/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Python-collections","slug":"Python-collections","date":"2021-05-19T02:26:45.000Z","updated":"2021-05-19T02:27:10.936Z","comments":true,"path":"post/Python-collections/","link":"","permalink":"https://shang.at/post/Python-collections/","excerpt":"简介：","text":"简介： https://docs.python.org/zh-cn/3.8/library/collections.html?highlight=counter#module-collections","categories":[],"tags":[]},{"title":"Yaml","slug":"Yaml","date":"2021-05-19T02:23:35.000Z","updated":"2021-05-19T02:25:25.619Z","comments":true,"path":"post/Yaml/","link":"","permalink":"https://shang.at/post/Yaml/","excerpt":"简介：","text":"简介： https://einverne.github.io/post/2015/08/yaml.html https://www.jianshu.com/p/173c734b7281 python操作yaml：https://www.jianshu.com/p/eaa1bf01b3a6","categories":[],"tags":[]},{"title":"Scala-Any,AnyRef,AnyVal","slug":"Scala-Any-AnyRef-AnyVal","date":"2021-05-19T02:18:25.000Z","updated":"2021-05-27T07:36:09.299Z","comments":true,"path":"post/Scala-Any-AnyRef-AnyVal/","link":"","permalink":"https://shang.at/post/Scala-Any-AnyRef-AnyVal/","excerpt":"简介：","text":"简介： https://my.oschina.net/cloudcoder/blog/856334 scala中的None和null","categories":[],"tags":[]},{"title":"Spark-获取DataFrame的大小","slug":"Spark-获取DataFrame的大小","date":"2021-05-19T02:14:29.000Z","updated":"2021-05-19T02:17:28.631Z","comments":true,"path":"post/Spark-获取DataFrame的大小/","link":"","permalink":"https://shang.at/post/Spark-获取DataFrame的大小/","excerpt":"简介：","text":"简介： https://stackoverflow.com/questions/35008123/how-to-find-spark-rdd-dataframe-size https://stackoverflow.com/questions/49492463/compute-size-of-spark-dataframe-sizeestimator-gives-unexpected-results https://stackoverflow.com/questions/48064638/how-to-find-the-size-of-an-object-in-scala Sizeof in Scala：https://www.yannmoisan.com/sizeof-in-scala.html 写的非常好","categories":[],"tags":[]},{"title":"Spark-性能优化","slug":"Spark-性能优化","date":"2021-05-19T02:11:28.000Z","updated":"2021-05-19T02:11:34.957Z","comments":true,"path":"post/Spark-性能优化/","link":"","permalink":"https://shang.at/post/Spark-性能优化/","excerpt":"简介：","text":"简介： https://www.cnblogs.com/dflmg/p/10430181.html","categories":[],"tags":[]},{"title":"Flink","slug":"Flink学习","date":"2021-05-19T02:09:21.000Z","updated":"2021-05-21T09:43:34.369Z","comments":true,"path":"post/Flink学习/","link":"","permalink":"https://shang.at/post/Flink学习/","excerpt":"简介：","text":"简介： flink中事件时间的理解两种不同的提交job的方法execute和executeAsync的区别 两种不同的Function Function：只能拿到需要处理的数据 RichFunction：不止能拿到需要处理的数据，同时也能拿到系统的运行时上下文对象 Flink的内存管理","categories":[],"tags":[]},{"title":"Scala-常用数据结构","slug":"Scala-常用数据结构","date":"2021-05-19T02:07:13.000Z","updated":"2021-05-26T16:42:47.657Z","comments":true,"path":"post/Scala-常用数据结构/","link":"","permalink":"https://shang.at/post/Scala-常用数据结构/","excerpt":"简介：","text":"简介： map：https://www.cnblogs.com/suanec/p/5760446.html scala 时间，时间格式转换：https://blog.csdn.net/An1090239782/article/details/100590390 官网的解读","categories":[],"tags":[]},{"title":"Scala-并行集合","slug":"Scala-并行集合","date":"2021-05-19T02:03:27.000Z","updated":"2021-05-19T02:04:01.918Z","comments":true,"path":"post/Scala-并行集合/","link":"","permalink":"https://shang.at/post/Scala-并行集合/","excerpt":"简介：","text":"简介： https://www.zhihu.com/question/23273115?sort=created https://stackoverflow.com/questions/17178201/parallel-iterator-in-scala https://github.com/tim-group/iterata","categories":[],"tags":[]},{"title":"Scala-并发编程","slug":"Scala-并发编程","date":"2021-05-19T02:02:22.000Z","updated":"2021-05-19T02:03:02.155Z","comments":true,"path":"post/Scala-并发编程/","link":"","permalink":"https://shang.at/post/Scala-并发编程/","excerpt":"简介：","text":"简介： https://liam-blog.ml/2019/07/14/Scala-Concurrency-in-Practice-1/ https://stackoverflow.com/questions/34877487/how-to-use-synchronized-in-scala","categories":[],"tags":[]},{"title":"工具使用-markdown文档","slug":"工具使用-markdown文档","date":"2021-05-12T02:41:39.000Z","updated":"2021-05-31T03:29:44.120Z","comments":true,"path":"post/工具使用-markdown文档/","link":"","permalink":"https://shang.at/post/工具使用-markdown文档/","excerpt":"简介：","text":"简介： 比较有用的使用方法折叠 Program Structure Note that it is possible to cascade one set of time windows after another, so long as the timeframes are compatible (the second set of windows needs to have a duration that is a multiple of the first set). So you can have a initial set of hour-long windows that is keyed by the driverId and use this to create a stream of (endOfHourTimestamp, driverId, totalTips), and then follow this with another hour-long window (this window is not keyed) that finds the record from the first window with the maximum totalTips. 分栏 分栏测试 左侧内容 右侧内容","categories":[],"tags":[]},{"title":"数仓项目及Spark分享","slug":"数仓项目及Spark分享","date":"2021-05-06T01:48:39.000Z","updated":"2021-06-03T07:55:58.085Z","comments":true,"path":"post/数仓项目及Spark分享/","link":"","permalink":"https://shang.at/post/数仓项目及Spark分享/","excerpt":"简介：","text":"简介： 【讲】项目介绍设计模式项目架构 - 模板方法的设计模式 - 框架的通用设计模式 - 主体流程进行限制，暴露必要的方法给用户实现，用户只需要关注具体的业务逻辑，无须关心其他的执行细节 数仓项目几个角色抽象 - APP、Task、Target、Datasource它们的关系 - 就是执行的具体细节 依赖管理：解析和执行 执行流程：业务逻辑 -&gt; （合并小文件）[判断是否分区] write to parquet 项目优化点 利用spark FIFO的队列模型提高资源利用率(多线程并发提交job)，避免了一次仅提交一个Job带来的 资源浪费：如果是一次仅提交一个Job，那么就目前的项目架构来说，相当于人为的将所有任务都串行起来了，一个Job的执行可能只需要极少的资源，但是这种模式又没有其他的Job可以运行，那么大部分时间集群的资源是处于空闲状态的，因此就会造成一种资源的浪费。 时间消耗：原因是如果是一次仅提交一个Job，那么每次提交Job，都要经历spark集群启动的过程，这一过程是很耗时的，在目前的情况中，spark集群启动的过程可能比一个Job运行的时间还要长。 潜在的问题：一个Job的失败会导致后续所有未执行Job的失败，但是这个问题对于批处理来说，并不是一个很大的问题，因为可以直接重跑一次批处理就可以了，并不会造成太大的问题。但是这种情况的出现还是会对其下游的任务造成一定的影响，会延迟下游任务的执行。 造成这类问题的主要原因有两种： ​ 代码的bug：代码上线前应该充分测试，上线之后立即执行一次确保没有问题 ​ 系统问题：由于数据越来越大，可能有些问题会逐渐的显露出来，比如一些内存溢出问题(下面就会介绍一些类似的问题：比如autoBroadcast造成的OOM以及coalesce造成的OOM) Flink如何实现并发提交任务 - application mode &amp; executeAsync() 非分区表小文件自动合并 - 根据hive matestore记录数据大小进行估算最后的文件数 - 最终的文件数实际上就是最后一个stage的分区数 12&gt; DESCRIBE EXTENDED databaseName.tableName&gt; 缓存管理 - 打算优化的点 - 不同的表可能会依赖了同样的基础表，希望是可以只读取一次原始表，后续再次使用该表则使用cache的数据 - 缓存管理、缓存释放 - 在目前不改动业务代码的情况下很难实现 - 进一步思考中 【讲】任务调度spark的Scheduler官方的解释 关于spark任务调度，实际上分为两层（以下针对基于yarn部署的spark集群）。第一层是yarn的资源调度层，第二层是spark的应用调度层，因此分为以下两个方面介绍： Scheduling Across Applications应用级别的调度 该级别的调度是基于yarn这样的一个统一的资源调度平台进行的，所有基于yarn运行的应用都要遵循同样的调度策略，详细理解Yarn Scheduler，可以阅读一下这篇博客。这里只是简单的介绍一下： 假设我们通过spark-submit以cluster模式提交的spark application到yarn集群，实际上就是向RM申请了一系列的资源(这里会遵循yarn的资源调度规则)，如果任务提交成功，那么随后会在yarn集群中启动一个spark的小集群，其中就包含了我们所熟悉的Driver、Executors，Driver端运行的就是我们提交的jar的main方法，后续的任务调度就是spark自己调度的了。（这里不涉及过多细节，因为细节实在太多，关于yarn和spark on yarn的更多细节，可以阅读下这篇博客）。 Scheduling Within an ApplicationJob级别的调度：FAIR、FIFO 这里是我们要介绍的重点。 在一个spark的Application中是允许并行运行多个Job的，前提是需要在不同的线程中提交的。这里所说的Job是指spark中的一个action算子(比如collect，save等)。 什么是Job？ FIFO这是spark内部调度Job的默认策略，即：第一个被提交的Job会有使用集群全部资源的优先权，分为两种情况： 如果当前Job把所有集群资源都占用了，那么后续提交的Job只能等待 如果当前Job仅仅占用了集群的一部分资源，集群还有空闲资源，那么后续提交的任务可以立即执行。无论一个Application中并发提交了多少个Job，都是这样的方式调度。 务必仅仅在scala/java语言编程中使用并发Job，不建议在pySpark中使用：Concurrent Jobs in PySpark Fair Sharing这是在spark0.8版本以后上线的特性，主要用于多租户使用同一个集群的情况，使得多个租户都可以享用到集群的资源，即使前面有一个很大的Job在执行，后续提交的任务也能够分到相应的资源去运行，保证良好的响应时间 如果想使用Fair Scheduler，需要作如下的配置才行： 123val conf = new SparkConf().setMaster(...).setAppName(...)conf.set(\"spark.scheduler.mode\", \"FAIR\")val sc = new SparkContext(conf) 除此以外，还需要一些系统配置：Fair Scheduler Pools，这里有些涉及到运维相关的了，与我们实际开发关系不是很大。 我们的Scheduler依赖管理我们项目中的DAG - 以一种比较简单数据结构实现了任务的依赖关系，下图是我们使用的数据结构的示意图 伪代码如下： 123456789101112class BaseTask &#123; def priority: Int = 100 private val prev: mu.HashSet[BaseTask] = mu.HashSet() def predecessors: Set[BaseTask] = prev.toSet private val next: mu.HashSet[BaseTask] = mu.HashSet() def successors: Set[BaseTask] = next.toSet def -&gt;(other: BaseTask): BaseTask = &#123; next += other other.prev += this other &#125;&#125; 解读：每个Task有前驱结点Set集合(前驱依赖节点)和后继节点Set集合(下游节点) 任务调度器： Scheduler维护了以下四个队列： 入度为0的任务节点进入candidates队列，candidates是一个优先级队列，即高优先级的任务会被优先执行 normal：初始化任务时所有任务都在这 running：正在运行的任务 failed：执行失败的任务。 completed：执行完成的任务 scheduler会设置一个并行度，也就是线程数 当然这只是控制了我们在Driver启动的线程数，它表示我们可以一次向spark集群提交多少并发的任务 至于集群能够使用多少资源去执行，那是集群的资源调度，它同时受到能够分配给同一个SparkSession的资源配置(系统级的配置，当前app申请的资源配置)和当前集群资源使用情况的影响 这里的并行度设置的思路： 1、在当前的应用场景下，我们的Driver端应该属于IO密集型的应用(Executor端属于计算密集型应用)，所以Driver端的线程大多数时间不是花在CPU运算上面，而是花在了等待IO(主要是网络IO)上，这时我们可以尽可能多的启动一些线程，去提交尽可能多(当然不会非常多，太多就真的是浪费资源了)的Job给spark，这对于Driver端并不会有很大的压力，因此我们根据当前可提交Job数设置了一个并行度，考虑到一些特殊情况(当前任务结束后，会释放多个下游节点，所以至少10个并行度，同时考虑一次性提交太多任务，又因为集群资源并不能一次性满足也减少driver的一点资源消耗，并行度上限设为20)，最后确定的并行度范围是[10,20] 2、根据上面的讲述，我们采用的是spark默认的FIFO的调度策略，所以就算我们一次性提交多个JOB，也不会有什么问题，spark会根据当前集群资源自动的启动相关的JOB运行，所以我们可以一次性提交多个JOB给spark。实际上这样能够更加充分的利用集群资源(一旦有job完成，spark立马就可以从队列中取出来一个新的job执行)。 调度步骤： 在主线程中，只管去问scheduler要当前可以执行的任务，只要有任务返回就不断的询问，直到scheduler返回None，那就有两个问题需要解决： 1、我们设置过并行提交任务数了，获取的任务数大于我们设置的任务数怎么办? 2、这里任务的依赖是怎么解决的? 3、优先级如何处理？ 4、如果一个任务执行失败了怎么办? 针对以上四个问题的解答如下： 1、得到多出来的任务会被放到线程池的队列中，等到之前的任务执行完毕，将线程归还之后，会自动调度执行。这是线程池的策略 2、我们之前说过，是将入度为0任务节点取出放到candidates队列，当一个任务执行完毕，那么将其从running队列中删除，加入到completed队列中，同时去更新其后驱任务节点入度减一，并检查其入度是否为0，如果为0，则将其放入candidates队列。 如果当前询问时candidates为空了，并不能说明已经没有候选任务了，还可能是由于前驱任务都没有执行完导致还没有新任务被放入，此时会block住(candidates.wait())，直到有某些任务执行完毕时提醒(candidates.notify())主线程可以继续探索candidates(再次判断candidates.nonEmpty)，如果此时candidates中又有了新的任务，那么将任务取出返回(从normal队列中删除，加入到running队列) 3、我们说过candidates是一个优先级队列，它会根据放入的任务的优先级进行排序，使得我们每次从其中取的任务是优先级最高的任务。从candidates中取到的任务放到线程池中是一个串行的执行，即它会根据送进来的任务的先后顺序依次执行(FIFO) 4、这里涉及到一个失败任务应该如何处理的问题 一旦失败，将整个app停止 某一个任务失败，只将受到其影响的后驱任务标记为失败放到failed队列。其他任务不受影响继续执行 明显后者更好 并发控制多线程，并发管理，同步处理(最重要的一部分，否则会发生难以debug的并发问题) 同步处理主要就是控制并发的线程对共享变量(内存)的访问与操作，最常用的手段就是加锁，确保同一时刻只有一个线程能够访问共享变量，同时该线程对此共享变量的改变，当该线程释放锁之后，改变对其他线程是立即可见的(当然这是编程语言自动支持的特性)。 具体基于java语言和scala语言的并发控制就不多说了，这里的内容非常多。 【讲】spark常见的异常处理内存异常spark的内存管理Driver的内存Driver的内存主要有两块 JVM进程运行所需要的内存，比如DAG的生成、网络通信、任务调度 数据所占用的内存，主要就是用户手动collect的数据，以及spark内部自己优化collect到driver的数据，比如autoBroadcast。后面会介绍这中优化有时候会导致Driver的OOM Executor内存spark的内存管理RESERVED_SYSTEM_MEMORY_BYTES 300M - 这是系统预留的支持系统运行的内存空间 systemMemory - Runtime.getRuntime.maxMemory - 默认是JVM进程启动的时候通过-Xmx指定的值 minSystemMemory = reservedMemory * 1.5 当systemMemory&lt;minSystemMemory时，Driver是启动不起来的，对应的启动参数是 --driver-memory，对应的是spark.driver.memory，默认是1G 参数：spark.executor.memory executorMemory 当其小于minSystemMemory时，executor是启动不起来的，对应的启动参数为--executor-memory，默认是1G usableMemory = systemMemory - reservedMemory 参数：spark.memory.fraction 0.6 maxMemory = usableMemory * memoryFraction 参数：spark.memory.storageFraction 0.5 onHeapStorage = maxMemory * spark.memory.storageFraction onHeapExecution = maxMemory - onHeapStorage 举个例子：假设设置spark.driver.memory=1G=1024M 那么Driver端实际上能够用于execution和storage的内存只有(默认情况下)：(1024-300)*0.6=434.4M 以上讲的是onHeap的部分，在实际spark中，还可以使用offHeap内存来增加执行的效率 TODO:offHeap这块的认知还需要打磨 offHeap和onHeap的区别在于，在java中onHeap是java管理的内存空间，会触发GC，会有java Object的额外的内存消耗(比如说对象头，再小的对象，对象头也会占用12Bytes)，而offHeap是直接内存空间，offHeap内存储的就是字节数组，纯数据，直接对物理内存的操作，效率也会提高 Spark参数spark.executor.memoryOverhead与spark.memory.offHeap.size的区别 Spark内存空间管理 1org.apache.spark.SparkConf: The configuration key &apos;spark.yarn.executor.memoryOverhead&apos; has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key &apos;spark.executor.memoryOverhead&apos; instead. 静态内存管理 - StaticMemoryManager - 已经不是默认选项了 当设置了spark.memory.storageFraction，那么用户execute的内存和用于storage的内存大小是限定死的，他们不会根据实际内存使用情况动态调整。即系统很可能因为某一部分的内存不足而异常退出 动态内存管理 - UnifiedMemoryManager - 是较新版本的默认选项 比如说当没有内存被使用为storage，或很少被使用为storage，同时execute会占用很多内存的时候，这个时候系统会自动调整execution和storage的比例。spark.memory.storageFraction只是一个初始比例 内存利用会更加充分，我们也基本不用手动设置相关的参数，使用默认就好 常见异常driver内存异常最常导致driver内存溢出的问题是collect算子导致大量数据涌向driver节点，将导致内存溢出 其中有一些是用户的误操作，比如在代码中对数据量比较大的df直接进行collect或toPandas(本质上就是collect)操作。 建议：在不知情的情况下慎用collect类型的操作，否则可能会发生毁灭性的事情😯 还有一些是spark内部优化导致的，比如下面将要介绍的autoBroadcast。 autoBroadcast导致的Driver内存溢出官方解读 spark会对小表进行优化，使用广播变量来加速数据的join操作。我们知道普通的join操作必然会导致shuffle操作，这样会导致集群内大量的网络数据传输，导致效率急剧下降，而且会占用网络带宽。但是如果发生join的表较小，那么spark会自动检测出来并且进行广播处理，从而执行mapjoin，可以极大地减小网络带宽的占用。 但是autoBroadcast的默认工作原理是将小表的数据collect到driver，然后使用broadcast算子进行广播操作，内存增加的计算公式为：spark.sql.autoBroadcastJoinThreshold * the number of broadcast table * 2，当广播任务比较频繁的时候，Driver有可能因为OOM而异常退出 解决方案： 1、调整spark.sql.autoBroadcastJoinThreshold，spark会将大小小于该值的小表自动广播，调小该值，可以让spark不广播某些表。但是属于指标不治本 或者直接设置为-1关闭autoBroadcast功能 2、调整driver内存，增加driver内存，某些场景下可用，可能换到另一个场景就不可用了，因此需要根据不同的业务场景调节driver内存以满足autoBroadcast的要求。 3、设置spark.sql.bigdata.useExecutorBroadcast为true，使用Executor广播，将表数据缓存在Executor中，而不是放在Driver之中，减少Spark Driver内存的压力。driver不用多做调整 经过spark2.4.2测试，完全没有效果！！！ coalesce导致的内存溢出Spark-error.log https://stackoverflow.com/questions/31610971/spark-repartition-vs-coalesce/65038141#65038141 https://stackoverflow.com/questions/31674530/write-single-csv-file-using-spark-csv https://stackoverflow.com/questions/38961251/java-lang-outofmemoryerror-unable-to-acquire-100-bytes-of-memory-got-0 test coalesce(1)、coalesce(2)、 repartition(1) join之后执行coalesce、filter之后执行coalesce都会 如果coalesce之前没有shuffle的操作，直接调用coalesce(1)，可能会导致Executor OOM，因为coalesce操作不会触发计算操作，如果它之前没有shuffle操作，spark不会执行任何的计算，直接将原始数据加载到一个Executor上，才会开始执行计算，这样的话，实际上Executor上就有可能发生OOM 比如： 123For instance: load().map(…).filter(…).coalesce(1).save()translates to: load().coalesce(1).map(…).filter(…).save() Spark - repartition() vs coalesce()：https://stackoverflow.com/questions/31610971/spark-repartition-vs-coalesce/65038141#65038141 数据倾斜主要体现在绝大多数task执行得都非常快，个别task执行很慢，拖慢整个任务的执行进程，甚至可能因为某个task处理的数据量过大而爆出OOM错误。 shuffle操作中需要将各个节点上相同的key拉取到某一个节点上的一个task处理，如果某个key对应的数据量特别大，就会发生数据倾斜。 4.3.1 分析数据分布 如果是Spark SQL中的group by、join语句导致的数据倾斜，可以使用SQL分析执行SQL中的表的key分布情况；如果是Spark RDD执行shuffle算子导致的数据倾斜，可以在Spark作业中加入分析Key分布的代码，使用countByKey()统计各个key对应的记录数。 4.3.2 数据倾斜的解决方案 这里参考美团技术博客中给出的几个方案。 1）针对hive表中的数据倾斜，可以尝试通过hive进行数据预处理，如按照key进行聚合，或是和其他表join，Spark作业中直接使用预处理后的数据。 2）如果发现导致倾斜的key就几个，而且对计算本身的影响不大，可以考虑过滤掉少数导致倾斜的key 3）设置参数spark.sql.shuffle.partitions，提高shuffle操作的并行度，增加shuffle read task的数量，降低每个task处理的数据量 4）针对RDD执行reduceByKey等聚合类算子或是在Spark SQL中使用group by语句时，可以考虑两阶段聚合方案，即局部聚合+全局聚合。第一阶段局部聚合，先给每个key打上一个随机数，接着对打上随机数的数据执行reduceByKey等聚合操作，然后将各个key的前缀去掉。第二阶段全局聚合即正常的聚合操作。 5）针对两个数据量都比较大的RDD/hive表进行join的情况，如果其中一个RDD/hive表的少数key对应的数据量过大，另一个比较均匀时，可以先分析数据，将数据量过大的几个key统计并拆分出来形成一个单独的RDD，得到的两个RDD/hive表分别和另一个RDD/hive表做join，其中key对应数据量较大的那个要进行key值随机数打散处理，另一个无数据倾斜的RDD/hive表要1对n膨胀扩容n倍，确保随机化后key值仍然有效。 6）针对join操作的RDD中有大量的key导致数据倾斜，对有数据倾斜的整个RDD的key值做随机打散处理，对另一个正常的RDD进行1对n膨胀扩容，每条数据都依次打上0~n的前缀。处理完后再执行join操作 spark的常见优化正确使用数据缓存cache：默认缓存级别是StorageLevel.MEMORY_AND_DISK，不能更改 persist：可以手动指定缓存级别，直接调用persist()，效果和cache()一样 mapjoin手动broadcastmapjoin &amp; autoBroadcast join优化，spark会默认将小表广播，按照如下的参数设置，满足条件就会广播 参数优化： spark.sql.broadcastTimeout：broadcast的加大超时的时间限制 spark.sql.autoBroadcastJoinThreshold：默认是10M，大小低于该参数设置的阈值时，会被广播，但是默认的BroadCastJoin会将小表的内容，全部收集到Driver中，导致Driver压力变大 spark.sql.bigdata.useExecutorBroadcast：设置为true时，使用Executor广播，将表数据缓存在Executor中，而不是放在Driver之中，减少Spark Driver内存的压力。 在join中使用or连接关键字，会导致笛卡尔积的产生CartesianProduct(当join多个表)|BroadcastNestedLoopJoin(只join一个table)，故不建议这样做。 本地化级别仅做了解，目前项目中没有发现很明显的这方面的问题导致的效率低下 PROCESS_LOCAL： task要计算的数据在本进程（Executor）的内存中。 NODE_LOCAL： ·task所计算的数据在本节点所在的磁盘上 task所计算的数据在本节点其他Executor进程的内存中。 NO_PREF task所计算的数据在关系型数据库中，如mysql。 RACK_LOCAL task所计算的数据在同机架的不同节点的磁盘或者Executor进程的内存中 ANY 跨机架。 Spark中任务调度时，TaskScheduler在分发之前需要依据数据的位置来分发，最好将task分发到数据所在的节点上，如果TaskScheduler分发的task在默认3s依然无法执行的话，TaskScheduler会重新发送这个task到相同的Executor中去执行，会重试5次，如果依然无法执行，那么TaskScheduler会降低一级数据本地化的级别再次发送task。 如上图中，会先尝试1,PROCESS_LOCAL数据本地化级别，如果重试5次每次等待3s,会默认这个Executor计算资源满了，那么会降低一级数据本地化级别到2，NODE_LOCAL,如果还是重试5次每次等待3s还是失败，那么还是会降低一级数据本地化级别到3，RACK_LOCAL。这样数据就会有网络传输，降低了执行效率。 ① 如何提高数据本地化的级别？ 可以增加每次发送task的等待时间（默认都是3s），将3s倍数调大， 结合WEBUI来调节： • spark.locality.wait • spark.locality.wait.process • spark.locality.wait.node • spark.locality.wait.rack 注意：等待时间不能调大很大，调整数据本地化的级别不要本末倒置，虽然每一个task的本地化级别是最高了，但整个Application的执行时间反而加长。 ② 如何查看数据本地化的级别？ 通过日志或者WEBUI 增量计算代替全量计算如何使用增量表来优化每次全量数据的跑批 - 拿我们的ads层的一些表为例来说 什么样的数据适合使用增量计算来代替全量计算呢？计算结果不会因为时间跨度的变化而改变，固定的时间范围的数据计算结果不会受到后续数据的影响 这个就需要具体问题具体分析了，通过增量计算来代替全量计算可以极大的增大执行效率 其他代码级别的优化 三类筛选 Partition Filters：使用分区字段过滤，完全跳过特定的子路径 Pushed Filters：横向减少存储与Apark之间的数据传输，比如👆说的基于时间戳字段使用Pushed Filters优化 123456789101112-- timestamp类型的字段create_timestamp between CONCAT(add_months(last_day(from_utc_timestamp(now(), 'Asia/Jakarta')), -7),' ','17:00:00') AND CONCAT(date_sub(to_date(from_utc_timestamp(now(), 'Asia/Jakarta')), 1), ' 16:59:59.999')-- long 类型的字段create_timestamp&lt;=cast(to_utc_timestamp(now(), 'Asia/Jakarta') as long)*1000-- 其他应用场景：取当月的数据，月初看上一个整月create_timestamp between if( day(to_date(from_utc_timestamp(now(), 'Asia/Jakarta')))&lt;&gt;1, concat(to_date(date_sub(date_trunc('MM', to_date(from_utc_timestamp(now(), 'Asia/Jakarta'))), 1)), ' 17:00:00'), concat(to_date(date_sub(date_trunc('MM', date_sub(to_date(from_utc_timestamp(now(), 'Asia/Jakarta')), 1)), 1)), ' 17:00:00')) AND concat(date_add(to_date(from_utc_timestamp(now(), 'Asia/Jakarta')), -1), ' 16:59:59.999') Projection Pushdown：只查询使用到的字段，避免使用*，纵向减少存储与Spark之间的数据传输 负载均衡 避免数据倾斜 避免使用高频词或者null值较多的字段分组统计 如果不得已要处理负载极度不均衡数据，首先处理均衡数据，然后特例处理不均衡数据 如何快捷的查看spark的执行计划spark web ui的SQL选项 - 可以查看一个具体的实例 或者调用df.explain()方法即可打印出来spark的执行计划。 spark中的五种join策略Broadcast Hash Join -&gt; Shuffle Hash Join -&gt; Sort Merge Join -&gt;Cartesian Product Join -&gt; Broadcast Nested Loop Joinhttps://blog.csdn.net/a934079371/article/details/108591314https://www.cnblogs.com/jmx-bigdata/p/14021183.html 沉思：join中的一个or就可能引发最低效的执行，or连接是非等值连接，所以极有可能会走Cartesian Product Join 或者 Broadcast Nested Loop Join，即会进行效率极差的双重for循环，来进行数据匹配 所以，我们在做表关联的时候，一定要考虑到是否有小表可以进行广播，最优的策略是Broadcast Hash Join，它会将小表广播到所有的Executor上，然后在Executor上执行Hash Join Hash Join：对于每个Shuffle之后的分区，会将小表的分区数据构建成一个Hash table，然后根据join key与大表的分区数据记录进行匹配。 【讲】大数据集的高效处理map vs mapPartitionmap：apply func for each Row，每一行都会调用一次func函数，传递给func函数的参数是Row(或者一个case class对象)，结果返回一个新Row(或者一个自定义的case class对象)。 mapPartition：apply func for each partition，对每一个分区调用一次func函数，传递给func函数的参数是Iterator[Row] (或者Iterator[case class])迭代器，结果返回的也是一个迭代器。 mapPartition相比于map的优势： 减少了CPU调用函数的次数 如果需要连接外部环境，使用它可以减少连接的创建 比如一些可以分散在集群内部进行的任务，比如解密 比如要在处理的过程读取一些外部介质的数据，比如mysql或redis 因为一次性要出一个分区的数据，所以可能导致Executor的OOM，这要在分区数与Executor内存之间做权衡。一般情况下在资源充足的情况下使用mapPartition会比使用map效率要高 map相比于mapPartition的优势： 不会导致OOM，因为一次只处理一条记录 写法简单，易于理解 https://blog.csdn.net/high2011/article/details/79384159 https://blog.csdn.net/weixin_39043567/article/details/89916221 foreach vs foreachPartition与map vs mapPartition一样 toLocalIteratorstoLocalIetator()的工作原理类似于python中的itertools.chain(*iterables)，类似于下面的方式： 12345def chain(*iterables): # chain('ABC', 'DEF') --&gt; A B C D E F for it in iterables: for element in it: yield element 它是将所有的partition形成一个类似于Ietator的形式，使用的时候，spark会将数据数据一部分一部分的从excutor端收集到driver端(从sparkui上也可以看出来)，并不会一次性将所有partition的数据全量收集到driver端，导致driver端的OOM，这也是一种导出大量数据时候的选择，但是数据需要经历一个从excutor向driver传输的过程，所有partition都是串行的进行数据传输，这个过程是比较费时的。一个使用案例如下： 123456789101112131415161718192021222324252627282930313233import shutildef _mkdir(base_dir, filename, delete=False): path = os.path.join(base_dir, filename) if os.path.exists(path) and delete: shutil.rmtree(path, True) if not os.path.exists(path): os.makedirs(path) return pathimport csvdef to_csv(df:DataFrame, filename, threshold=1000000, base_dir='result/'): row_num = 0 index = 0 csvwrite = None csvfile = None first = True for val in df.toLocalIterator():# print(type(val.asDict()))# print(val.asDict())# print([v for k,v in val.asDict().items()]) if first or row_num&gt;=threshold: print(filename + ' write ' + str(index) + '...' + str(threshold*index)) file_path = _mkdir(base_dir, filename, first) csvfile = open(os.path.join(file_path, filename+'_%04d.csv' % index), 'w', buffering=4096) csvwrite = csv.writer(csvfile) index += 1 row_num = 0 first = False csvwrite.writerow(val) csvfile.flush() row_num += 1 使用spark过程中的一些小Tipsspark中的map-struct-array关于spark中的map、struct、array的一些操作 首先创建一个dataframe 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263import org.apache.spark.sql.expressions.UserDefinedFunctionimport org.apache.spark.sql.functions._import org.apache.spark.sql.&#123;DataFrame, Row, SparkSession&#125;val sparkConf = new SparkConfsparkConf.set(\"spark.maxRemoteBlockSizeFetchToMem\", 1024 * 1024 * 256 toString)sparkConf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")sparkConf.set(\"spark.master\", \"local[*]\")lazy val spark: SparkSession = &#123; SparkSession.builder() .config(sparkConf) .appName(appName) .enableHiveSupport() .getOrCreate()&#125;import spark.implicits._case class TestStruct(f1: String, f2: String)def testUdf: UserDefinedFunction = udf((a: String) =&gt; &#123; List(TestStruct(a, \"f1\"), TestStruct(s\"$a.00\", \"f2\"))&#125;)def testUdf1: UserDefinedFunction = udf((a: String) =&gt; &#123; TestStruct(a, \"f1\")&#125;)def testUdf2: UserDefinedFunction = udf((a: String) =&gt; &#123; Map(\"key\" -&gt; a, \"key1\" -&gt; a)&#125;)val lines: List[(String, Int, Int, Map[String, String])] = List( Tuple4(\"a\", 1, 2, Map(\"a\" -&gt; \"3\", \"c\" -&gt; \"a\")), Tuple4(\"b\", 11, 22, Map(\"a\" -&gt; \"33\", \"c\" -&gt; \"aa\")), Tuple4(\"c\", 111, 2222, Map(\"a\" -&gt; \"3333\", \"c\" -&gt; \"aaa\")))lines.toDF( \"f1\", \"f2\", \"f3\", \"extra\" ).withColumn( \"struct_array\", testUdf('f1) ).withColumn( \"map\", testUdf2('f1) ).withColumn( \"struct\", testUdf1('f1) ).withColumn( \"extra_a\", $\"extra.a\" ).withColumn( \"extra_c\", $\"extra.c\" ).withColumn( \"extra_d\", $\"extra.d\" ).withColumn( \"struct_array_f1\", $\"struct_array.f1\" ).withColumn( \"struct_f1\", $\"struct.f1\" ).withColumn( \"struct_f2\", expr(\"struct['f2']\") ).withColumn( \"struct_array0\", expr(\"struct_array[0]\") ).withColumn( \"struct_array0_f1\", expr(\"struct_array[0].f1\") ) 打印上述dataframe的schema如下： 12345678910111213141516171819202122232425262728root |-- f1: string (nullable = true) |-- f2: integer (nullable = true) |-- f3: integer (nullable = true) |-- extra: map (nullable = true) | |-- key: string | |-- value: string (valueContainsNull = true) |-- struct_array: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- f1: string (nullable = true) | | |-- f2: string (nullable = true) |-- map: map (nullable = true) | |-- key: string | |-- value: string (valueContainsNull = true) |-- struct: struct (nullable = true) | |-- f1: string (nullable = true) | |-- f2: string (nullable = true) |-- extra_a: string (nullable = true) |-- extra_c: string (nullable = true) |-- extra_d: string (nullable = true) |-- struct_array_f1: array (nullable = true) | |-- element: string (containsNull = true) |-- struct_f1: string (nullable = true) |-- struct_f2: string (nullable = true) |-- struct_array0: struct (nullable = true) | |-- f1: string (nullable = true) | |-- f2: string (nullable = true) |-- struct_array0_f1: string (nullable = true) 读取这个表数据，打印出来如下： 结论： map结构的数据如果想使用col或者$操作符取value，可以直接通过.取到对应的值，如果map中没有对应的key，则返回None； 不能使用类似col(&quot;extra[&#39;a&#39;]&quot;)或者\"extra['a']\"`的方式取`value`，也可以想象：`col`或者`操作符最终返回的是一个Column对象，它就是一个货真价实的Column，不是那种需要计算得来的Column 要想使用extra[&#39;a&#39;]的形式取值，需要使用expr操作符，如expr(&quot;extra[&#39;a&#39;]&quot;)，顾名思义，expr接受的是一个语句，它会解释并执行它，返回一个Column struct结构的字段，取法和map结构基本一致，但是人一上来就会想到用.来取值，而不是用[&#39;a&#39;]的方式来取值，是因为它是一个定义清晰的结构体，字段定义清晰，但是expr(struct[&#39;f2&#39;])这种取值方式是可以的 对于一个array类型的字段，如果想要取第i个元素，那么必须要使用expr操作符，比如：struct_array[0]或struct_array[0].f1 如果array的每个元素是一个结构体，取值时不需要考虑取哪一个，直接使用.取某个字段，则返回的是一个对应的字段的array，比如：struct_array.f1 在sparkSQL中不需要忌讳以上所有的问题，想怎么取就怎么取，因为它本身就是一个expr，他可以兼容所有的情况^_^ 意外收获： 如果使用scala来编写spark的程序，那么创建map和struct类型的字段，就十分简单了。创建一个udf，然后balala一通，udf返回的是case class就会形成一个struct类型的字段，udf返回的是一个Map就会形成一个map类型的字段，比如上面的testUdf2、testUdf1、testUdf 同时也可以使用org.apache.spark.sql.functions中的map和struct方法创建map或者struct spark中如何处理json数据有以下的json 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869&#123; \"status\": \"0x0000\", \"msg\": \"执行成功\", \"result\": \"通过\", \"score\": \"0\", \"engineName\": \"credit_unit_salim\", \"versionCode\": \"20200702credit_salim\", \"versionId\": 356307673651200, \"engineId\": 355251417716736, \"outputFields\": [ &#123; \"code\": \"return_reason\", \"name\": \"输出打回原因\", \"value\": \"null\" &#125;, &#123; \"code\": \"deny_days\", \"name\": \"输出拒绝天数\", \"value\": \"0\" &#125;, &#123; \"code\": \"deny_reason\", \"name\": \"输出拒绝原因\", \"value\": \"null\" &#125;, &#123; \"code\": \"decision\", \"name\": \"输出决策\", \"value\": \"forward_manual\" &#125;, &#123; \"code\": \"limit\", \"name\": \"输出授信额度\", \"value\": \"0\" &#125;, &#123; \"code\": \"cash_limit\", \"name\": \"现金贷款额度\", \"value\": \"0\" &#125; ], \"inputFields\": [ &#123; \"indo_id_check\": \"DEDY DWI SETYAWAN\", \"indo_identical_accuracy_ktp\": \"-2.0\", \"indo_mobile_number_approving\": \"1\", \"indo_name_diff_id_check\": \"0\", \"indo_name_diff_ocr\": \"1\", \"indo_nik_approving\": \"1\", \"indo_nik_diff_employee_nik\": \"0\", \"indo_nik_diff_ocr\": \"1\", \"indo_ocr_name\": \"DEDY DWI SEVYAWAN\", \"indo_ocr_nik\": \"3525051812850002\", \"indo_reject_his_nik\": \"0\", \"indo_reject_his_tel\": \"0\", \"同一个申请下return次数\": \"0\" &#125; ], \"outputFieldInfo\": [ &#123; \"输出打回原因\": \"null\", \"输出拒绝天数\": \"0\", \"输出拒绝原因\": \"null\", \"输出决策\": \"forward_manual\", \"输出授信额度\": \"0\", \"现金贷款额度\": \"0\" &#125; ]&#125; sql版本取json中的array，array的每个元素作为一个map结构 12345678910111213spark.sql(\"\"\"select from_json(data, 'array&lt;map&lt;string,string&gt;&gt;') rfrom ( select json_tuple(json, 'inputFields') data) t\"\"\").printSchema()root |-- r: array (nullable = true) | |-- element: map (containsNull = true) | | |-- key: string | | |-- value: string (valueContainsNull = true) array&lt;map&lt;string,string&gt;&gt;这个是如何得来的呢？是这样的，可以使用schema_of_json函数来获取一个json的结构是怎样的。 12345spark.sql(\"\"\"select schema_of_json(json)\"\"\").toPandas()#array&lt;struct&lt;indo_id_check:string,indo_identical_accuracy_ktp:string,indo_mobile_number_approving:string,indo_name_diff_id_check:string,indo_name_diff_ocr:string,indo_nik_approving:string,indo_nik_diff_employee_nik:string,indo_nik_diff_ocr:string,indo_ocr_name:string,indo_ocr_nik:string,indo_reject_his_nik:string,indo_reject_his_tel:string,同一个申请下return次数:string&gt;&gt; 但是通过上述方式得到的结果不能直接拿来用，需要做一些变形：如果array的每个元素的字段都是固定的，那么可以将array的元素定义为一个struct，但是我们的需求中的数据，array的每个元素的字段是不固定的，且我们没有将其字段都穷举出来，所以我们就把它定义为一个map类型的，后续将它使用explode_outer展开再进行处理。通过查询spark的api，可以知道MapType的构造方法需要两个参数，分别为key的Type和value的Type，这里我们直接使用最通用的String类型代替了 pyspark 1234567891011from pyspark.sql.functions import *from pyspark.sql.types import *schema = ArrayType(MapType(StringType(), StringType()))spark.sql(\"\"\"select json_tuple(json, 'inputFields') items\"\"\").withColumn( 'items', from_json('items', schema)).toPandas() 123456789101112131415161718192021222324252627282930313233343536373839spark.sql(\"\"\"select json_tuple(json, 'inputFields') items-- from atome_id_mysql_snapshot_ruleengine.t_result_catalog limit 1\"\"\").withColumn( 'items', from_json('items', schema)).withColumn( 'item', explode('items')).withColumn( 'keys', map_keys('item')).withColumn( 'values', map_values('item')).withColumn( 'k_v', arrays_zip('keys', 'values')).withColumn( 'kv', explode_outer('k_v')).printSchema()root |-- items: array (nullable = true) | |-- element: map (containsNull = true) | | |-- key: string | | |-- value: string (valueContainsNull = true) |-- item: map (nullable = true) | |-- key: string | |-- value: string (valueContainsNull = true) |-- keys: array (nullable = true) | |-- element: string (containsNull = true) |-- values: array (nullable = true) | |-- element: string (containsNull = true) |-- k_v: array (nullable = true) | |-- element: struct (containsNull = false) | | |-- keys: string (nullable = true) | | |-- values: string (nullable = true) |-- kv: struct (nullable = true) | |-- keys: string (nullable = true) | |-- values: string (nullable = true) 123456789101112131415df = spark.sql(\"\"\"select json_tuple(json, 'inputFields') items\"\"\").withColumn( 'items', from_json('items', schema)).withColumn( 'item', explode('items'))# df.printSchema()# df.select(expr(\"posexplode(d)\")).printSchemadf.select(expr('explode(item)')).toPandas() # 将map 展开 posexplode会多一个pos的字段 key value 0 indo_id_check DEDY DWI SETYAWAN 1 indo_identical_accuracy_ktp -2.0 2 indo_mobile_number_approving 1 3 indo_name_diff_id_check 0 4 indo_name_diff_ocr 1 5 indo_nik_approving 1 6 indo_nik_diff_employee_nik 0 7 indo_nik_diff_ocr 1 8 indo_ocr_name DEDY DWI SEVYAWAN 9 indo_ocr_nik 3525051812850002 10 indo_reject_his_nik 0 11 indo_reject_his_tel 0 12 同一个申请下return次数 0 如何理解import spark.implicits._在初期使用spark的时候，大家都会遇见一个很奇怪的写法import spark.implicits._ 这里面包含了四个关键字：import、spark、implicits、_ import和_实际上是Scala中包引入的写法，表示引入指定包内的所有成员 本文主要想记录一下另外两个关键字：spark、implicits 关键字一：spark spark在这里是这样产生的： 12345val conf: SparkConf = new SparkConf().setMaster(\"local\").setAppName(\"test\")val spark: SparkSession = SparkSession .builder() .config(conf) .getOrCreate() 那么这就会有点让人奇怪的地方了，在scala中import到底是如何工作的呢？为什么可以import一个实例对象呢？ 经过查询发现，scala确实能够导入运行时对象实例的成员，举例如下： 1234567891011121314151617181920scala&gt; case class User(name:String, age:Int)defined class Userscala&gt; val a=User(\"zzz\",39)a: User = User(zzz,39)scala&gt; println(\"%s's age is %d\" format (a.name, a.age))zzz's age is 39scala&gt; import a._import a._scala&gt; println(\"%s's age is %d\" format (name, age))zzz's age is 39scala&gt; nameres3: String = zzzscala&gt; ageres4: Int = 39 定义了一个case class User，然后执行了一次import a._，然后发现可以直接使用a实例对象的属性变量，而不需要加a.的前缀。 那么为什么在spark中需要这么做呢？我们点进去spark.implicits，会发现源码是这样的： 12345class SparkSession &#123; object implicits extends SQLImplicits with Serializable &#123; protected override def _sqlContext: SQLContext = SparkSession.this.sqlContext &#125;&#125; implicits是SparkSession的一个内部单例对象，它有一个_sqlContext的函数，用于获取SQLContext，但是它必须调用当前正在运行的SparkSession实例，通过调用它的sqlContext属性来获取。源码中的SparkSession.this实际上是调用了SparkSession class的伴生对象的.this，这个this实际上就是正在运行的SparkSession实例。 具体的隐式转换都可以在org.apache.spark.sql.SQLImplicits中看到 这里实际上是scala中class与伴生对象的爱恨纠缠以及import关键字的使用技巧 下面给出一个测试案例： 1234567891011121314151617181920212223// TestObject.scalaclass TestObject(name: String, age: Int) &#123; val user: User = new User(name, age) object innerObj &#123; val user1: User = TestObject.this.user def say(name: String) &#123; println(s\"$&#123;name&#125; innerObj say TestObject.this = $&#123;TestObject.this&#125;\") &#125; &#125; def say2(): Unit = &#123; println(s\"TestObject say2 TestObject.this = $&#123;TestObject.this&#125;\") &#125;&#125;object TestObject &#123;&#125; 1234567// User.scalaclass User(name: String, age: Int) &#123; def say(): Unit = &#123; println(\"%s's age is %d\" format(name, age)) &#125;&#125; 123456789101112131415161718192021222324252627// Test.scalaobject Test &#123; def main(args: Array[String]): Unit = &#123; val testObject = new TestObject(\"sfa\", 12) println(s\"testObject = $testObject\") // a testObject.say2() // b println(s\"testObject.user = $&#123;testObject.user&#125;\") // c import testObject.innerObj._ println(s\"user1 = $user1\") // d say(\"testObject\") // e println(\"------------------------------\")// val testObject1 = new TestObject(\"abc\", 24)// println(s\"testObject1 = $testObject1\")// testObject1.say2()// println(s\"testObject1.user = $&#123;testObject1.user&#125;\")// import testObject1.innerObj._//// println(s\"testObject1 user1 = $user1\")// say(\"testObject1\") &#125;&#125; 打印结果如下： 123456testObject = com.msb.bigdata.scala.test_object.TestObject@5702b3b1TestObject say2 TestObject.this = com.msb.bigdata.scala.test_object.TestObject@5702b3b1testObject.user = com.msb.bigdata.scala.test_object.User@192b07fduser1 = com.msb.bigdata.scala.test_object.User@192b07fdtestObject innerObj say TestObject.this = com.msb.bigdata.scala.test_object.TestObject@5702b3b1------------------------------ 通过打印结果可以看到： a、b、e三处打印的是同一个对象实例，所以可以断定在class TestObject的内部中如果调用了TestObject.this实际上是指向的正在运行中的testObject对象 TestObject.this只能在class TestObject的内部调用，不能在外部调用 Test.scala中注掉的那一部分，这一句import testObject1.innerObj._会在编译期间报错，如下 1234567891011Error:(29, 36) reference to user1 is ambiguous;it is imported twice in the same scope byimport testObject1.innerObj._and import testObject.innerObj._ println(s&quot;testObject1 user1 Error:(30, 5) reference to say is ambiguous;it is imported twice in the same scope byimport testObject1.innerObj._and import testObject.innerObj._ say(&quot;testObject1&quot;)= $user1&quot;) import 实例对象的行为不能被多次使用，说的是user1变量和say函数被重复import在同一个作用域内，引发了歧义 在spark那里，不允许以下的写法： 1234567891011121314151617181920212223242526272829303132333435object lesson02_sql_api01_1 &#123; def main(args: Array[String]): Unit = &#123; val conf: SparkConf = new SparkConf().setMaster(\"local\").setAppName(\"test\") val session: SparkSession = SparkSession .builder() .config(conf) .getOrCreate() import session.implicits._ println(session) val session1: SparkSession = SparkSession .builder() .config(conf) .getOrCreate() println(session1) import session1.implicits._ val dataDF: DataFrame = List( \"hello world\", \"hello world\", \"hello msb\", \"hello world\", \"hello world\", \"hello spark\", \"hello world\", \"hello spark\" ).toDF(\"line\") dataDF.show() &#125;&#125;// import session.implicits._ 和 import session1.implicits._可以同时出现，但是如果用到了其中的隐式转换，就会报错，一般也没人会这么写 将Test.scala替换成如下的写法： 1234567891011121314151617181920212223242526272829object Test &#123; def main(args: Array[String]): Unit = &#123; val testObject = new TestObject(\"sfa\", 12) println(s\"testObject = $testObject\") // a testObject.say2() // b println(s\"testObject.user = $&#123;testObject.user&#125;\") // c import testObject.innerObj._ println(s\"user1 = $user1\") // d say(\"testObject\") // e println(\"------------------------------\") val testObject1 = new TestObject(\"abc\", 24) println(s\"testObject1 = $testObject1\") testObject1.say2() println(s\"testObject1.user = $&#123;testObject1.user&#125;\")// import testObject1.innerObj._//// println(s\"testObject1 user1 = $user1\")// say(\"testObject1\") println(s\"testObject.innerObj.user1 = $&#123;testObject.innerObj.user1&#125;\") println(s\"testObject.innerObj.say = $&#123;testObject.innerObj.say(\"testObject1\")&#125;\") &#125;&#125; 打印结果如下： 123456789101112testObject = com.msb.bigdata.scala.test_object.TestObject@5702b3b1TestObject say2 TestObject.this = com.msb.bigdata.scala.test_object.TestObject@5702b3b1testObject.user = com.msb.bigdata.scala.test_object.User@192b07fduser1 = com.msb.bigdata.scala.test_object.User@192b07fdtestObject innerObj say TestObject.this = com.msb.bigdata.scala.test_object.TestObject@5702b3b1------------------------------testObject1 = com.msb.bigdata.scala.test_object.TestObject@64bfbc86TestObject say2 TestObject.this = com.msb.bigdata.scala.test_object.TestObject@64bfbc86testObject1.user = com.msb.bigdata.scala.test_object.User@64bf3bbftestObject1.innerObj.user1 = com.msb.bigdata.scala.test_object.User@64bf3bbftestObject1 innerObj say TestObject.this = com.msb.bigdata.scala.test_object.TestObject@64bfbc86testObject1.innerObj.say = () 我们会发现，就没问题了。而且testObject1的InnerObject调用的TestObject.this实际上是指向的testObject1对象。 关键字二：implicits import spark.implicits._中的implicits关键字实际上是SparkSession的内部object，它继承了SQLImplicits 123object implicits extends SQLImplicits with Serializable &#123; protected override def _sqlContext: SQLContext = SparkSession.this.sqlContext&#125; org.apache.spark.sql.SQLImplicits：A collection of implicit methods for converting common Scala objects into Datasets. 从以上的案例中可以得到： SparkSession.this是当前运行的SparkSession实例，我们可以通过实例对象将SparkSession的内部object引入进来。这里为什么一定要使用SparkSession实例的这种方式引入隐式转换呢？根据源码可以知道，在进行隐式转换的时候需要用到sqlContext，它是sparkSession的一个属性，只有在SparkSession创建完成之后，它才有值。 可以看到在org.apache.spark.sql.SQLImplicits中这里用到了_sqlContext对象 12345678910111213141516/** * Creates a [[Dataset]] from an RDD. * * @since 1.6.0 */implicit def rddToDatasetHolder[T : Encoder](rdd: RDD[T]): DatasetHolder[T] = &#123; DatasetHolder(_sqlContext.createDataset(rdd))&#125;/** * Creates a [[Dataset]] from a local Seq. * @since 1.6.0 */implicit def localSeqToDatasetHolder[T : Encoder](s: Seq[T]): DatasetHolder[T] = &#123; DatasetHolder(_sqlContext.createDataset(s))&#125; 从org.apache.spark.sql.SQLImplicits中 我们可以看到几个非常漂亮的隐式转换，如下： 12345678910111213141516/** * Converts $\"col name\" into a [[Column]]. * * @since 2.0.0 */implicit class StringToColumn(val sc: StringContext) &#123; def $(args: Any*): ColumnName = &#123; new ColumnName(sc.s(args: _*)) &#125;&#125;/** * An implicit conversion that turns a Scala `Symbol` into a [[Column]]. * @since 1.3.0 */implicit def symbolToColumn(s: Symbol): ColumnName = new ColumnName(s.name) 这就是我们平时可以直接下如下的脚本的关键所在： 1df.select(\"a\", \"b\", $\"c\", $\"d\"+1, 'e, 'f) why create SparkSession\\what is SparkSessionClient - Driver 1、第一个属性就是：sparkContext 一个上下文对象 是用户端程序与spark集群沟通(通信)的桥梁：提交任务、监控任务、任务调度，支持spark分布式的核心，这里面要讲的内容就太多了， 2、提供执行spark sql的入口 3、提供table方法 直接读取hive表 4、封装了一系列创建DataFrame的方法 什么是DataFrame：type DataFrame = Dataset[Row] 所以这就是为什么我们每个application都要先创建一SparkSession或者sparkContext的原因了。具体细节暂时无法展开，对于不理解为什么要创建这个对象的同学来说，理解到这里就够了 窗口函数什么是窗口函数？应该如何理解开窗操作？ 窗口函数是对一组具有相同key的数据(partition by)，根据特定的字段顺序(order by)做的一个特殊的处理，既然是窗口，那么除了可以限定根据哪个key分组，根据哪个字段排序，当然也可以限定窗口的大小(rows between或者 range between) 开窗操作，实际上是从划分好的窗口内的提取一些关键信息，比如：一个窗口内的顺序(row_number)，统计值(max、min、mean)，第一个值(first)，最后一个值(last)；以及一些特殊信息，比如：取窗口内当前行的前n行数据(lag)，取窗口内当前行的后n行(lead)等 注意，开窗操作不会减少数据量 窗口函数适合什么样的场景 类似于见到以下的一些场景，就可以考虑使用窗口函数，需要分组，[需要组内排序，]同时又不能减少列的数量(其他的列应该保持不变) 取用户的第1条订单， row_number() over(partition by user_id order by create_timestamp) = 1 给每个用户的订单按照创建时间加个序号，row_number() over(partition by user_id order by create_timestamp) 需要判断用户从哪个页面过来的，根据目前我们的埋点结构，就可以： 离开上一个页面：last_page，LeavePage 进入当前页面：current_page，PageEvent 严格情况，埋点打点的时候一定会按照先离开再进入，且这两者之间再没有其他的action发生，那么可以直接 1select current_page, lag(last_page) over(partition by user_id order by timestamp) last_page from ... 但是呢，通常情况下这种情况是不可能一直保持的，由于各种打点的问题，所以我们可以退一步判断，只要保证进入当前页面前一定有一个离开上一个页面的action，就可以了，但是也有个要求，埋点不能漏掉，否则也无法进行判断了。可以这样做 1select current_page, last(if(action='LeavePage' last_page, null), true) over(partition by user_id order by timestamp) last_page from ... true表示只返回最后一个不为空的last_page 将某个值辐射到窗口内的所有记录上 窗口函数与分组聚合函数的区别 窗口函数不会减少数据量，它的结果是把从特定窗口提取的信息放到一个新列上 分组聚合函数，会相应的减少数据量，最终的结果只会保留被分组的字段以及聚合的字段 窗口的大小如何设定？ 窗口大小有两种方式可以设定 子窗口需要指定一个边界，有以下两种方式： ROWS between CURRENT ROW | UNBOUNDED PRECEDING | [num] PRECEDING AND UNBOUNDED FOLLOWING | [num] FOLLOWING| CURRENT ROW RANGE between [num] PRECEDING AND [num] FOLLOWING 窗口的含义 ROWS是物理窗口，从行数上控制窗口的尺寸的，表示在当前行的前后偏移量，较好理解。RANGE是逻辑窗口，从列值上控制窗口的尺寸，表示根据当前列值的前后偏移量，是与当前列的具体值挂钩的！！！ 比如：range between 4 preceding AND 7 following，描述的是：如果当前值为10的话就取前后的值在6到17之间的数据。 可以参考这篇文章：https://blog.csdn.net/qq_42374697/article/details/115109386 窗口函数的一些默认情况 123sum(1) over(partition by shop_id order by date)-- 有order by 关键字，排序的时候 会有空值在前还是在后之分，默认是 NULLS FIRST，窗口大小为RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW，如下sum(CAST(1 AS BIGINT)) OVER (PARTITION BY shop_id ORDER BY date ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) 与 NULLS FIRST对应的是NULLS LAST 123sum(1) over(partition by shop_id)-- 不加order by 关键字，实际上row between为UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING，如下sum(CAST(1 AS BIGINT)) OVER (PARTITION BY shop_id ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) 常用的join函数介绍 outer，full，full outer：全连 left，left outer：左连 right，right outer：右连 inner，不写：返回joinDF1和joinDF2合并的rows，如果joinDF2中有多条记录对应于joinDF1的同一条记录，那么返回的row number会大于joinDF1的row number left anti：过滤出joinDF1中joinDF2没有的部分，只返回joinDF1中的rows left semi：过滤出joinDF1中和joinDF2共有的部分，只返回joinDF1中的rows left anti和left semi在某些场景下也会经常用到 如何调试使用Scala编写的Spark程序key points：set(“spark.master”, “local[*]”) 由于spark task是运行在集群的executor节点上，如果在某个具体的步骤想通过打印一些数据来调试程序，是不可能的，除非你可以进入到真正执行对应task的executor节点，去看它的log。 所以如果想要调试spark程序，只能在local模式，同时，为了增加调试的效率，可以适量的缩减数据量，比如你要在一个很大的日志数据上测试逻辑，那么你可以只读取其中一个分区。 local模式的时候，Driver端会扮演Executor的角色，所以是可以看到你打印的信息的。 【讲】Spark的设计原理分布式、函数闭包（是否可序列化）、高阶函数（函数可以作为参数传递给另一个函数）函数式编程，下面的介绍中你将会很明显的感受到 RDDA Resilient Distributed Dataset (RDD) 弹性[仅介绍]分布式[仅介绍]数据集[讲] What is RDD? A list of partitions - 分区计算 A function for computing each split - 每个RDD都含有一个对当前RDD进行处理的方法 A list of dependencies on other RDDs - 这里就是我们后面要说的宽窄依赖，即上下游RDD是如何映射的 a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned) - 分区的策略 a list of preferred locations to compute each split on (e.g. block locations for an HDFS file) - 计算向数据移动 的思想 离线与实时计算的区别： 离线计算，是数据已经存放在某个位置，所以如果在数据所在的节点启动计算，那么可以减少网络IO的消耗，提高计算效率，这是离线计算平台应该有的优化策略 实时计算：数据未知，源源不断的产生，一般都会从消息队列中拿到对应的数据，然后将数据走一遍流式处理，这个时候数据具体存在的节点已经不是集群中的datanode，一般是数据计算引擎主动去消息队列拉取数据，所以此时就是数据向计算移动 Spark 与Flink的区别： spark的shuffle过程是采用下游stage主动拉取上游stage的结果数据，pull模式 Flink是将上游结果数据主动推给下游的处理节点，push模式，所以它在处理流式数据的时候更加有效，更像是水流一样流畅 Resilient - 弹性：支持数据结果cache，checkpoint，可以很高效的自定义不同的缓存点，加速任务的执行效率 Distributed - 分布式：所有用户编写的程序都不是在Driver运行(local模式除外)，都是在集群中的executor上运行的，对全量数据集进行分区处理，可以在大量的executor节点上进行并行处理 Dataset - 数据集：支持数据集的所有操作，针对RDD的操作，实际上就和针对于一个本地的数据集操作那么简单，直观。支持的操作比如下面所示： DataSet采用的是基于迭代器模式的丰富实现模式可以有比较直观的了解。 Resilient 和Distributed 需要结合SparkContext讲解，这里面的内容更加复杂，就先不讲解了。 Iterator模式什么是迭代器模式，迭代器本身不包含数据，它包含的访问数据的方式，它主要有两个方法hasNext和next方法。 hasNext用来询问集合中是否还有数据可供访问，next用来实际获取数据，使用迭代器模式，可以做到，数据从始至终只有一份(中间过程中被cache的不算)。 scala版本的迭代器模式使用scala编写了一段wordcount的代码如下： 123456789101112131415object TestIterator &#123; def main(args: Array[String]): Unit = &#123; val listStr = List( \"hello world\", \"hello msb\", \"good idea\" ) val flatMap: List[String] = listStr.flatMap((x: String) =&gt; x.split(\" \")) flatMap.foreach(println) val mapList: List[(String, Int)] = flatMap.map((_, 1)) mapList.foreach(println) &#125;&#125; 分析一下上面程序的执行过程： 创建listStr对象 listStr调用flatMap，作用是将listStr的每个元素使用空格分隔，然后合并成一个大的list 继续调用map，将每个元素转换成一个[String,Int]的二元组 打印结果 以上代码有一个致命的问题：在数据量非常的的时候，会急剧的消耗内存空间。为什么？简单分析下：第一步空间复杂度为O(N)，第二步又生成了一个全新的List[String]对象，又是O(N)的空间复杂度，第三步中又生成了一个List[(String,Int)]对象，空间复杂度依然是O(N)。通过分析可知，在数据统计的过程中，貌似并没有必要将中间过程的数据存储下来，不但占用空间，还没有任何用处。 于是，我们想到了一种设计模式-迭代器模式。迭代器模式在内部维护了一个指针，实际上并不会存储数据，在遍历数据集的时候，不断的消耗当前指针。 现在开始复习一下迭代器模式 迭代器一定有两个方法： 12345public interface Iterator&lt;E&gt; &#123; //Element E //Type T //Key K //Value V boolean hasNext(); // 是否还有下一个元素 E next(); // 获取下一个元素&#125; 集合类一定有一个返回迭代器的函数 123456public interface Collection&lt;E&gt; &#123; void add(E o); int size(); Iterator iterator();&#125; 具体的集合类实现Collection接口 1234567891011121314151617181920212223242526272829303132333435363738394041class ArrayList&lt;E&gt; implements Collection&lt;E&gt; &#123; E[] objects = (E[]) new Object[10]; private int index = 0; public void add(E o) &#123; if (index == objects.length) &#123; E[] newObjects = (E[]) new Object[objects.length * 2]; System.arraycopy(objects, 0, newObjects, 0, objects.length); objects = newObjects; &#125; objects[index] = o; index++; &#125; public int size() &#123; return index; &#125; @Override public Iterator&lt;E&gt; iterator() &#123; return new ArrayListIterator(); &#125; private class ArrayListIterator&lt;E&gt; implements Iterator&lt;E&gt; &#123; private int currentIndex = 0; @Override public boolean hasNext() &#123; if (currentIndex &gt;= index) return false; return true; &#125; @Override public E next() &#123; E o = (E) objects[currentIndex]; currentIndex++; return o; &#125; &#125;&#125; 可以发现，迭代器中不会存数据，只是保存了一个指针，指向当前遍历到了哪一个索引，只有真正开始遍历的时候，指针才会开始移动，并且没有回退的方法，即迭代器只能遍历一次(另有设计的除外)。 解决方案 使用迭代器实现上面的功能 123456789101112131415161718192021object TestIterator &#123; def main(args: Array[String]): Unit = &#123; val listStr = List( \"hello world\", \"hello msb\", \"good idea\" ) val iter: Iterator[String] = listStr.iterator //什么是迭代器，为什么会有迭代器模式？ 迭代器里不存数据！ val iterFlatMap = iter.flatMap((x: String) =&gt; x.split(\" \")) // iterFlatMap.foreach(println) // 中途不能打印，否则后续就读取不到数据了 val iterMapList = iterFlatMap.map((_, 1)) while (iterMapList.hasNext) &#123; val tuple: (String, Int) = iterMapList.next() println(tuple) &#125; //1.listStr真正的数据集，有数据的 //2.iter.flatMap 没有发生计算，返回了一个新的迭代器 &#125;&#125; 分析：基于迭代器的方案中，除了listStr中存储了数据，中间的过程中只有计算逻辑没有存储数据(faltMap会有一点少少的数据缓冲存储)。这和spark中的算子的思想一样啊，也可以说spark是借鉴了迭代器的编程模式。 Spark的transformation算子：类比这里的flatMap/map Spark的action算子：类比这里的foreach 关于scala中flatMap、map、foreach的过程分析 iter = listStr.iterator 12345678def iterator: Iterator[A] = new AbstractIterator[A] &#123; var these = self def hasNext: Boolean = !these.isEmpty def next(): A = if (hasNext) &#123; val result = these.head; these = these.tail; result &#125; else Iterator.empty.next()&#125; iterator返回的是一个AbstractIterator对象，重写了hasNext和next函数： hasNext：调用listStr的isEmpty方法，如果为空则返回false next： 先检测是否有元素，有的话返回listStr的头结点，并且移动these到剩余部分的头部； 否则返回空。按理说不会为空，因为都是先判断了hasNext为true才会调用next iterFlatMap = iter.flatMap((x:String) =&gt; x.split(“ “)) 1234567891011121314def flatMap[B](f: A =&gt; GenTraversableOnce[B]): Iterator[B] = new AbstractIterator[B] &#123; private var cur: Iterator[B] = empty private def nextCur() &#123; cur = f(self.next()).toIterator &#125; def hasNext: Boolean = &#123; // Equivalent to cur.hasNext || self.hasNext &amp;&amp; &#123; nextCur(); hasNext &#125; // but slightly shorter bytecode (better JVM inlining!) while (!cur.hasNext) &#123; if (!self.hasNext) return false nextCur() &#125; true &#125; def next(): B = (if (hasNext) cur else empty).next()&#125; flatMap返回的也是一个AbstractIterator，也重写了hasNext和next函数： 它在这里维护了一个cur的小迭代器，之所以说小，是因为它会缓存上一个调用节点的一条记录经过f处理之后的结果 hasNext：重点看这个函数 这里会先判断cur是否有元素，有的话直接返回true； 否则的话，调用父类的hasNext，判断是否还有值，没有的话，返回false； 否则的话调用父类的next获取一条新的记录，并交给处理函数f处理，处理完之后交给cur缓存起来 next：如果有值，则直接从cur中取值，而且永远都只从cur中取值 iterMapList = iterFlatMap.map((_, 1)) 1234def map[B](f: A =&gt; B): Iterator[B] = new AbstractIterator[B] &#123; def hasNext = self.hasNext def next() = f(self.next())&#125; map返回的也是一个AbstractIterator，也重写了hasNext和next函数，不过他这里的逻辑比较简单了，因为map只是完成了一个映射的过程 iterMapList.foreach(println) 打印收工。 通过解析spark的wordcount程序理解迭代器模式以下是最精简的一个spark的入门程序代码：统计一个文件的单词数量 1234567val conf = new SparkConf()conf.setAppName(\"wordcount\")conf.setMaster(\"local\") //单击本地运行val sc = new SparkContext(conf)val fileRDD: RDD[String] = sc.textFile(\"testdata.txt\")fileRDD.flatMap( _.split(\" \") ).map((_,1)).reduceByKey( _+_ ).foreach(println) 下面根据这个程序详细解读一下spark的执行原理： 行1-4创建上下文对象 行6创建第一个rdd 行7通过pipeline的api方式组装我们的业务逻辑 主要解读行6和行的7所发生的事情 sc.textFile(“testdata.txt”) Read a text file from HDFS, a local file system (available on all nodes), or any Hadoop-supported file system URI, and return it as an RDD of Strings. 得到一个HadoopRDD，贴源的RDD，可以用来读取数据，它会返回一个数据源的iterator flatMap 12345 // def flatMap[U: ClassTag](f: T =&gt; TraversableOnce[U]): RDD[U] = withScope &#123; val cleanF = sc.clean(f) new MapPartitionsRDD[U, T](this, (context, pid, iter) =&gt; iter.flatMap(cleanF)) &#125; flatMap会返回一个MapPartitionsRDD，他的构造方法中有两个参数：this和一个func，this表示当前对象，func是一个新的函数，它调包装了我们传递进去的函数，这其实就是一个闭包，最终这个函数是作用在一个本地(executor上)的scala的iterator上 这里还有一个函数调用sc.clean(f)，这里就是我们上面所说的闭包检查，因为spark程序是一个分布式的，我们这里定义的函数实际上并不是在我们的这台机器上运行的，而是会在集群中的任意Executor上运行，所以spark就需要将其序列化然后传输到其他的Executor上，然后在Executor上在反序列化出来执行，如果你传递的f不能够被序列化，那么就会抛出一个异常org.apache.spark.SparkException: Task not serializable，程序直接退出。 clean的过程需要做的事情如下： removes unreferenced variables in $outer’s，去除未被使用的变量 updates REPL variables，直接填充计算结果 所以如果你定义的一些f，引用了map外部不可序列化的对象就会报不可序列化的异常。 关于闭包的知识点可以看下这个文章：https://blog.csdn.net/qq_26838315/article/details/114700368 map 12345 // def map[U: ClassTag](f: T =&gt; U): RDD[U] = withScope &#123; val cleanF = sc.clean(f) new MapPartitionsRDD[U, T](this, (context, pid, iter) =&gt; iter.map(cleanF)) &#125; 同理如上 reduceByKey 1234 // def reduceByKey(partitioner: Partitioner, func: (V, V) =&gt; V): RDD[(K, V)] = self.withScope &#123; combineByKeyWithClassTag[V]((v: V) =&gt; v, func, func, partitioner) &#125; reduceByKey，是一个聚合类的算子，会引发shuffle行为，它内部实际上是调用了combineByKey，了解过hadoop的MR程序的话，我们就会知道，combine在MR中是一项优化点，即会在map端进行数据的预聚合，可以减少需要网络传输的数据量，极大程度的减少带宽的使用，提高效率。 reduce操作，需要考虑三种情况 第一条数据怎么放 后续数据怎么放 在大数据量的情况下，必然会发生数据溢写，那么就会涉及溢写数据的合并 所以combineByKey需要三个函数，分别对应以上三种情况 createCombiner: V =&gt; C V是输入的原始数据类型，C是输出数据类型。可见，我们可以对数据做一些变换操作 mergeValue: (C, V) =&gt; C V是输入的原始数据类型，C是输出数据类型。这个函数的第一个参数是已经聚合过的历史结果，第二个参数是新进来的数据，最终输出一个结果 mergeCombiners: (C, C) =&gt; C 这里是对溢写文件的合并，所以数据类型都是输出的数据类型。 12345678910111213 // def combineByKeyWithClassTag[C]( createCombiner: V =&gt; C, mergeValue: (C, V) =&gt; C, mergeCombiners: (C, C) =&gt; C, partitioner: Partitioner, mapSideCombine: Boolean = true, serializer: Serializer = null)(implicit ct: ClassTag[C]): RDD[(K, C)] = self.withScope &#123; new ShuffledRDD[K, V, C](self, partitioner) .setSerializer(serializer) .setAggregator(aggregator) .setMapSideCombine(mapSideCombine) &#125; 这里就需要区分reduceByKey与groupByKey的区别了。 groupByKey底层也是调用的combineByKeyWithClassTag，但是把mapSideCombine参数设置为了false，即默认不开启map的预聚合 foreach 1234def foreach(f: T =&gt; Unit): Unit = withScope &#123; val cleanF = sc.clean(f) sc.runJob(this, (iter: Iterator[T]) =&gt; iter.foreach(cleanF))&#125; action算子，调用runJob提交Job，触发计算逻辑 从这个过程中可以发现，我们每次调用的算子，并不会触发计算，而是创建了一个个的RDD对象，直到我们调用了action算子。 简单介绍一下上面这幅图： 这幅图一共有两个层次： 第一层：api层 api层就是spark暴露给我们的编程接口，也就是我们平时所写的rdd.map.filter.flatMap等等，请注意，我们这里写的这些只是向spark声明了一系列的依赖关系以及，我们想在job运行时对每条数据实施的具体操作，在这个时候，spark并不会立刻去执行我们的逻。 第二层：pipeline层 这里是spark依据我们组装的业务逻辑，在底层构造的一条完整的数据管道，当遇到action算子，触发了runJob，那么spark就会把以上所有的逻辑分发到集群的Executor节点上，这个时候，数据处理的逻辑才真正的开始运行 宽窄依赖Dependency NarrowDependency - 窄依赖 OneToOneDependency 分区的对应关系，上游RDD的分区一一映射到下游RDD RDD的数量不变 常见操作：map，filter RangeDependency 上游RDD和下游RDD的对应关系是多对一，分区总数不变，常见操作：union 上游多个RDD的分区形成下游的一个分区，常见操作：coalesce ShuffleDependency - 宽依赖 下游RDD的分区数据来自于上游RDD所有分区的部分数据 一个RDD：分组聚合操作 多个RDD形成一个RDD：join 对某个RDD手动分区：repartition shuffle：实际上就是人类惯用的思考模式，你的数据是否需要按照一个key为一组进行操作。 包括统计相同key的记录数、最大最小等聚合指标 做数据的关联-将相同key的数据放到一起考虑 Partition什么是分区？分区有什么作用(并行度)，分区与Task的关系 分区是spark并行计算的核心，也就是说不同分区的数据可以并行的被处理，提高执行效率。 一个分区对应了一个具体的可执行的task，这里所说的并行处理，实际上就是多个thread并发的进行 分区与文件的关系? 初始分区数貌似是不确定的，他会根据原始数据文件的大小，DataFrame和Dataset的api是经过Catalyst optimizer优化的，所以初始分区数是经过优化计算得来的。这一般不是我们所担心的。 什么时候需要重分区？ 遇到shuffle的时候，就会进行重分区 手动进行重分区，以希望能解决数据倾斜， 手动进行重分区，以希望减少输出的文件数 什么是shuffle，shuffle的本质是什么？ shuffle是是在计算过程中，对数据按照某种规则进行混洗的过程，他的目标是将具有相同特征的数据集中到一起，以便于对他们进行聚合以及关联等操作 本质其实就是根据某种策略进行重分区，这里的策略最多的就是根据特定字段进行hash取模，计算出每一条记录应该属于的分区号 重分区的操作有哪些：repartition&amp;coalesce，他们之间有什么差异？什么时候该用repartition，什么时候该用coalesce？ 此处的coalesce与functions中的coalesce是不一样的哦 可以参考coalesce导致的内存溢出这里的回答 具体案例分析： 不对原始数据源做任务处理，或者做一些映射类的处理，直接写入目标路径，会产生多少文件？如果有分区字段，会产生多少文件？小文件数很多对于集群会有怎样的影响？如何减少小文件的数量？ 结果文件数和原始分区数是一样的 每个分区的文件数都会和原始分区数是一样的 针对于HDFS来说，小文件太多会对NameNode节点造成较大的压力，因为NameNode节点需要管理集群文件块的元数据。同时刷新hive matestore的效率也会变慢 可以使用我们上面说的重分区操作，那么具体使用repartition还是coalesce还是需要视具体情况而定的。如果要进行重分区的dataframe前面已经经过了，coalesce是某些情况下效率较repartition要高。 RDD与DataFrame[DataSet]的区别是什么RDD处理的数据都是没有具体schema的，较为底层，基于RDD开发就必须要非常清楚每次要处理的那一条记录的每个位置的数据的含义是什么 DataFrame或者DataSet是有具体schema的RDD的实现，它基于RDD的基础之上给我们提供了更加友好的使用RDD的方式，它在底层的数据与具体的schema之间做了一个转化，让用户在使用的时候看到的是具体schema信息，使用者可以不用操心每次操作数据还要担心我要处理的当前字段的含义是否是我所需要的。 【讲】Spark框架几个角色资源层：Master、Worker计算层：Client、Driver、Executor 存储层：Hdfs或者S3 部署模式standalonemaster、worker 主从架构 - 主要是负责资源调度只支持client模式：Driver运行在本地 - main方法运行在本地 - client与Driver都在本地 yarnhdfs：NameNode、DataNode ResourceManager、NodeManager、ApplicationMasterspark的所有角色都是运行在NodeManager上，每个角色实际上就是一个JVM进程，所有进程都是运行在NodeManager节点上的Container里面 Container是对一组资源的抽象，包括内存、CPU、磁盘，网络等资源，但是没有做到CPU的隔离，所有进程都是共享的宿主机的CPUappMaster(Container)-Driver、Executor(Container)利用yarn的资源调度简化了spark自身的职能，master不需要自己维护集群中的资源使用情况，只负责接收client提交的任务 yarn是一个通用的资源调度平台，很多应用都可以运行在yarn上，因此，yarn管理的集群资源是一种更加广泛的资源，不止spark这一种应用所占用的资源。这样一来可以更加充分的利用整个集群的资源。同理一些其他的资源调度平台，如k8s等也是一样的道理 支持client和cluster模式cluster：Driver会运行在急群众的任意一个节点上 - client在执行spark-submit的节点 - 这种模式无法进行打印形式的debug，除非集群中可以看到运行Driver的那个节点的日志 k8s&amp;Mesos略 Spark中的一些概念 Term Meaning Application User program built on Spark. Consists of a driver program and executors on the cluster. Application jar A jar containing the user’s Spark application. In some cases users will want to create an “uber jar” containing their application along with its dependencies. The user’s jar should never include Hadoop or Spark libraries, however, these will be added at runtime. Driver program The process running the main() function of the application and creating the SparkContext Cluster manager An external service for acquiring resources on the cluster (e.g. standalone manager, Mesos, YARN) Deploy mode Distinguishes where the driver process runs. In “cluster” mode, the framework launches the driver inside of the cluster. In “client” mode, the submitter launches the driver outside of the cluster. Worker node Any node that can run application code in the cluster Executor A process launched for an application on a worker node, that runs tasks and keeps data in memory or disk storage across them. Each application has its own executors. Task A unit of work that will be sent to one executor Job A parallel computation consisting of multiple tasks that gets spawned in response to a Spark action (e.g. save, collect); you’ll see this term used in the driver’s logs. Stage Each job gets divided into smaller sets of tasks called stages that depend on each other (similar to the map and reduce stages in MapReduce); you’ll see this term used in the driver’s logs. job+stage+task 是逻辑上的概念 一个Job会根据rdd的dependency关系被切分成不同的stage(遇到宽依赖就会切分stage)，每个stage会根据分区器(partitioner)并结合系统的并行度计算出对应的分区数，每个分区实际上可以运行一个task，所以一个stage包含了一组task executor + thread 是物理上的概念 具体的task是运行在executor上的一个线程上，所以我们设置的一个executor的cores决定了每个executor最多可以并行多少个thread，也就是并行多少个task 所以我们才能在spark的web ui的Executors一栏中看到如上图的表示：我们会发现有两列 Cores和Active Tasks，就是对应了我上面说的那句话。 Spark2.4新特性spark2.4新增了一些特性，可以让spark SQL更加灵活使用，可以从官网文档上搜索Since: 2.4.0，下面罗列一些 array array_sort array_union arrays_overlap arrays_zip array_distinct array_except array_intersect array_join array_max array_min array_position array_remove array_repeat exists filter flatten shuffle slice array&amp;map element_at map map_concat map_from_arrays map_from_entries sequence sequence lambda transform zip_with aggregate：这个函数很有用，我们发现新特性里面并没有提供array_sum的方法，使用这个方法就可以实现😃 以上这些新的特性，在使用的时候非常方便，大家可以在自己的应用场景下尽可能的尝试，有些函数组合起来可以产生很强大的效果哦。比如，下面是一个组合使用sequence、transform、explode方法实现的一个自动生成日期序列的方式。 在某些场景下非常好用：系统中没有日期表，但是我们又需要一个日期序列来进行类似于cross join的操作，可以直接通过这些新特性直接实现。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061-- schema:-- date: 日期-- year: 年份-- month: 月份-- week1: 周的第一天(start from Monday)-- week2: 周的第一天(start from Sunday)-- quarter: 季度-- today: 今天的day_of_month-- is_curr_month: 是否为当月-- day 维度SELECT date[0] date, date[1] year, date[2] month, date[3] week1, date[4] quarter, date[5] week2 , date[6] next_month, date[7] last_month, date[8] today, date[9] is_curr_monthfrom ( SELECT explode( transform( sequence( date_sub(from_utc_timestamp(now(), 'Asia/Jakarta'), 7 * 52 + dayofweek(from_utc_timestamp(now(), 'Asia/Jakarta')) - 1), date_add(to_date(from_utc_timestamp(now(), 'Asia/Jakarta')), -1), interval 1 day ), x -&gt; array( x, to_date(date_trunc('YEAR', x)), to_date(date_trunc('MM', x)), to_date(date_trunc('WEEK', x)), to_date(date_trunc('QUARTER', x)), to_date(date_sub(next_day(x, 'Sun'), 7)), add_months(to_date(date_trunc('MM', x)), 1), add_months(to_date(date_trunc('MM', x)), -1), string(day(to_date(from_utc_timestamp(now(), 'Asia/Jakarta')))), string(to_date(date_trunc('MM', x))==date_trunc('MM', to_date(from_utc_timestamp(now(), 'Asia/Jakarta')))) ) ) ) date) tmp-- week 维度 SELECT EXPLODE( SEQUENCE( date_sub(from_utc_timestamp(now(), 'Asia/Jakarta'), 60), date_add(to_date(from_utc_timestamp(now(), 'Asia/Jakarta')), -1), INTERVAL 1 WEEK ) )-- month 维度select tmp.d[0] date from ( SELECT EXPLODE( transform( SEQUENCE( to_date('2018-09-30'), add_months(to_date(from_utc_timestamp(now(), 'Asia/Jakarta')), 1), INTERVAL 1 MONTH ), x -&gt; array( if(date_trunc('MM', x)=date_trunc('MM', to_date(from_utc_timestamp(now(), 'Asia/Jakarta'))), date_add(to_date(from_utc_timestamp(now(), 'Asia/Jakarta')), -1), x) ) ) ) d) tmp Spark启动参数可以通过SparkConf配置参数： spark.driver.memory spark.executor.memory spark.executor.cores spark.yarn.executor.memoryOverhead spark.executor.memoryOverhead spark.sql.shuffle.partitions spark.default.parallelism spark.port.maxRetries spark.sql.legacy.parser.havingWithoutGroupByAsWhere spark.maxRemoteBlockSizeFetchToMem spark.sql.sources.partitionOverwriteMode spark.dynamicAllocation.maxExecutors spark.sql.autoBroadcastJoinThreshold spark.master spark.ui.killEnabled spark启动参数的优先级：SparkConf &gt; spark-submit 或 spark-shell &gt;spark-defaults.conf","categories":[],"tags":[]},{"title":"Python学习-高级函数","slug":"Python学习-高级函数","date":"2021-05-03T23:16:09.000Z","updated":"2021-05-03T23:34:51.912Z","comments":true,"path":"post/Python学习-高级函数/","link":"","permalink":"https://shang.at/post/Python学习-高级函数/","excerpt":"简介：","text":"简介： map1map(func, *iterables) --&gt; map object Make an iterator that computes the function using arguments from each of the iterables. Stops when the shortest iterable is exhausted. reduce12from functools import reducereduce(function, sequence[, initial]) -&gt; value Apply a function of two arguments cumulatively to the items of a sequence, from left to right, so as to reduce the sequence to a single value. For example, reduce(lambda x, y: x+y, [1, 2, 3, 4, 5]) calculates ((((1+2)+3)+4)+5). If initial is present, it is placed before the items of the sequence in the calculation, and serves as a default when the sequence is empty. filter1filter(function or None, iterable) --&gt; filter object sum1sum(iterable, start=0, /) 列表含有子列表展开成一个列表，列表的扁平化 1234listA = [['a'], ['b'], ['c', 'd']] listA1 = sum(listA, []) # ['a', 'b', 'c', 'd'] 或者使用reduce也是可以的： 12345from functools import reducea=[[1,2,3],[4,5,],[6]]a=reduce(lambda x,y:x+y,a)a# [1, 2, 3, 4, 5, 6]","categories":[],"tags":[]},{"title":"Spark学习-udf","slug":"Spark学习-udf","date":"2021-04-27T02:45:15.000Z","updated":"2021-05-21T10:03:08.428Z","comments":true,"path":"post/Spark学习-udf/","link":"","permalink":"https://shang.at/post/Spark学习-udf/","excerpt":"简介：","text":"简介： spark的udf应该如何使用？ 声明的位置不对会不会有什么影响？ spark中的闭包(closure-cleaned) 123456789101112131415161718192021222324252627282930313233Helper method to clean the given closure in place.The mechanism is to traverse the hierarchy of enclosing closures and null out anyreferences along the way that are not actually used by the starting closure, but arenevertheless included in the compiled anonymous classes. Note that it is unsafe tosimply mutate the enclosing closures in place, as other code paths may depend on them.Instead, we clone each enclosing closure and set the parent pointers accordingly.By default, closures are cleaned transitively. This means we detect whether enclosingobjects are actually referenced by the starting one, either directly or transitively,and, if not, sever these closures from the hierarchy. In other words, in addition tonulling out unused field references, we also null out any parent pointers that referto enclosing objects not actually needed by the starting closure. We determinetransitivity by tracing through the tree of all methods ultimately invoked by theinner closure and record all the fields referenced in the process.For instance, transitive cleaning is necessary in the following scenario: class SomethingNotSerializable &#123; def someValue = 1 def scope(name: String)(body: =&gt; Unit) = body def someMethod(): Unit = scope(&quot;one&quot;) &#123; def x = someValue def y = 2 scope(&quot;two&quot;) &#123; println(y + 1) &#125; &#125; &#125;In this example, scope &quot;two&quot; is not serializable because it references scope &quot;one&quot;, whichreferences SomethingNotSerializable. Note that, however, the body of scope &quot;two&quot; does notactually depend on SomethingNotSerializable. This means we can safely null out the parentpointer of a cloned scope &quot;one&quot; and set it the parent of scope &quot;two&quot;, such that scope &quot;two&quot;no longer references SomethingNotSerializable transitively 匿名函数 - lambda函数","categories":[],"tags":[]},{"title":"Spark学习-map-struct-array","slug":"Spark学习-map-struct-array","date":"2021-04-26T02:50:00.000Z","updated":"2021-04-26T03:38:35.509Z","comments":true,"path":"post/Spark学习-map-struct-array/","link":"","permalink":"https://shang.at/post/Spark学习-map-struct-array/","excerpt":"简介：","text":"简介： 关于spark中的map、struct、array的一些操作 首先创建一个dataframe 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263import org.apache.spark.sql.expressions.UserDefinedFunctionimport org.apache.spark.sql.functions._import org.apache.spark.sql.&#123;DataFrame, Row, SparkSession&#125;val sparkConf = new SparkConfsparkConf.set(\"spark.maxRemoteBlockSizeFetchToMem\", 1024 * 1024 * 256 toString)sparkConf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")sparkConf.set(\"spark.master\", \"local[*]\")lazy val spark: SparkSession = &#123; SparkSession.builder() .config(sparkConf) .appName(appName) .enableHiveSupport() .getOrCreate()&#125;import spark.implicits._case class TestStruct(f1: String, f2: String)def testUdf: UserDefinedFunction = udf((a: String) =&gt; &#123; List(TestStruct(a, \"f1\"), TestStruct(s\"$a.00\", \"f2\"))&#125;)def testUdf1: UserDefinedFunction = udf((a: String) =&gt; &#123; TestStruct(a, \"f1\")&#125;)def testUdf2: UserDefinedFunction = udf((a: String) =&gt; &#123; Map(\"key\" -&gt; a, \"key1\" -&gt; a)&#125;)val lines: List[(String, Int, Int, Map[String, String])] = List( Tuple4(\"a\", 1, 2, Map(\"a\" -&gt; \"3\", \"c\" -&gt; \"a\")), Tuple4(\"b\", 11, 22, Map(\"a\" -&gt; \"33\", \"c\" -&gt; \"aa\")), Tuple4(\"c\", 111, 2222, Map(\"a\" -&gt; \"3333\", \"c\" -&gt; \"aaa\")))lines.toDF( \"f1\", \"f2\", \"f3\", \"extra\" ).withColumn( \"struct_array\", testUdf('f1) ).withColumn( \"map\", testUdf2('f1) ).withColumn( \"struct\", testUdf1('f1) ).withColumn( \"extra_a\", $\"extra.a\" ).withColumn( \"extra_c\", $\"extra.c\" ).withColumn( \"extra_d\", $\"extra.d\" ).withColumn( \"struct_array_f1\", $\"struct_array.f1\" ).withColumn( \"struct_f1\", $\"struct.f1\" ).withColumn( \"struct_f2\", expr(\"struct['f2']\") ).withColumn( \"struct_array0\", expr(\"struct_array[0]\") ).withColumn( \"struct_array0_f1\", expr(\"struct_array[0].f1\") ) 打印上述dataframe的schema如下： 12345678910111213141516171819202122232425262728root |-- f1: string (nullable = true) |-- f2: integer (nullable = true) |-- f3: integer (nullable = true) |-- extra: map (nullable = true) | |-- key: string | |-- value: string (valueContainsNull = true) |-- struct_array: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- f1: string (nullable = true) | | |-- f2: string (nullable = true) |-- map: map (nullable = true) | |-- key: string | |-- value: string (valueContainsNull = true) |-- struct: struct (nullable = true) | |-- f1: string (nullable = true) | |-- f2: string (nullable = true) |-- extra_a: string (nullable = true) |-- extra_c: string (nullable = true) |-- extra_d: string (nullable = true) |-- struct_array_f1: array (nullable = true) | |-- element: string (containsNull = true) |-- struct_f1: string (nullable = true) |-- struct_f2: string (nullable = true) |-- struct_array0: struct (nullable = true) | |-- f1: string (nullable = true) | |-- f2: string (nullable = true) |-- struct_array0_f1: string (nullable = true) 读取这个表数据，打印出来如下： 结论： map结构的数据如果想使用col或者$操作符取value，可以直接通过.取到对应的值，如果map中没有对应的key，则返回None； 不能使用类似col(&quot;extra[&#39;a&#39;]&quot;)或者\"extra['a']\"`的方式取`value`，也可以想象：`col`或者`操作符最终返回的是一个Column对象，它就是一个货真价实的Column，不是那种需要计算得来的Column 要想使用extra[&#39;a&#39;]的形式取值，需要使用expr操作符，如expr(&quot;extra[&#39;a&#39;]&quot;)，顾名思义，expr接受的是一个语句，它会解释并执行它，返回一个Column struct结构的字段，取法和map结构基本一致，但是人一上来就会想到用.来取值，而不是用[&#39;a&#39;]的方式来取值，是因为它是一个定义清晰的结构体，字段定义清晰，但是expr(struct[&#39;f2&#39;])这种取值方式是可以的 对于一个array类型的字段，如果想要取第i个元素，那么必须要使用expr操作符，比如：struct_array[0]或struct_array[0].f1 如果array的每个元素是一个结构体，取值时不需要考虑取哪一个，直接使用.取某个字段，则返回的是一个对应的字段的array，比如：struct_array.f1 在sparkSQL中不需要忌讳以上所有的问题，想怎么取就怎么取，因为它本身就是一个expr，他可以兼容所有的情况^_^ 意外收获： 如果使用scala来编写spark的程序，那么创建map和struct类型的字段，就十分简单了。创建一个udf，然后balala一通，udf返回的是case class就会形成一个struct类型的字段，udf返回的是一个Map就会形成一个map类型的字段，比如上面的testUdf2、testUdf1、testUdf","categories":[],"tags":[]},{"title":"Spark学习-异常分析","slug":"Spark学习-异常分析","date":"2021-04-22T00:16:02.000Z","updated":"2021-05-19T02:55:33.037Z","comments":true,"path":"post/Spark学习-异常分析/","link":"","permalink":"https://shang.at/post/Spark学习-异常分析/","excerpt":"简介：","text":"简介： broadcast引发的OOMSpark-error.log coalesce 引发的OOMSpark-error.log https://stackoverflow.com/questions/31610971/spark-repartition-vs-coalesce/65038141#65038141 https://stackoverflow.com/questions/31674530/write-single-csv-file-using-spark-csv https://stackoverflow.com/questions/38961251/java-lang-outofmemoryerror-unable-to-acquire-100-bytes-of-memory-got-0 test coalesce(1) coalesce(2) repartition(1) join之后执行coalesce、filter之后执行coalesce都会 Spark - repartition() vs coalesce()：https://stackoverflow.com/questions/31610971/spark-repartition-vs-coalesce/65038141#65038141 查看执行计划的好地方spark web ui的 SQL 页 Task not serializablewhat is java.io.NotSerializableException？and why？and when？","categories":[],"tags":[]},{"title":"Python数据分析-matplotlib学习","slug":"Python数据分析-matplotlib学习","date":"2021-04-14T22:58:12.000Z","updated":"2021-04-14T23:19:47.528Z","comments":true,"path":"post/Python数据分析-matplotlib学习/","link":"","permalink":"https://shang.at/post/Python数据分析-matplotlib学习/","excerpt":"简介：","text":"简介： 案例matplotlib画等高线图1234567891011121314151617181920212223import numpy as npimport pandas as pdimport matplotlib.pyplot as plt# 计算x,y坐标对应的高度值def f(x, y): return (1-x/2+x**5+y**3) * np.exp(-x**2-y**2)# 生成x,y的数据n = 256x = np.linspace(-3, 3, n)y = np.linspace(-3, 3, n)# 把x,y数据生成mesh网格状的数据，因为等高线的显示是在网格的基础上添加上高度值X, Y = np.meshgrid(x, y)# 填充等高线plt.contourf(X, Y, f(X, Y), 20, cmap=plt.cm.hot)# 添加等高线C = plt.contour(X, Y, f(X, Y), 20)plt.clabel(C, inline=True, fontsize=12)# 显示图表plt.show() 使用 GridSpec 绘制非均匀子图1234567891011121314151617181920212223242526272829303132333435# 图的比例大小不同但没有跨行或跨列fig = plt.figure(figsize=(6, 6))spec = fig.add_gridspec(nrows=2, ncols=2, width_ratios=[5,1], height_ratios=[1,5])fig.suptitle('数据的散点图和边际分布', size=20)# 绘制第一个x轴上的直方图ax = fig.add_subplot(spec[0,0])ax.hist(data[0])ax.xaxis.set_visible(False) # 隐藏x坐标轴显示ax.yaxis.set_visible(False) # 隐藏y坐标轴显示# 隐藏边框for loc in ['top', 'right', 'bottom', 'left']: ax.spines[loc].set_visible(False)# ax.spines['top'].set_visible(False)# ax.spines['right'].set_visible(False)# ax.spines['bottom'].set_visible(False)# ax.spines['left'].set_visible(False)# 绘制散点图ax = fig.add_subplot(spec[1,0])ax.scatter(data[0], data[1])ax.grid(True) # 添加网格显示# 绘制y轴上的直方图ax = fig.add_subplot(spec[1,1])ax.hist(data[1], orientation='horizontal') # orientation设置水平显示，默认是垂直显示的ax.xaxis.set_visible(False) # 隐藏x坐标轴显示ax.yaxis.set_visible(False) # 隐藏y坐标轴显示# 隐藏边框for loc in ['top', 'right', 'bottom', 'left']: ax.spines[loc].set_visible(False)fig.tight_layout() 1234567891011121314151617181920212223242526272829# 图为跨列或跨行状态fig = plt.figure(figsize=(6, 6))spec = fig.add_gridspec(nrows=6, ncols=6, width_ratios=[1,1,1,1,1,1], height_ratios=[1,1,1,1,1,1])fig.suptitle('数据的散点图和边际分布', size=20)ax = fig.add_subplot(spec[0,0:5])ax.hist(data[0])ax.xaxis.set_visible(False)ax.yaxis.set_visible(False)for loc in ['top', 'right', 'bottom', 'left']: ax.spines[loc].set_visible(False)# ax.spines['top'].set_visible(False)# ax.spines['right'].set_visible(False)# ax.spines['bottom'].set_visible(False)# ax.spines['left'].set_visible(False)ax = fig.add_subplot(spec[1:5,0:5])ax.scatter(data[0], data[1])ax.grid(True)ax = fig.add_subplot(spec[1:5,5])ax.hist(data[1], orientation='horizontal')ax.xaxis.set_visible(False)ax.yaxis.set_visible(False)for loc in ['top', 'right', 'bottom', 'left']: ax.spines[loc].set_visible(False)fig.tight_layout()","categories":[{"name":"Python数据分析","slug":"Python数据分析","permalink":"https://shang.at/categories/Python数据分析/"}],"tags":[{"name":"Matplotlib","slug":"Matplotlib","permalink":"https://shang.at/tags/Matplotlib/"}]},{"title":"Python数据分析-numpy学习","slug":"Python数据分析-numpy学习","date":"2021-04-14T22:50:01.000Z","updated":"2021-04-14T22:56:22.990Z","comments":true,"path":"post/Python数据分析-numpy学习/","link":"","permalink":"https://shang.at/post/Python数据分析-numpy学习/","excerpt":"简介：","text":"简介： np.newaxis 功能:在行上或列上新增一个维度 12345678910111213141516171819202122a=np.array([1,2,3,4,5])print(a.shape)print (a)#(5,)#[1 2 3 4 5]a=np.array([1,2,3,4,5])aa=a[:,np.newaxis]print(aa.shape)print (aa)#(5, 1)#[[1]#[2]#[3]#[4]#[5]]a=np.array([1,2,3,4,5])aa=a[np.newaxis,:]print(aa.shape)print (aa)#(1, 5)","categories":[{"name":"Python数据分析","slug":"Python数据分析","permalink":"https://shang.at/categories/Python数据分析/"}],"tags":[{"name":"Numpy","slug":"Numpy","permalink":"https://shang.at/tags/Numpy/"}]},{"title":"Scala学习-编程Tips-类型转换","slug":"Scala学习-编程Tips-类型转换","date":"2021-04-14T08:42:26.000Z","updated":"2021-04-14T08:47:14.800Z","comments":true,"path":"post/Scala学习-编程Tips-类型转换/","link":"","permalink":"https://shang.at/post/Scala学习-编程Tips-类型转换/","excerpt":"简介：","text":"简介： 如果实例化了子类的对象，但是将其赋予了父类类型的变量，在后续的过程中，又需要将父类类型的变量转换为子类类型的变量，应该如何做？ Ø 首先，需要使用isInstanceOf 判断对象是否为指定类的对象，如果是的话，则可以使用 asInstanceOf 将对象转换为指定类型； Ø 注意：p.isInstanceOf[XX] 判断 p 是否为 XX 对象的实例；p.asInstanceOf[XX] 把 p 转换成 XX 对象的实例 Ø 注意：如果没有用isInstanceOf 先判断对象是否为指定类的实例，就直接用 asInstanceOf 转换，则可能会抛出异常； Ø 注意：如果对象是 null，则isInstanceOf 一定返回 false， asInstanceOf 一定返回 null； Ø Scala与Java类型检查和转换 Scala Java obj.isInstanceOf[C] obj instanceof C obj.asInstanceOf[C] (C)obj classOf[C] C.class Ø 举例说明： 123456789101112131415161718package cn.itcast.extends_democlass Person3 &#123;&#125;class Student3 extends Person3object Student3&#123; def main (args: Array[String] ) &#123; val p: Person3 = new Student3 var s: Student3 = null //如果对象是 null，则 isInstanceOf 一定返回 false println (s.isInstanceOf[Student3]) // 判断 p 是否为 Student3 对象的实例 if (p.isInstanceOf[Student3] ) &#123; //把 p 转换成 Student3 对象的实例 s = p.asInstanceOf[Student3] &#125; println (s.isInstanceOf[Student3] ) &#125;&#125; Scala中getClass 和 classOfØ isInstanceOf 能判断出对象是否为指定类以及其子类的对象，而不能精确的判断出，对象就是指定类的对象； Ø 如果要求精确地判断出对象就是指定类的对象，那么就只能使用 getClass 和 classOf 了； Ø p.getClass 可以精确地获取对象的类，classOf[XX]可以精确的获取类，然后使用 == 操作符即可判断； Ø 举例说明： 123456789101112131415package cn.itcast.extends_democlass Person4 &#123;&#125;class Student4 extends Person4object Student4&#123; def main(args: Array[String]) &#123; val p:Person4=new Student4 //判断p是否为Person4类的实例 println(p.isInstanceOf[Person4])//true //判断p的类型是否为Person4类 println(p.getClass == classOf[Person4])//false //判断p的类型是否为Student4类 println(p.getClass == classOf[Student4])//true &#125;&#125;","categories":[],"tags":[]},{"title":"Scala学习-编程Tips-异常处理","slug":"Scala学习-编程Tips-异常处理","date":"2021-04-12T09:33:21.000Z","updated":"2021-05-12T02:41:49.935Z","comments":true,"path":"post/Scala学习-编程Tips-异常处理/","link":"","permalink":"https://shang.at/post/Scala学习-编程Tips-异常处理/","excerpt":"简介：","text":"简介： 0.异常处理来看看下面一段代码 123456789 def main(args: Array[String]): Unit = &#123; val i = 10 / 0 println(\"你好！\") &#125;Exception in thread \"main\" java.lang.ArithmeticException: / by zero at ForDemo$.main(ForDemo.scala:3) at ForDemo.main(ForDemo.scala) 执行程序，可以看到scala抛出了异常，而且没有打印出来”你好”。说明程序出现错误后就终止了。 那怎么解决该问题呢？ 在scala中，可以使用异常处理来解决这个问题 1.捕获异常语法格式 12345678910try &#123; // 代码&#125;catch &#123; case ex:异常类型1 =&gt; // 代码 case ex:异常类型2 =&gt; // 代码&#125;finally &#123; // 代码&#125; 尖叫提示： Scala中捕捉异常的catch子句，语法与其他语言中不太一样。在Scala里，借用了模式匹配的思想来做异常的匹配，因此，在catch的代码里，是一系列case字句 try中的代码是我们编写的业务处理代码 在catch中表示当出现某个异常时，需要执行的代码 在finally中，是不管是否出现异常都会执行的代码 示例说明 使用try..catch来捕获除零异常 参考代码 123456789101112131415161718192021222324package com.robot.scalademoobject _08ObjectDemo &#123; def main(args: Array[String]): Unit = &#123; try &#123; val i = 10 / 0 println(\"你好！\") &#125; catch &#123; //case ex: Exception =&gt; println(ex.getMessage) // / by zero case ex:Exception =&gt; ex.printStackTrace() //调用自带方法打印,case只执行一个。 &#125; finally &#123; println(\"我一定会执行，不管异常上面发生了什么\") &#125; &#125;&#125;/*** java.lang.ArithmeticException: / by zero at com.robot.scalademo._08ObjectDemo$.main(_08ObjectDemo.scala:7) at com.robot.scalademo._08ObjectDemo.main(_08ObjectDemo.scala) 我一定会执行，不管异常上面发生了什么 */ 2.抛出异常我们也可以在一个方法中，抛出异常。语法格式和Java类似，使用throw new Exception... 示例说明 在main方法中抛出一个异常 参考代码 12345678910111213141516171819202122232425262728293031//scala主动抛出异常package com.robot.scalademoobject _08ObjectDemo &#123; def main(args: Array[String]): Unit = &#123; throw new Exception(\"这是一个异常\") &#125;&#125;/***Exception in thread \"main\" java.lang.Exception: 这是一个异常 at com.robot.scalademo._08ObjectDemo$.main(_08ObjectDemo.scala:6) at com.robot.scalademo._08ObjectDemo.main(_08ObjectDemo.scala) *///2.Scala捕获主动抛出的异常package com.robot.scalademoobject _08ObjectDemo &#123; def main(args: Array[String]): Unit =&#123; try &#123; throw new Exception(\"这是一个异常\") &#125;catch &#123; case ex:Exception =&gt; print(\"哈哈，被我捕获了\") &#125; &#125;&#125;/***哈哈，被我捕获了 */ 尖叫提示：注意在scala不需要在方法上声明要抛出的异常，它已经解决了再Java中被认为是设计失败的检查型异常。 12345&lt;details&gt;&lt;summary&gt;&lt;strong&gt;Program Structure&lt;/strong&gt;&lt;/summary&gt;Note that it is possible to cascade one set of time windows after another, so long as the timeframes are compatible (the second set of windows needs to have a duration that is a multiple of the first set). So you can have a initial set of hour-long windows that is keyed by the `driverId` and use this to create a stream of `(endOfHourTimestamp, driverId, totalTips)`, and then follow this with another hour-long window (this window is not keyed) that finds the record from the first window with the maximum `totalTips`.&lt;/details&gt;","categories":[],"tags":[]},{"title":"Scala学习-编程Tips-特殊符号操作符","slug":"Scala学习-编程Tips-特殊符号操作符","date":"2021-04-12T06:07:30.000Z","updated":"2021-04-12T23:19:40.060Z","comments":true,"path":"post/Scala学习-编程Tips-特殊符号操作符/","link":"","permalink":"https://shang.at/post/Scala学习-编程Tips-特殊符号操作符/","excerpt":"简介：","text":"简介： 摘要：本文汇总在 scala 中遇到各种符号操作符。 模式匹配中使用 @这个 @ 的主要功能是在模式匹配中，匹配到一个模式后，但是在处理过程中，使用对象本身而不是匹配后的元素。 案例1：匹配 Some()参考该 Overflow 回答 当我们匹配对象是否为 Some(x) 的时候，如果没有 @ 那么最后我们在匹配中的值将是 Some(x) 中的 x， 而如果加入 @，那么最后匹配的将会是 Some(x) It enables one to bind a matched pattern to a variable. Consider the following, for instance: 1val o: Option[Int] = Some(2) You can easily extract the content: 1234o match &#123; case Some(x) =&gt; println(x) case None =&gt;&#125; But what if you wanted not the content of Some, but the option itself? That would be accomplished with this: 1234o match &#123; case x @ Some(_) =&gt; println(x) case None =&gt;&#125; Note that @ can be used at any level, not just at the top level of the matching. 案例2：API 匹配 Request123456789def intent = &#123; case req @ GET(Path(Seg(\"api\" :: \"user\" :: IntPathElement(userId) :: Nil))) =&gt; val f = (userManager ? FindUserById(userId)) respond(f, req) case req @ GET(Path(Seg(\"api\" :: \"user\" :: Nil))) &amp; Params(EmailParam(email)) =&gt; val f = (userManager ? FindUserByEmail(email)) respond(f, req) &#125; 在处理 request 请求的时候，需要匹配请求的路径，然后还需要直接使用 requet。 下划线 _ 的使用场景import 通配符1import org.apache.spark.SparkContext._ 集合操作指代每一个元素在所有的集合操作中都可以使用下划线指代集合内容。当前的集合操作只有一个参数时，可以使用_来代替，不用非要起一个别名 123456object Sample &#123; def main (args: Array[String])&#123; val newArry= (1 to 10).map(_*2) println(newArry) &#125;&#125; 在一个 Array a中筛出偶数，并乘以2 1a.filter(_%2==0).map(2*_) 模式匹配使用主要用于匹配通配的情形 123456789Some(5) match &#123; case Some(_) =&gt; println(\"Yes\") &#125;match &#123; case List(1,_,_) =&gt; \" a list with three element and the first element is 1\" case List(_*) =&gt; \" a list with zero or more elements \" case Map[_,_] =&gt; \" matches a map with any key type and any value type \" case _ =&gt; &#125;val (a, _) = (1, 2)for (_ &lt;- 1 to 10) 其他案例 12val m = Map(1 -&gt; 2,2 -&gt; 4)for ((k,_) &lt;- m) println(k) //如果不需要所有部件， 则在不需要的部件使用_； 本例只取key,因此在value处用_ 变量初始化为 null123456object Sample &#123; var name:String=_ def main (args: Array[String])&#123; name=\"hello world\" println(name)&#125; 在这里，name 也可以声明为 null，例：var name:String=null。这里的下划线和 null 的作用是一样的。 :_* 参数序列处理:_* 作为一个整体，告诉编译器你希望将某个参数当作参数序列处理！ 例如，当函数接收的参数不定长的时候，假如你想输入一个队列，可以在一个队列后加入“:_*”，因此，这里的“1 to 5”也可以改写为：“Seq(1,2,3,4,5)”。 1234567891011object Sample &#123; def main (args: Array[String])=&#123; val result=sum(1 to 5:_*) println(result) &#125; def sum(parms:Int*)=&#123; var result=0 for(parm &lt;- parms)result+=parm result &#125;&#125; 元组访问成员在元组中，可以用方法_1, _2, _3访问组员。如 a._2。其中点操作符可以用空格替代，也就是 a _2。 1234567object Sample &#123; def main (args: Array[String])=&#123; val value=(1,2) println(value._1) println(value _2) &#125;&#125; 匿名函数 =&gt;这个比较简单，因为 Java8 的 Lambda 已经广泛使用。 =&gt; 匿名函数（Anonymous Functions）： 表示创建一个函数实例。 比如：(x: Int) =&gt; x + 1 和如下JAVA方法表示的含义一样： 123public int function(int x)&#123; return x+1;&#125; 可以这么理解： =&gt;左边 是输入参数，: 后面 Int 是参数类型 =&gt;右边 当作函数体, 类似匿名函数的 {} 模式匹配中配合 case12345678val bools = Seq(true, false)for (bool &lt;- bools) &#123; bool match &#123; case true =&gt; println(\"Got heads\") case false =&gt; println(\"Got tails\") &#125;&#125; 集合遍历 &lt;-简单说这是一个集合遍历的方法： 1234var list = Array(1,2,3,4)for (aa &lt;- list) &#123; printf(aa+\" \")&#125; 但是在 for 推导式中，这个符号表示生成器，其解释比循环更加复杂，是一个 scala 的语法糖，最后会被解释为一系列的容器操作：map 和 flatMap 等。参考另一篇详细介绍 Scala For 推导式的文章。 集合拼接 ++= --= 等操作参考官方集合 Set 文档 ++= 用于拼接容器，而 += 用于拼接元素。 WHAT IT IS WHAT IT DOES 加法： xs += x 把元素 x 添加到集合 xs 中。该操作有副作用，它会返回左操作符，这里是 xs 自身。 xs += (x, y, z) 添加指定的元素到集合 xs 中，并返回 xs 本身。（同样有副作用） xs ++= ys 添加集合 ys 中的所有元素到集合 xs 中，并返回 xs 本身。（表达式有副作用） xs add x 把元素 x 添加到集合 xs 中，如集合 xs 之前没有包含 x，该操作返回 true，否则返回 false。 移除： xs -= x 从集合 xs 中删除元素 x，并返回 xs 本身。（表达式有副作用） xs -= (x, y, z) 从集合 xs 中删除指定的元素，并返回 xs 本身。（表达式有副作用） xs --= ys 从集合 xs 中删除所有属于集合 ys 的元素，并返回 xs 本身。（表达式有副作用） xs remove x 从集合 xs 中删除元素 x 。如之前 xs 中包含了 x 元素，返回 true，否则返回 false。 xs retain p 只保留集合 xs 中满足条件 p 的元素。 xs.clear() 删除集合 xs 中的所有元素。 更新： xs(x) = b （ 同 xs.update(x, b) ）参数 b 为布尔类型，如果值为 true 就把元素x加入集合 xs，否则从集合 xs 中删除 x。 克隆： xs.clone 产生一个与 xs 具有相同元素的可变集合。 冒号操作符:::运算符三个冒号表示List的连接操作，比如： 123val a = List(1,2)val b = List(3,4)val c = a:::b //c=List(1,2,3,4) ::两个冒号两个冒号表示普通元素与List的连接操作，比如： 123val a=1 val b=List(66,88) val c = 1::b //c=List(1,66,88) 元组操作 -&gt;scala中元组含义： 元组是不同类型的值聚集线程的列表通过将多个值使用小括号括起来，即表示元组 scala中元组与数组区别 数组中元素 数据类型必须一样，但是元组数据类型可以不同。 123456val first = (1,2,3) // 定义三元元组val one = 1val two = 2val three = one -&gt; twoprintln(three) // 构造二元元组println(three._2) // 访问二元元组中第二个值 上下界约束符 &lt;: 与 &gt;:这对符号个人觉得是里面最好理解的了，这对符号用于写范型类/函数时约束范型类型。 123456def using[A &lt;: Closeable, B](closeable: A) (getB: A =&gt; B): B = try &#123; getB(closeable) &#125; finally &#123; closeable.close() &#125; 例子中 A &lt;: Closeable(java.io.Cloaseable) 的意思就是保证类型参数 A 是 Closeable 的子类（含本类）。语法“A &lt;: B“定义了B为A的上界；同理相反的 A &gt;: B 的意思就是A是B的超类（含本类），定义了B为A的下界。其实 &lt;: 和 &gt;: 就等价于java范型编程中的 extends，super。 协变与逆变符号+T， -T“协变”是指能够使用与原始指定的派生类型相比，派生程度更大的类型。e.g. String =&gt; AnyRef “逆变”则是指能够使用派生程度更小的类型。e.g. AnyRef =&gt; String 【+T】表示协变，【-T】表示逆变 参考 《Scala 程序设计第2版》 2.13 抽象类型和参数化类型： Scala 支持参数化类型，与 Java 中的泛型十分类似。这两个术语，但 Scala 社区中多使用“参数化类型”， Java 社区中常用泛型一词。）在语法上， Java 使用尖括号（ &lt;… &gt;），而 Scala 使用方括号（ [… ]），因为在 Scala 中 &lt; 和 &gt; 常用作方法名。 例如，字符串列表可以声明如下： 1val strings: List[String] = List(\"one\", \"two\", \"three\") 由于我们可以在集合 List[A] 中使用任何类型作为类型 A，这种特性被称为参数多态。在方法 List 的通用实现中，允许使用任何类型的实例作为 List 的元素。 A 之前的 + 表示：如果 B 是 A 的子类，则 List[B] 也是 List[A] 的子类型，这被称为协类型。协类型很符合直觉，如果我们有一个函数 f(list: List[Any])，那么传递 List[String] 给这个函数，也应该能正常工作。 如果类型参数前有 -，则表示另一种关系：如果 B 是 A 的子类型，且 Foo[A] 被声明为 Foo[-A]，则 Foo[B] 是 Foo[A] 的父类型（称为逆类型）。这一机制没那么符合直觉，我们将在参数化类型中与参数化类型的其他细节一起解释这一点。 Scala 还支持另一种被称为“抽象类型”的抽象机制，它可以运用在许多参数化类型中，也能够解决设计上的问题。然而，尽管两种机制有所重合，但并不冗余，两种机制对不同的设计问题各有优势与不足。 view bounds(视界) 与 &lt;%&lt;%的意思是“view bounds”(视界)，它比&lt;:适用的范围更广，除了所有的子类型，还允许隐式转换过去的类型 1def method [A &lt;% B](arglist): R = ... 等价于 1def method [A](arglist)(implicit viewAB: A =&gt; B): R = ... 表示 A 可以视为类型 B 案例2： 视界，就像类型边界，要求存在一个能够将某类型转换为指定类型的函数。你可以使用 &lt;% 指定类型限制，例如： 12scala&gt; class Container[A &lt;% Int] &#123; def addIt(x: A) = 123 + x &#125;defined class Container 这是说 A 必须“可被视作” Int 。让我们试试。 12345678910scala&gt; (new Container[String]).addIt(\"123\")res11: Int = 246scala&gt; (new Container[Int]).addIt(123) res12: Int = 246scala&gt; (new Container[Float]).addIt(123.2F)&lt;console&gt;:8: error: could not find implicit value for evidence parameter of type (Float) =&gt; Int (new Container[Float]).addIt(123.2) ^ &lt;% 除了方法使用之外，class声明类型参数时也可使用： 12scala&gt; class A[T &lt;% Int]defined class A 但无法对trait的类型参数使用 &lt;%， 12scala&gt; trait A[T &lt;% Int]&lt;console&gt;:1: error: traits cannot have type parameters with context bounds `: ...' nor view bounds `&lt;% ...' 广义类型约束符号 =:=, &lt;:&lt;, &lt;%&lt;方法可以通过隐式参数执行更复杂的类型限制。例如，List 支持对数字内容执行 sum，但对其他内容却不行。可是 Scala 的数字类型并不都共享一个超类，所以我们不能使用T &lt;: Number。相反，要使之能工作，Scala的math库对适当的类型T 定义了一个隐含的 Numeric[T]。 然后在 List 定义中使用它： 1sum[B &gt;: A](implicit num: Numeric[B]): B 如果你调用 List(1,2).sum()，你并不需要传入一个 num 参数；它是隐式设置的。但如果你调用 List(&quot;whoop&quot;).sum()，它会抱怨无法设置 num。 在没有设定陌生的对象为 Numeric 的时候，方法可能会要求某种特定类型的“证据”。这时可以使用以下类型-关系运算符： A =:= B A 必须和 B相等A &lt;:&lt; B A 必须是 B的子类A &lt;%&lt; B A 必须可以被看做是 B （如果你在尝试使用 &lt;:&lt; 或者 &lt;%&lt; 的时候出错了，那请注意这些符号在 Scala 2.10 中被移除了。Scala School 里的例子仅能在 Scala 2.9.x 下正常工作。你可以使用新版本的 Scala，但可能会遇到错误。） 123456789101112131415scala&gt; class Container[A](value: A) &#123; def addIt(implicit evidence: A =:= Int) = 123 + value &#125;defined class Containerscala&gt; (new Container(123)).addItres11: Int = 246scala&gt; (new Container(\"123\")).addIt&lt;console&gt;:10: error: could not find implicit value for parameter evidence: =:=[java.lang.String,Int]// 类似地，根据之前的隐式转换，我们可以将约束放松为可视性：scala&gt; class Container[A](value: A) &#123; def addIt(implicit evidence: A &lt;%&lt; Int) = 123 + value &#125;defined class Containerscala&gt; (new Container(\"123\")).addItres15: Int = 246 字符串插值 s&quot;$c&quot;参考官方文档：字符串插值 s 字符串插值器1234val name=\"James\"println(s\"Hello,$name\") //Hello,James 此例中，$name嵌套在一个将被s字符串插值器处理的字符串中。插值器知道在这个字符串的这个地方应该插入这个name变量的值，以使输出字符串为Hello,James。使用s插值器，在这个字符串中可以使用任何在处理范围内的名字。 println(s\"1+1=$&#123;1+1&#125;\") //将会输出字符串1+1=2。任何表达式都可以嵌入到$&#123;&#125;中。 f 插值器在任何字符串字面前加上 f，就可以生成简单的格式化串，功能相似于其他语言中的 printf 函数。当使用 f 插值器的时候，所有的变量引用都应当后跟一个printf-style格式的字符串，如%d。看下面这个例子： 123456789101112val height=1.9dval name=\"James\"println(f\"$name%s is $height%2.2f meters tall\")//James is 1.90 meters tall f 插值器是类型安全的。如果试图向只支持 int 的格式化串传入一个double 值，编译器则会报错。例如：val height:Double=1.9dscala&gt;f\"$height%4d\"&lt;console&gt;:9: error: type mismatch; found : Double required: Int f\"$height%4d\" ^ f 插值器利用了java中的字符串数据格式。这种以%开头的格式在 raw 插值器除了对字面值中的字符不做编码外，raw 插值器与 s 插值器在功能上是相同的。如下是个被处理过的字符串： 123456789scala&gt;s\"a\\nb\"res0:String=ab // 这里，s 插值器用回车代替了\\n。而raw插值器却不会如此处理。scala&gt;raw\"a\\nb\"res1:String=a\\nb // 当不想输入\\n被转换为回车的时候，raw 插值器是非常实用的。 Akka 相关特殊字符send !12345case ArticleBody(uri, body) =&gt; //If we get the parsed article back, then we've just parsed it cacheActor ! SetRequest(uri, body) //Cache it as we just parsed it senderRef ! body context.stop(self) ask ?1val future = pongActor ? \"unknown\" 参考文章浅谈 Scala 中下划线的用途 Scala中的下划线到底有多少种应用场景 细数Scala下划线“_”的用法 scala中常用特殊符号 【Scala一】Scala各种符号的含义 Scala各种常见的符号小结 Scala中符号语法糖 scala一些符号含义总结","categories":[],"tags":[]},{"title":"Scala学习-0-4-伴生对象","slug":"Scala学习-0-4-伴生对象","date":"2021-04-12T02:19:06.000Z","updated":"2021-04-12T03:54:07.123Z","comments":true,"path":"post/Scala学习-0-4-伴生对象/","link":"","permalink":"https://shang.at/post/Scala学习-0-4-伴生对象/","excerpt":"简介：","text":"简介： class C定义了一个类，就和java和C++中一样。A class is a definition, a description. It defines a type in terms of methods and composition of other types. object O创建了一个单例(singleton)对象O，它是一个匿名类的实例对象；可以用object来承载静态变量和静态方法。静态方法和静态变量是指那些与任何类都没有关联的方法和参数，他们不需要依赖类的初始化可以完全独立存在 An object is a singleton — an instance of a class which is guaranteed to be unique. For every object in the code, an anonymous class is created, which inherits from whatever classes you declared object to implement. This class cannot be seen from Scala source code — though you can get at it through reflection. object O extends T的写法是创建了一个trait T的一个实例，我们可以在任何需要传入T的地方传入O 如果有一个class C，那么object C就是class C的伴生对象(companion object)。需要注意的是伴生对象并不是class C的一个实例 Also see Scala documentation for object and class. object作为静态成员的宿主通常，你可能需要一个包含静态成员的object，以便你可以不用初始化对象实例就可以直接访问这些成员，这和java中static方法类似。比如 123object A &#123; def twice(i: Int): Int = 2*i&#125; 你可以直接通过A.twice(2)去调用该方法 如果twice被声明在class里面，那么你需要先初始化类实例 123456class A() &#123; def twice(i: Int): Int = 2 * i&#125;val a = new A()a.twice(2) 可以看出，初始化A实例完全是没有必要的，因为twice不需要任何依赖于类实例的数据 object作为特殊的命名实例你也可以将object看做是某些特殊的class或者trait的实例对象。这个时候你的object需要扩展某些trait以成为其子类的实例。 123object A extends B with C &#123; ...&#125; 这个声明首先声明了一个拓展了B和C的匿名(不可访问)类，并实例化名为A的此类的单个实例 这意味着可以将A传递给期望对象类型为B或C或B with C函数。 object额外的特征官方文档：official documentation 伴生对象中可以提供一个工厂方法来创建对应的class实例 12345678910class Email(val username: String, val domainName: String)object Email &#123; def fromString(emailString: String): Option[Email] = &#123; emailString.split('@') match &#123; case Array(a, b) =&gt; Some(new Email(a, b)) case _ =&gt; None &#125; &#125;&#125; def apply(...)：如果class A的伴生对象object A定义了apply方法，那么创建A实例的时候可以直接这么写A(...)，即省略关键字new apply 方法就像一个构造器，接受参数然后创建一个实例对象 def unapply(...)允许创建自定义模式匹配提取器(extractors) unapply 方法接受一个实例对象然后返回最初创建它所用的参数 12345678910111213141516171819import scala.util.Randomobject CustomerID &#123; // 特别注意这里：apply方法返回是一个String类型的对象 def apply(name: String) = s\"$name--$&#123;Random.nextLong&#125;\" // 所以这里unapply方法接收的是一个String类型的对象 def unapply(customerID: String): Option[String] = &#123; val stringArray: Array[String] = customerID.split(\"--\") if (stringArray.tail.nonEmpty) Some(stringArray.head) else None &#125;&#125;val customer1ID = CustomerID(\"Sukyoung\") // 等价于CustomerID.apply(\"Sukyoung\")customer1ID match &#123; case CustomerID(name) =&gt; println(name) // 等价于CustomerID.unapply(customer2ID) case _ =&gt; println(\"Could not extract a CustomerID\")&#125; 1234val customer2ID = CustomerID(\"Nico\")// 等价于 val name = CustomerID.unapply(customer2ID).getval CustomerID(name) = customer2ID println(name) // prints Nico unapply 方法的返回值应当符合下面的某一条： 如果只是用来判断真假，可以返回一个 Boolean 类型的值。例如 case even()。 如果只是用来提取单个 T 类型的值，可以返回 Option[T]。 如果你想要提取多个值，类型分别为 T1,...,Tn，可以把它们放在一个可选的元组中 Option[(T1,...,Tn)]。 有时，要提取的值的数量不是固定的，因此我们想根据输入来返回随机数量的值。这种情况下，你可以用 unapplySeq 方法来定义提取器，此方法返回 Option[Seq[T]]。常见的例子有，用 case List(x, y, z) =&gt; 来解构一个列表 List，以及用一个正则表达式 Regex 来分解一个字符串 String，例如 case r(name, remainingFields @ _*) =&gt;。 如果作为一个class C的伴生对象存在，那么object在解析隐式参数(implicit parameters)时将会扮演特殊的角色。scala在搜索隐式转换时有一个搜索的scope，与C有关的所有类型的伴生对象都在搜索的范围内。","categories":[],"tags":[]},{"title":"机器学习-特征工程与相关性分析","slug":"机器学习-特征工程与相关性分析","date":"2021-02-28T02:37:26.000Z","updated":"2021-02-28T02:38:26.144Z","comments":true,"path":"post/机器学习-特征工程与相关性分析/","link":"","permalink":"https://shang.at/post/机器学习-特征工程与相关性分析/","excerpt":"简介：","text":"简介：","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://shang.at/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://shang.at/tags/机器学习/"}]},{"title":"机器学习-数据探索流程","slug":"机器学习-数据探索流程","date":"2021-02-27T08:02:38.000Z","updated":"2021-02-27T08:06:13.658Z","comments":true,"path":"post/机器学习-数据探索流程/","link":"","permalink":"https://shang.at/post/机器学习-数据探索流程/","excerpt":"简介：","text":"简介：","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://shang.at/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://shang.at/tags/机器学习/"}]},{"title":"Spark学习-json数据操作","slug":"Spark学习-json数据操作","date":"2021-02-07T07:03:49.000Z","updated":"2021-02-07T08:18:11.048Z","comments":true,"path":"post/Spark学习-json数据操作/","link":"","permalink":"https://shang.at/post/Spark学习-json数据操作/","excerpt":"简介：","text":"简介： 有以下的json 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869&#123; \"status\": \"0x0000\", \"msg\": \"执行成功\", \"result\": \"通过\", \"score\": \"0\", \"engineName\": \"credit_unit_salim\", \"versionCode\": \"20200702credit_salim\", \"versionId\": 356307673651200, \"engineId\": 355251417716736, \"outputFields\": [ &#123; \"code\": \"return_reason\", \"name\": \"输出打回原因\", \"value\": \"null\" &#125;, &#123; \"code\": \"deny_days\", \"name\": \"输出拒绝天数\", \"value\": \"0\" &#125;, &#123; \"code\": \"deny_reason\", \"name\": \"输出拒绝原因\", \"value\": \"null\" &#125;, &#123; \"code\": \"decision\", \"name\": \"输出决策\", \"value\": \"forward_manual\" &#125;, &#123; \"code\": \"limit\", \"name\": \"输出授信额度\", \"value\": \"0\" &#125;, &#123; \"code\": \"cash_limit\", \"name\": \"现金贷款额度\", \"value\": \"0\" &#125; ], \"inputFields\": [ &#123; \"indo_id_check\": \"DEDY DWI SETYAWAN\", \"indo_identical_accuracy_ktp\": \"-2.0\", \"indo_mobile_number_approving\": \"1\", \"indo_name_diff_id_check\": \"0\", \"indo_name_diff_ocr\": \"1\", \"indo_nik_approving\": \"1\", \"indo_nik_diff_employee_nik\": \"0\", \"indo_nik_diff_ocr\": \"1\", \"indo_ocr_name\": \"DEDY DWI SEVYAWAN\", \"indo_ocr_nik\": \"3525051812850002\", \"indo_reject_his_nik\": \"0\", \"indo_reject_his_tel\": \"0\", \"同一个申请下return次数\": \"0\" &#125; ], \"outputFieldInfo\": [ &#123; \"输出打回原因\": \"null\", \"输出拒绝天数\": \"0\", \"输出拒绝原因\": \"null\", \"输出决策\": \"forward_manual\", \"输出授信额度\": \"0\", \"现金贷款额度\": \"0\" &#125; ]&#125; sql版本取json中的array，array的每个元素作为一个map结构12345678910111213spark.sql(\"\"\"select from_json(data, 'array&lt;map&lt;string,string&gt;&gt;') rfrom ( select json_tuple(json, 'inputFields') data) t\"\"\").printSchema()root |-- r: array (nullable = true) | |-- element: map (containsNull = true) | | |-- key: string | | |-- value: string (valueContainsNull = true) array&lt;map&lt;string,string&gt;&gt;这个是如何得来的呢？是这样的，可以使用schema_of_json函数来获取一个json的结构是怎样的。 12345spark.sql(\"\"\"select schema_of_json(json)\"\"\").toPandas()#array&lt;struct&lt;indo_id_check:string,indo_identical_accuracy_ktp:string,indo_mobile_number_approving:string,indo_name_diff_id_check:string,indo_name_diff_ocr:string,indo_nik_approving:string,indo_nik_diff_employee_nik:string,indo_nik_diff_ocr:string,indo_ocr_name:string,indo_ocr_nik:string,indo_reject_his_nik:string,indo_reject_his_tel:string,同一个申请下return次数:string&gt;&gt; 但是通过上述方式得到的结果不能直接拿来用，需要做一些变形：如果array的每个元素的字段都是固定的，那么可以将array的元素定义为一个struct，但是我们的需求中的数据，array的每个元素的字段是不固定的，且我们没有将其字段都穷举出来，所以我们就把它定义为一个map类型的，后续将它使用explode_outer展开再进行处理。通过查询spark的api，可以知道MapType的构造方法需要两个参数，分别为key的Type和value的Type，这里我们直接使用最通用的String类型代替了 pyspark1234567891011from pyspark.sql.functions import *from pyspark.sql.types import *schema = ArrayType(MapType(StringType(), StringType()))spark.sql(\"\"\"select json_tuple(json, 'inputFields') items\"\"\").withColumn( 'items', from_json('items', schema)).toPandas() 123456789101112131415161718192021222324252627282930313233343536373839spark.sql(\"\"\"select json_tuple(json, 'inputFields') items-- from atome_id_mysql_snapshot_ruleengine.t_result_catalog limit 1\"\"\").withColumn( 'items', from_json('items', schema)).withColumn( 'item', explode('items')).withColumn( 'keys', map_keys('item')).withColumn( 'values', map_values('item')).withColumn( 'k_v', arrays_zip('keys', 'values')).withColumn( 'kv', explode_outer('k_v')).printSchema()root |-- items: array (nullable = true) | |-- element: map (containsNull = true) | | |-- key: string | | |-- value: string (valueContainsNull = true) |-- item: map (nullable = true) | |-- key: string | |-- value: string (valueContainsNull = true) |-- keys: array (nullable = true) | |-- element: string (containsNull = true) |-- values: array (nullable = true) | |-- element: string (containsNull = true) |-- k_v: array (nullable = true) | |-- element: struct (containsNull = false) | | |-- keys: string (nullable = true) | | |-- values: string (nullable = true) |-- kv: struct (nullable = true) | |-- keys: string (nullable = true) | |-- values: string (nullable = true) 123456789101112131415df = spark.sql(\"\"\"select json_tuple(json, 'inputFields') items\"\"\").withColumn( 'items', from_json('items', schema)).withColumn( 'item', explode('items'))# df.printSchema()# df.select(expr(\"posexplode(d)\")).printSchemadf.select(expr('explode(item)')).toPandas() # 将map 展开 posexplode会多一个pos的字段 key value 0 indo_id_check DEDY DWI SETYAWAN 1 indo_identical_accuracy_ktp -2.0 2 indo_mobile_number_approving 1 3 indo_name_diff_id_check 0 4 indo_name_diff_ocr 1 5 indo_nik_approving 1 6 indo_nik_diff_employee_nik 0 7 indo_nik_diff_ocr 1 8 indo_ocr_name DEDY DWI SEVYAWAN 9 indo_ocr_nik 3525051812850002 10 indo_reject_his_nik 0 11 indo_reject_his_tel 0 12 同一个申请下return次数 0 Spark SQL集合数据类型array\\map的取值方式","categories":[{"name":"Spark学习","slug":"Spark学习","permalink":"https://shang.at/categories/Spark学习/"}],"tags":[{"name":"json数据操作","slug":"json数据操作","permalink":"https://shang.at/tags/json数据操作/"}]},{"title":"操作系统","slug":"操作系统","date":"2021-02-01T02:21:35.000Z","updated":"2021-02-01T03:00:38.252Z","comments":true,"path":"post/操作系统/","link":"","permalink":"https://shang.at/post/操作系统/","excerpt":"简介：","text":"简介： 汇编语言汇编语言的本质：机器语言的助记符，它就是机器语言 汇编的执行过程： 计算机通电-》CPU读取内存中固定位置的程序（电信号输入）-》时钟发生器不断震荡通断电-》推动CPU内部一步一步执行（执行多少步取决于指令需要的时钟周期）-》计算完成-》写回（电信号）-》写给显卡输出（sout，或者图形） 计算机的组成冯诺依曼体系结构：计算器，控制器，主存储器，输入设备，输出设备 计算机有一个主板，它实际上是一个集成电路板，上面有很多插槽，CPU、内存、磁盘控制器、磁盘、显卡、网卡驱动都可以插在上面，一旦插上，整体就组成了一个连通的电路状态 32位、64位表示的是CPU一次性可以读取的位数，寄存器可以存储的位数 CPU的基本组成PC：Program Counter 程序计数器 记录当前指令地址 Registers：暂时存储CPU计算需要用到的数据 ALU：Arithmetic &amp; Logic Unit 运算单元 CU：Control Unit 控制单元 MMU：Memory Management Unit 内存管理单元 cache：","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://shang.at/categories/操作系统/"}],"tags":[{"name":"操作系统","slug":"操作系统","permalink":"https://shang.at/tags/操作系统/"}]},{"title":"大数据-zookeeper","slug":"大数据-zookeeper","date":"2021-01-14T11:26:42.000Z","updated":"2021-01-14T11:31:29.312Z","comments":true,"path":"post/大数据-zookeeper/","link":"","permalink":"https://shang.at/post/大数据-zookeeper/","excerpt":"简介：","text":"简介： 1、client向集群请求创建一个节点，集群内部会经过两阶段的提交，半数通过，才能最终创建成功，第一阶段是写磁盘日志，第二阶段是在内存中创建节点。看上去很复杂，为什么zk集群还能很快速的响应？ 2、leader挂掉之后，如果有节点的Zxid不是最大的，那么选举了新的leader之后，该节点的Zxid如何更新 3、如果一个client请求数据的follower，该follower返回的数据如何保证是最新的","categories":[{"name":"大数据","slug":"大数据","permalink":"https://shang.at/categories/大数据/"}],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"https://shang.at/tags/zookeeper/"}]},{"title":"Mysql学习-优化案例","slug":"Mysql学习-优化案例","date":"2020-12-18T02:22:47.000Z","updated":"2021-02-18T09:03:56.877Z","comments":true,"path":"post/Mysql学习-优化案例/","link":"","permalink":"https://shang.at/post/Mysql学习-优化案例/","excerpt":"简介：","text":"简介： 在进行优化案例之前，先看一下几个概念：索引、聚集索引、普通索引、回表查询、索引覆盖 概念数据库表结构： 123456create table user ( id int primary key, name varchar(20), sex varchar(5), index(name))engine=innodb; 多查询了一个属性，为何检索过程完全不同？ 什么是回表查询？ 什么是索引覆盖？ 如何实现索引覆盖？ 哪些场景，可以利用索引覆盖来优化SQL？ 这些，这是今天要分享的内容。 画外音：本文试验基于MySQL5.6-InnoDB。 一、什么是回表查询？ 这先要从InnoDB的索引实现说起，InnoDB有两大类索引： 聚集索引(clustered index) 普通索引(secondary index) InnoDB聚集索引和普通索引有什么差异？ InnoDB聚集索引的叶子节点存储行记录，因此， InnoDB必须要有，且只有一个聚集索引： （1）如果表定义了PK，则PK就是聚集索引； （2）如果表没有定义PK，则第一个not NULL unique列是聚集索引； （3）否则，InnoDB会创建一个隐藏的row-id作为聚集索引； 画外音：所以PK查询非常快，直接定位行记录。 InnoDB普通索引的叶子节点存储主键值。 画外音：注意，不是存储行记录头指针，MyISAM的索引叶子节点存储记录指针。 举个栗子，不妨设有表： t(id PK, name KEY, sex, flag); 画外音：id是聚集索引，name是普通索引。 表中有四条记录： 1, shenjian, m, A 3, zhangsan, m, A 5, lisi, m, A 9, wangwu, f, B 两个B+树索引分别如上图： （1）id为PK，聚集索引，叶子节点存储行记录； （2）name为KEY，普通索引，叶子节点存储PK值，即id； 既然从普通索引无法直接定位行记录，那普通索引的查询过程是怎么样的呢？ 通常情况下，需要扫码两遍索引树。 例如： 1select * from t where name='lisi' 是如何执行的呢？ 如粉红色路径，需要扫码两遍索引树： （1）先通过普通索引定位到主键值id=5； （2）在通过聚集索引定位到行记录； 这就是所谓的回表查询，先定位主键值，再定位行记录，它的性能较扫一遍索引树更低。 二、什么是索引覆盖(Covering index)？ 额，楼主并没有在MySQL的官网找到这个概念。 画外音：治学严谨吧？ 借用一下SQL-Server官网的说法。 MySQL官网，类似的说法出现在explain查询计划优化章节，即explain的输出结果Extra字段为Using index时，能够触发索引覆盖。 不管是SQL-Server官网，还是MySQL官网，都表达了：只需要在一棵索引树上就能获取SQL所需的所有列数据，无需回表，速度更快。 三、如何实现索引覆盖？ 常见的方法是：将被查询的字段，建立到联合索引里去。 仍是之前中的例子： 123456create table user ( id int primary key, name varchar(20), sex varchar(5), index(name))engine=innodb; 第一个SQL语句： 1select id,name from user where name='shenjian' 能够命中name索引，索引叶子节点存储了主键id，通过name的索引树即可获取id和name，无需回表，符合索引覆盖，效率较高。 画外音，Extra：Using index。 第二个SQL语句： 1select id,name,sex from user where name='shenjian' 能够命中name索引，索引叶子节点存储了主键id，但sex字段必须回表查询才能获取到，不符合索引覆盖，需要再次通过id值扫码聚集索引获取sex字段，效率会降低。 画外音，Extra：Using index condition。 如果把(name)单列索引升级为联合索引(name, sex)就不同了。 123456create table user ( id int primary key, name varchar(20), sex varchar(5), index(name, sex))engine=innodb; 可以看到： 12select id,name... where name='shenjian';select id,name,sex ... where name='shenjian' 都能够命中索引覆盖，无需回表。 画外音，Extra：Using index。 四、哪些场景可以利用索引覆盖来优化SQL？ 场景1：全表count查询优化 原表为： user(PK id, name, sex)； 直接： 1select count(name) from user; 不能利用索引覆盖。 添加索引： 1alter table user add key(name); 就能够利用索引覆盖提效。 场景2：列查询回表优化 1select id,name,sex ... where name='shenjian' 这个例子不再赘述，将单列索引(name)升级为联合索引(name, sex)，即可避免回表。 场景3：分页查询 1select id,name,sex ... order by name limit 500,100 将单列索引(name)升级为联合索引(name, sex)，也可以避免回表。 优化案例","categories":[{"name":"Mysql","slug":"Mysql","permalink":"https://shang.at/categories/Mysql/"}],"tags":[{"name":"mysql优化","slug":"mysql优化","permalink":"https://shang.at/tags/mysql优化/"}]},{"title":"Mysql学习-索引","slug":"Mysql学习-索引","date":"2020-12-15T01:48:57.000Z","updated":"2020-12-15T08:56:13.741Z","comments":true,"path":"post/Mysql学习-索引/","link":"","permalink":"https://shang.at/post/Mysql学习-索引/","excerpt":"简介：唯一索引（unique index）和非唯一索引（普通索引）（index） 区别","text":"简介：唯一索引（unique index）和非唯一索引（普通索引）（index） 区别 如果在一个列上同时建唯一索引和普通索引的话，mysql会自动选择唯一索引。 谷歌一下： 唯一索引和普通索引使用的结构都是B-tree,执行时间复杂度都是O(log n)。 补充下概念： 1、普通索引(非唯一索引) 普通索引（由关键字KEY或INDEX定义的索引）的唯一任务是加快对数据的访问速度。因此，应该只为那些最经常出现在查询条件（WHEREcolumn=）或排序条件（ORDERBYcolumn）中的数据列创建索引。只要有可能，就应该选择一个数据最整齐、最紧凑的数据列（如一个整数类型的数据列）来创建索引。 2、唯一索引 普通索引允许被索引的数据列包含重复的值。比如说，因为人有可能同名，所以同一个姓名在同一个“员工个人资料”数据表里可能出现两次或更多次。 如果能确定某个数据列将只包含彼此各不相同的值，在为这个数据列创建索引的时候就应该用关键字UNIQUE把它定义为一个唯一索引。这么做的好处：一是简化了MySQL对这个索引的管理工作，这个索引也因此而变得更有效率；二是MySQL会在有新记录插入数据表时，自动检查新记录的这个字段的值是否已经在某个记录的这个字段里出现过了；如果是，MySQL将拒绝插入那条新记录。也就是说，唯一索引可以保证数据记录的唯一性。事实上，在许多场合，人们创建唯一索引的目的往往不是为了提高访问速度，而只是为了避免数据出现重复。 主键索引主键索引是唯一索引的特殊类型。数据库表通常有一列或列组合，其值用来唯一标识表中的每一行。该列称为表的主键。在数据库关系图中为表定义一个主键将自动创建主键索引，主键索引是唯一索引的特殊类型。主键索引要求主键中的每个值是唯一的。当在查询中使用主键索引时，它还允许快速访问数据。 它们的一些比较：(1)对于主健/unique constraint ， oracle/sql server/mysql等都会自动建立唯一索引；(2)主键不一定只包含一个字段，所以如果你在主键的其中一个字段建唯一索引还是必要的；(3)主健可作外健，唯一索引不可；(4)主健不可为空，唯一索引可；(5)主健也可是多个字段的组合；(6)主键与唯一索引不同的是：a.有not null属性；b.每个表只能有一个。 3. 实例： 索引的作用是“排列好次序，使得查询时可以快速找到” 》唯一索引：被索引的字段组合，其数据在全表中唯一。如下表中，为’学号‘建索引：学号 姓名-———————————001 张三002 李四 》非唯一索引：数据可以不唯一。如下表中，为Score建索引，不可唯一： Score | Name-—————————-98 张三98 李四96 王五 -————————————————————————————————————————————————————- CREATE INDEX 语句用于在表中创建索引。 在不读取整个表的情况下，索引使数据库应用程序可以更快地查找数据。 索引您可以在表中创建索引，以便更加快速高效地查询数据。 用户无法看到索引，它们只能被用来加速搜索/查询。 注释：更新一个包含索引的表需要比更新一个没有索引的表更多的时间，这是由于索引本身也需要更新。因此，理想的做法是仅仅在常常被搜索的列（以及表）上面创建索引。 SQL CREATE INDEX 语法在表上创建一个简单的索引。允许使用重复的值： 1CREATE INDEX index_name ON table_name (column_name) 注释：”column_name” 规定需要索引的列。 SQL CREATE UNIQUE INDEX 语法在表上创建一个唯一的索引。唯一的索引意味着两个行不能拥有相同的索引值。 1CREATE UNIQUE INDEX index_name ON table_name (column_name) CREATE INDEX 实例本例会创建一个简单的索引，名为 “PersonIndex”，在 Person 表的 LastName 列： 1CREATE INDEX PersonIndex ON Person (LastName) 如果您希望以降序索引某个列中的值，您可以在列名称之后添加保留字 DESC： 1CREATE INDEX PersonIndex ON Person (LastName DESC) 假如您希望索引不止一个列，您可以在括号中列出这些列的名称，用逗号隔开： 1CREATE INDEX PersonIndex ON Person (LastName, FirstName) Drop INDEX 实例 drop index idx_name on db; From: http://blog.chinaunix.net/uid-30041424-id-5680256.html https://zhidao.baidu.com/question/456499708168710645.html http://www.cnblogs.com/ymj0906/p/4240856.html 索引键的唯一性（1/4）：堆表上的唯一与非唯一非聚集索引的区别 http://www.cnblogs.com/woodytu/p/4560478.html 唯一索引和非唯一索引的区别简析 http://www.cnblogs.com/Joe-T/p/3573672.html http://www.w3school.com.cn/sql/sql_create_index.asp","categories":[{"name":"Mysql","slug":"Mysql","permalink":"https://shang.at/categories/Mysql/"}],"tags":[{"name":"索引","slug":"索引","permalink":"https://shang.at/tags/索引/"}]},{"title":"Python学习-正则","slug":"Python学习-正则","date":"2020-12-08T10:18:33.000Z","updated":"2020-12-08T10:19:14.366Z","comments":true,"path":"post/Python学习-正则/","link":"","permalink":"https://shang.at/post/Python学习-正则/","excerpt":"简介：","text":"简介：","categories":[{"name":"正则","slug":"正则","permalink":"https://shang.at/categories/正则/"}],"tags":[{"name":"正则","slug":"正则","permalink":"https://shang.at/tags/正则/"}]},{"title":"Python学习-Literal String Interpolation","slug":"Python学习-Literal-String-Interpolation","date":"2020-12-08T10:18:18.000Z","updated":"2020-12-08T10:19:07.362Z","comments":true,"path":"post/Python学习-Literal-String-Interpolation/","link":"","permalink":"https://shang.at/post/Python学习-Literal-String-Interpolation/","excerpt":"简介：","text":"简介： https://www.python.org/dev/peps/pep-0498/","categories":[{"name":"python-lsi","slug":"python-lsi","permalink":"https://shang.at/categories/python-lsi/"}],"tags":[{"name":"python-lsi","slug":"python-lsi","permalink":"https://shang.at/tags/python-lsi/"}]},{"title":"大数据-flink","slug":"大数据-flink","date":"2020-12-07T02:51:11.000Z","updated":"2021-01-31T01:39:53.471Z","comments":true,"path":"post/大数据-flink/","link":"","permalink":"https://shang.at/post/大数据-flink/","excerpt":"简介：","text":"简介： https://ci.apache.org/projects/flink/flink-docs-release-1.11/try-flink/table_api.html5 -","categories":[{"name":"大数据","slug":"大数据","permalink":"https://shang.at/categories/大数据/"}],"tags":[{"name":"flink","slug":"flink","permalink":"https://shang.at/tags/flink/"}]},{"title":"java学习-Date和时区转换","slug":"java学习-Date和时区转换","date":"2020-11-06T02:07:20.000Z","updated":"2020-11-06T02:09:36.492Z","comments":true,"path":"post/java学习-Date和时区转换/","link":"","permalink":"https://shang.at/post/java学习-Date和时区转换/","excerpt":"简介：","text":"简介： 1.Date中保存的是什么 在java中，只要我们执行 Date date = new Date(); 就可以得到当前时间。如： 12Date date = new Date();System.out.println(date); 输出结果是： Thu Aug 24 10:15:29 CST 2017 也就是我执行上述代码的时刻：2017年8月24日10点15分29秒。是不是Date对象里存了年月日时分秒呢？不是的，Date对象里存的只是一个long型的变量，其值为自1970年1月1日0点至Date对象所记录时刻经过的毫秒数，调用Date对象getTime()方法就可以返回这个毫秒数，如下代码： 12Date date = new Date();System.out.println(date + \", \" + date.getTime()); 输出如下： Thu Aug 24 10:48:05 CST 2017, 1503542885955 即上述程序执行的时刻是2017年8月24日10点48分05秒，该时刻距离1970年1月1日0点经过了1503542885955毫秒。反过来说，输出的年月日时分秒其实是根据这个毫秒数来反算出来的。 2.时区 全球分为24个时区，相邻时区时间相差1个小时。比如北京处于东八时区，东京处于东九时区，北京时间比东京时间晚1个小时，而英国伦敦时间比北京晚7个小时（英国采用夏令时时，8月英国处于夏令时）。比如此刻北京时间是2017年8月24日11:17:10，则东京时间是2017年8月24日12:17:10，伦敦时间是2017年8月24日4:17:10。 既然Date里存放的是当前时刻距1970年1月1日0点时刻的毫秒数，如果此刻在伦敦、北京、东京有三个程序员同时执行如下语句： 1Date date = new Date(); 那这三个date对象里存的毫秒数是相同的吗？还是北京的比东京的小3600000（北京时间比东京时间晚1小时，1小时为3600秒即3600000毫秒）？答案是，这3个Date里的毫秒数是完全一样的。确切的说，Date对象里存的是自格林威治时间（ GMT）1970年1月1日0点至Date对象所表示时刻所经过的毫秒数。所以，如果某一时刻遍布于世界各地的程序员同时执行new Date语句，这些Date对象所存的毫秒数是完全一样的。也就是说，Date里存放的毫秒数是与时区无关的。 继续上述例子，如果上述3个程序员调用那一刻的时间是北京时间2017年8月24日11:17:10，他们继续调用 1System.out.println(date); 那么北京的程序员将会打印出2017年8月24日11:17:10，而东京的程序员会打印出2017年8月24日12:17:10，伦敦的程序员会打印出2017年8月24日4:17:10。既然Date对象只存了一个毫秒数，为什么这3个毫秒数完全相同的Date对象，可以打印出不同的时间呢？这是因为Sysytem.out.println函数在打印时间时，会取操作系统当前所设置的时区，然后根据这个时区将同毫秒数解释成该时区的时间。当然我们也可以手动设置时区，以将同一个Date对象按不同的时区输出。可以做如下实验验证： 12345678910Date date = new Date(1503544630000L); // 对应的北京时间是2017-08-24 11:17:10SimpleDateFormat bjSdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\"); // 北京bjSdf.setTimeZone(TimeZone.getTimeZone(\"Asia/Shanghai\")); // 设置北京时区SimpleDateFormat tokyoSdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\"); // 东京tokyoSdf.setTimeZone(TimeZone.getTimeZone(\"Asia/Tokyo\")); // 设置东京时区SimpleDateFormat londonSdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\"); // 伦敦londonSdf.setTimeZone(TimeZone.getTimeZone(\"Europe/London\")); // 设置伦敦时区System.out.println(\"毫秒数:\" + date.getTime() + \", 北京时间:\" + bjSdf.format(date));System.out.println(\"毫秒数:\" + date.getTime() + \", 东京时间:\" + tokyoSdf.format(date));System.out.println(\"毫秒数:\" + date.getTime() + \", 伦敦时间:\" + londonSdf.format(date)); 输出为： 毫秒数:1503544630000, 北京时间:2017-08-24 11:17:10 毫秒数:1503544630000, 东京时间:2017-08-24 12:17:10 毫秒数:1503544630000, 伦敦时间:2017-08-24 04:17:10 可以看出，同一个Date对象，按不同的时区来格式化，将得到不同时区的时间。由此可见，Date对象里保存的毫秒数和具体输出的时间（即年月日时分秒）是模型和视图的关系，而时区（即Timezone)则决定了将同一个模型展示成什么样的视图。 3.从字符串中读取时间 有时我们会遇到从一个字符串中读取时间的要求，即从字符串中解析时间并得到一个Date对象，比如将 “2017-8-24 11:17:10” 解析为一个Date对象。现在问题来了，这个时间到底指的是北京时间的2017年8月24日11:17:10，还是东京时间的2017年8月24日11:17:10？如果指的是北京时间，那么这个时间对应的东京时间2017年8月24日12:17:10；如果指的是东京时间，那么这个时间对应的北京时间就是2017年8月24日10:17:10。因此，只说年月日时分秒而不说是哪个时区的，是有歧义的，没有歧义的做法是，给出一个时间字符串，同时指明这是哪个时区的时间。 从字符串中解析时间的正确作法是：指定时区来解析。示例如下： 12345String timeStr = \"2017-8-24 11:17:10\"; // 字面时间SimpleDateFormat sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\");sdf.setTimeZone(TimeZone.getTimeZone(\"Asia/Shanghai\")); // 设置北京时区Date d = sdf.parse(timeStr);System.out.println(sdf.format(d) + \", \" + d.getTime()); 输出为： 2017-08-24 11:17:10, 1503544630000, 将一个时间字符串按不同时区来解释，得到的Date对象的值是不同的。验证如下： 123456789101112131415String timeStr = \"2017-8-24 11:17:10\"; // 字面时间SimpleDateFormat bjSdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\");bjSdf.setTimeZone(TimeZone.getTimeZone(\"Asia/Shanghai\"));Date bjDate = bjSdf.parse(timeStr); // 解析System.out.println(\"字面时间: \" + timeStr +\",按北京时间来解释:\" + bjSdf.format(bjDate) + \", \" + bjDate.getTime());SimpleDateFormat tokyoSdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\"); // 东京tokyoSdf.setTimeZone(TimeZone.getTimeZone(\"Asia/Tokyo\")); // 设置东京时区Date tokyoDate = tokyoSdf.parse(timeStr); // 解析System.out.println(\"字面时间: \" + timeStr +\",按东京时间来解释:\" + tokyoSdf.format(tokyoDate) + \", \" + tokyoDate.getTime()); 输出为： 字面时间: 2017-8-24 11:17:10,按北京时间来解释:2017-08-24 11:17:10, 1503544630000 字面时间: 2017-8-24 11:17:10,按东京时间来解释:2017-08-24 11:17:10, 1503541030000 可以看出，对于”2017-8-24 11:17:10”这个字符串，按北京时间来解释得到Date对象的毫秒数是 1503544630000；而按东京时间来解释得到的毫秒数是1503541030000，前者正好比后者大于3600000毫秒即1个小时，正好是北京时间和东京时间的时差。这很好理解，北京时间2017-08-24 11:17:10对应的毫秒数是1503544630000，而东京时间2017-08-24 11:17:10对应的北京时间其实是2017-08-24 10:17:10（因为北京时间比东京时间晚1个小时），北京时间2017-08-24 10:17:10自然比北京时间2017-08-24 11:17:10少3600000毫秒。 4.将字符串表示的时间转换成另一个时区的时间字符串 综合以上分析，如果给定一个时间字符串，并告诉你这是某个时区的时间，要将它转换为另一个时区的时间并输出，正确的做法是： 1.将字符串按原时区转换成Date对象； 2.将Date对象格式化成目标时区的时间。 比如，将北京时间”2017-8-24 11:17:10 “输出成东京时间，代码为： 123456789String timeStr = \"2017-8-24 11:17:10\"; // 字面时间SimpleDateFormat bjSdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\");bjSdf.setTimeZone(TimeZone.getTimeZone(\"Asia/Shanghai\"));Date date = bjSdf.parse(timeStr); // 将字符串时间按北京时间解析成Date对象SimpleDateFormat tokyoSdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\"); // 东京tokyoSdf.setTimeZone(TimeZone.getTimeZone(\"Asia/Tokyo\")); // 设置东京时区System.out.println(\"北京时间: \" + timeStr +\"对应的东京时间为:\" + tokyoSdf.format(date)); 输出为： 北京时间:2017-8-24 11:17:10对应的东京时间为:2017-08-24 12:17:10","categories":[],"tags":[]},{"title":"工具使用-jupyter","slug":"工具使用-jupyter","date":"2020-10-30T08:40:52.000Z","updated":"2020-10-30T08:45:50.890Z","comments":true,"path":"post/工具使用-jupyter/","link":"","permalink":"https://shang.at/post/工具使用-jupyter/","excerpt":"简介：","text":"简介： 如何在Jupyter notebook中debug？在需要breakpoint的地方插入import pdb; pdb.set_trace()，运行后会进入debugger，有一个交互界面。 脚本控制清空output12345from IPython.display import clear_outputfor i in range(10): clear_output(wait=True) print(\"Hello World!\")","categories":[{"name":"jupyter","slug":"jupyter","permalink":"https://shang.at/categories/jupyter/"}],"tags":[{"name":"工具使用","slug":"工具使用","permalink":"https://shang.at/tags/工具使用/"}]},{"title":"工具使用-idea","slug":"工具使用-idea","date":"2020-10-29T05:51:45.000Z","updated":"2020-12-11T08:04:43.562Z","comments":true,"path":"post/工具使用-idea/","link":"","permalink":"https://shang.at/post/工具使用-idea/","excerpt":"简介：","text":"简介： 调整注释的颜色 调整单词 选中之后 相同单词的颜色","categories":[{"name":"idea","slug":"idea","permalink":"https://shang.at/categories/idea/"}],"tags":[{"name":"工具","slug":"工具","permalink":"https://shang.at/tags/工具/"}]},{"title":"程序设计思想-池化思想","slug":"程序设计思想-池化思想","date":"2020-10-23T00:00:48.000Z","updated":"2020-10-23T01:34:19.987Z","comments":true,"path":"post/程序设计思想-池化思想/","link":"","permalink":"https://shang.at/post/程序设计思想-池化思想/","excerpt":"简介：为了提高程序的性能，在设计中很巧妙的使用了池化的思想，包括线程池、连接池、内存池","text":"简介：为了提高程序的性能，在设计中很巧妙的使用了池化的思想，包括线程池、连接池、内存池 线程池对线程进行池化管理，可以实现对线程的复用，避免大量创建线程的资源浪费 疑问：每个task结束之后，线程是如何复用的呢？ 连接池内存池对内存进行池化管理，可以实现对内存的重复使用，对于java等一系列需要GC的语言来说，可以节省大量GC的时间，很大程度上提高程序的运行效率","categories":[{"name":"池化","slug":"池化","permalink":"https://shang.at/categories/池化/"}],"tags":[{"name":"程序设计","slug":"程序设计","permalink":"https://shang.at/tags/程序设计/"}]},{"title":"java学习-基础3-对象内存布局","slug":"java学习-基础3-对象内存布局","date":"2020-09-27T23:56:58.000Z","updated":"2020-12-11T08:21:32.601Z","comments":true,"path":"post/java学习-基础3-对象内存布局/","link":"","permalink":"https://shang.at/post/java学习-基础3-对象内存布局/","excerpt":"简介：","text":"简介：","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"对象内存布局","slug":"对象内存布局","permalink":"https://shang.at/tags/对象内存布局/"}]},{"title":"工具使用-gitlab","slug":"工具使用-gitlab","date":"2020-09-11T06:52:31.000Z","updated":"2020-12-22T15:18:08.734Z","comments":true,"path":"post/工具使用-gitlab/","link":"","permalink":"https://shang.at/post/工具使用-gitlab/","excerpt":"简介：","text":"简介： Local repo：就是存在你本地的仓库；Remote repo：就是保存在远端的仓库，两者的内容在某些时候是不一样的，需要手动去保持一致 local和remote的关系：第一次从remote将仓库克隆到本地的时候，本地的repo和远端的一模一样，如果你在本地创建了新的分支，那么在你执行git push之前，remote repo是没有你创建的分支的。同样的如果有其他人创建了分支并且push到了remote repo，你本地也是不知道的，这个时候就需要执行git fetch去更新你本地的repo。 新建repo1234567891011121314151617181920212223242526# Git global setupgit config --global user.name \"xxx\"git config --global user.email \"xxx@abc.com\"# Create a new repositorygit clone git_urlcd atome-id-apaylater-data-warehousetouch README.mdgit add README.mdgit commit -m \"add README\"git push -u origin master# Push an existing foldercd existing_foldergit initgit remote add origin git_urlgit add .git commit -m \"Initial commit\"git push -u origin master# Push an existing Git repositorycd existing_repogit remote rename origin old-origingit remote add origin git_urlgit push -u origin --allgit push -u origin --tags 常用操作查看本地分支：git branch 查看全部分支：git branch -a 删除本地分支：git branch -d branchName 强制删除本地分支：git branch -D branchName 切换分支：git checkout branchName 基于当前分支创建一个新的分支：git checkout -b newBranch 暂存本地修改（修改了本地文件，但是又不想提交。慎用）：git stash 释放 暂存的本地修改：git stash pop 查看local repo 的状态：git status 将修改的文件添加到local repo（tracked）：git add . 或git add 文件名 将修改的文件提交到local repo：git commit -m ‘msg’ 合并分支：git merge 更新本地repo：git fetch git pull与git fetch的更详细的解读 工作中常用的操作步骤： 创建自己的分支 开发代码 执行 git add和git commit命令，将修改提交到local的分支中（这个时候remote是没有这个分支的） 切回master分支，更新master代码 切回自己的分支，将master分支merge到自己的分支，顺便解决冲突（冲突是两个人同时修改同一个文件的时候可能会发生的，一般很少会发生冲突） 可能需要继续执行git add 和 git commit 将本地分支推到remote：git push 到web端提交MR 用rebase -i 汇合提交进入一个repo目录，使用git log可以查看每次commit的记录 然后使用git rebase -i HEAD~~命令进入汇合提交页面(可以使用vim编辑的状态) 每次rebase只能把当前的commit合并到前一个commit上，编辑状态如下 12345678910111213141516pick 9a54fd4 添加commit的说明pick 0d4a808 添加pull的说明# Rebase 326fc9f..0d4a808 onto d286baa## Commands:# p, pick = use commit# r, reword = use commit, but edit the commit message# e, edit = use commit, but stop for amending# s, squash = use commit, but meld into previous commit# f, fixup = like \"squash\", but discard this commit's log message# x, exec = run command (the rest of the line) using shell## If you remove a line here THAT COMMIT WILL BE LOST.# However, if you remove everything, the rebase will be aborted.# 将第二行的pick改成squash保存即可。 再次使用git log查看commit log git如何删除已经 add 的文件使用 git rm 命令即可，有两种选择, 一种是 git rm —cached “文件路径”，不删除物理文件，仅将该文件从缓存中删除； 一种是 git rm —f “文件路径”，不仅将该文件从缓存中删除，还会将物理文件删除（不会回收到垃圾桶）。 如何如何更新fork的repository首先，检查一下当前的配置，看看当前有没有已经设置了上游，这要使用 git remote -v 命令。如下： git remote -v origin https://github.com/YOUR_USERNAME/YOUR_FORK.git (fetch) origin https://github.com/YOUR_USERNAME/YOUR_FORK.git (push) 以上表明，origin这个repository对应的是远端的https开头的这个链接指向的repository，即自己fork出的repository. 第二步，将原repository设置为自己fork出的repository的上游（upstream）。运用如下的命令： $git remote add upstream https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY.git 运用第一步中提到的git remove -v命令再次检查一下，结果如下： $git remote -v origin https://github.com/YOUR_USERNAME/YOUR_FORK.git (fetch) origin https://github.com/YOUR_USERNAME/YOUR_FORK.git (push) upstream https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY.git (fetch) upstream https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY.git (push) 第三步，运行 git fetch upstream 命令，如下： $git fetch upstream remote: Counting objects: 75, done. remote: Compressing objects: 100% (53/53), done. remote: Total 62 (delta 27), reused 44 (delta 9) Unpacking objects: 100% (62/62), done. From https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY * [new branch] master -&gt; upstream/master 以上表明，远程的原repository上确实有一些更新，现在它们已经被download到本地的.git文件夹下了，但是还没有合并到本地的代码中。 第四步，git checkout master，这是保证切换到本地的repository的master上，如果本来就在，那么这一步不是必须的。 第五步，运行 git merge upstream/master 命令，将upstream/master上的更新合并到本地的master上，其实就是将第三步中download到.git文件夹下的那些change合并到本地的master中。如下： $git merge upstream/master Updating a422352..5fdff0f Fast-forward README | 9 ———- README.md | 7 ++++++ 2 files changed, 7 insertions(+), 9 deletions(-) delete mode 100644 README create mode 100644 README.md 如果本地没有什么自己独立的更新的话，那么将执行”Fast-forward”的合并。如果本地有自己独立的更新，而又会引起冲突的话，则要解决冲突，再commit. 关于解决冲突，如果明确所有冲突都是使用upstream/master上的来override自己的，那么可以直接运行如下命令，则无需解决冲突了 git merge -X theirs upstream/master 注意，以上步骤结束后，仅仅是本地的fork出的repository和原repository取得了同步，如果想让远程的fork出的repository也同样取得同步，必须再git push上去。 submodule 创建一个repo：test_1 1234git clone git@repo.advai.net:shang.wang/test_1.gittouch README.mdgit commit -m \"1\"git push -u origin master 创建一个lib repo：test_2 1234git clone git@repo.advai.net:shang.wang/test_2.gittouch README.mdgit commit -m &quot;1&quot;git push -u origin master 将test_2作为submodule添加到test_1中 1git submodule add git@repo.advai.net:shang.wang/test_2.git test_lib 提交test_1 12345cd test_1git add .git commit -m &apos;2&apos;git submodule initgit push 修改test_2 12345cd test_2echo &quot;123445&quot; &gt; test.txtgit add .git commit -m &apos;3&apos;git push 在test_1中更新test_2 12cd test_2git submodule foreach git pull clone一个带有submodule的repo 1234567891、clone父repo的时候直接clonesubmodulegit clone git@repo.advai.net:shang.wang/test_1.git ttttt --recursive2、clone完父repo之后，再初始化submodulegit clone git@repo.advai.net:shang.wang/test_1.gitcd test_1git submodule initgit submodule update 删除submodule 123456789cd test_1git rm --cached test_librm -rf test_librm .gitmodules删除git配置文件config中关于submodule的信息vim .git/config 找到submodule的部分 删掉git commit -a -m 'del submodule' 查看仓库中的submodule 1git submodule","categories":[{"name":"工具使用","slug":"工具使用","permalink":"https://shang.at/categories/工具使用/"}],"tags":[{"name":"gitlab","slug":"gitlab","permalink":"https://shang.at/tags/gitlab/"}]},{"title":"Linux-使用问答","slug":"Linux-使用问答","date":"2020-09-10T08:12:59.000Z","updated":"2020-09-10T08:14:14.808Z","comments":true,"path":"post/Linux-使用问答/","link":"","permalink":"https://shang.at/post/Linux-使用问答/","excerpt":"简介：","text":"简介： 什么是Linux?问：什么是Linux? 答：开源的操作系统，通过代码来引导计算机执行命令 答：Linux是一套免费使用和自由传播的类Unix操作系统，是一个基于POSIX和Unix的多用户、多任务、支持多线程和多CPU的操作系统。它能运行主要的Unix工具软件、应用程序和网络协议。它支持32位和64位硬件。Linux继承了Unix以网络为核心的设计思想，是一个性能稳定的多用户网络操作系统。 Unix和Linux有什么区别？问：Unix和Linux有什么区别？ 答：Linux和Unix都是功能强大的操作系统，都是应用广泛的服务器操作系统，有很多相似之处，甚至有一部分人错误地认为Unix和Linux操作系统是一样的，然而，事实并非如此，以下是两者的区别。开源性 Linux是一款开源操作系统，不需要付费，即可使用；Unix是一款对源码实行知识产权保护的传统商业软件，使用需要付费授权使用。跨平台性 Linux操作系统具有良好的跨平台性能，可运行在多种硬件平台上；Unix操作系统跨平台性能较弱，大多需与硬件配套使用。可视化界面 Linux除了进行命令行操作，还有窗体管理系统；Unix只是命令行下的系统。硬件环境 Linux操作系统对硬件的要求较低，安装方法更易掌握；Unix对硬件要求比较苛刻，按照难度较大。用户群体 Linux的用户群体很广泛，个人和企业均可使用；Unix的用户群体比较窄，多是安全性要求高的大型企业使用，如银行、电信部门等，或者Unix硬件厂商使用，如Sun等。 相比于Unix操作系统，Linux操作系统更受广大计算机爱好者的喜爱，主要原因是Linux操作系统具有Unix操作系统的全部功能，并且能够在普通PC计算机上实现全部的Unix特性，开源免费的特性，更容易普及使用！ 什么是 Linux 内核？问：什么是 Linux 内核？ 答：作用是将应用层序的请求传递给硬件，并充当底层驱动程序，对系统中的各种设备和组件进行寻址。 答：Linux 系统的核心是内核。内核控制着计算机系统上的所有硬件和软件，在必要时分配硬件，并根据需要执行软件。系统内存管理 应用程序管理 硬件设备管理 文件系统管理 Linux的基本组件是什么？问：Linux的基本组件是什么？ 答：内存管理,进程管理,进程间通信,虚拟文件系统和网络接口 答：就像任何其他典型的操作系统一样，Linux拥有所有这些组件：内核，shell和GUI，系统实用程序和应用程序。Linux比其他操作系统更具优势的是每个方面都附带其他功能，所有代码都可以免费下载。 Linux 的体系结构问：Linux 的体系结构 答：从大的方面讲，Linux 体系结构可以分为两块： 用户空间(User Space) ：用户空间又包括用户的应用程序(User Applications)、C 库(C Library) 。 内核空间(Kernel Space) ：内核空间又包括系统调用接口(System Call Interface)、内核(Kernel)、平台架构相关的代码(Architecture-Dependent Kernel Code) 。 为什么 Linux 体系结构要分为用户空间和内核空间的原因？问：为什么 Linux 体系结构要分为用户空间和内核空间的原因？ 答：1、现代 CPU 实现了不同的工作模式，不同模式下 CPU 可以执行的指令和访问的寄存器不同。 2、Linux 从 CPU 的角度出发，为了保护内核的安全，把系统分成了两部分。 用户空间和内核空间是程序执行的两种不同的状态，我们可以通过两种方式完成用户空间到内核空间的转移：1）系统调用；2）硬件中断。 BASH和DOS之间的基本区别是什么？问：BASH和DOS之间的基本区别是什么？ 答：BASH和DOS控制台之间的主要区别在于3个方面：BASH命令区分大小写，而DOS命令则不区分; 在BASH下，/ character是目录分隔符，\\作为转义字符。在DOS下，/用作命令参数分隔符，\\是目录分隔符 DOS遵循命名文件中的约定，即8个字符的文件名后跟一个点，扩展名为3个字符。BASH没有遵循这样的惯例。 Linux 开机启动过程？问：Linux 开机启动过程？ 答：1、主机加电自检，加载 BIOS 硬件信息。2、读取 MBR 的引导文件(GRUB、LILO)。3、引导 Linux 内核。4、运行第一个进程 init (进程号永远为 1 )。5、进入相应的运行级别。6、运行终端，输入用户名和密码。 Linux系统缺省的运行级别？问：Linux系统缺省的运行级别？ 答：关机。 单机用户模式。 字符界面的多用户模式(不支持网络)。 字符界面的多用户模式。 未分配使用。 图形界面的多用户模式。 重启。 Linux 使用的进程间通信方式？问：Linux 使用的进程间通信方式？ 答：1、管道(pipe)、流管道(s_pipe)、有名管道(FIFO)。 2、信号(signal) 。 3、消息队列。 4、共享内存。 5、信号量。 6、套接字(socket) 。 Linux 有哪些系统日志文件？问：Linux 有哪些系统日志文件？ 答：比较重要的是 /var/log/messages 日志文件。该日志文件是许多进程日志文件的汇总，从该文件可以看出任何入侵企图或成功的入侵。另外，如果胖友的系统里有 ELK 日志集中收集，它也会被收集进去。 Linux系统安装多个桌面环境有帮助吗？问：Linux系统安装多个桌面环境有帮助吗？ 答：通常，一个桌面环境，如KDE或Gnome，足以在没有问题的情况下运行。尽管系统允许从一个环境切换到另一个环境，但这对用户来说都是优先考虑的问题。有些程序在一个环境中工作而在另一个环境中无法工作，因此它也可以被视为选择使用哪个环境的一个因素。 什么是交换空间？问：什么是交换空间？ 答：交换空间是Linux使用的一定空间，用于临时保存一些并发运行的程序。当RAM没有足够的内存来容纳正在执行的所有程序时，就会发生这种情况。 什么是root帐户问：什么是root帐户 答：root帐户就像一个系统管理员帐户，允许你完全控制系统。你可以在此处创建和维护用户帐户，为每个帐户分配不同的权限。每次安装Linux时都是默认帐户。 什么是LILO？问：什么是LILO？ 答：LILO是Linux的引导加载程序。它主要用于将Linux操作系统加载到主内存中，以便它可以开始运行。 什么是BASH？问：什么是BASH？ 答：BASH是Bourne Again SHell的缩写。它由Steve Bourne编写，作为原始Bourne Shell（由/ bin / sh表示）的替代品。它结合了原始版本的Bourne Shell的所有功能，以及其他功能，使其更容易使用。从那以后，它已被改编为运行Linux的大多数系统的默认shell。 什么是CLI？问：什么是CLI？ 答： 12命令行界面（英语**：command-line interface**，缩写]：CLI）是在图形用户界面得到普及之前使用最为广泛的用户界面，它通常不支持鼠标，用户通过键盘输入指令，计算机接收到指令后，予以执行。也有人称之为字符用户界面（CUI）。 通常认为，命令行界面（CLI）没有图形用户界面（GUI）那么方便用户操作。因为，命令行界面的软件通常需要用户记忆操作的命令，但是，由于其本身的特点，命令行界面要较图形用户界面节约计算机系统的资源。在熟记命令的前提下，使用命令行界面往往要较使用图形用户界面的操作速度要快。所以，图形用户界面的操作系统中，都保留着可选的命令行界面。 什么是GUI？问：什么是GUI？ 答：图形用户界面 答：图形用户界面（Graphical User Interface，简称 GUI，又称图形用户接口）是指采用图形方式显示的计算机操作用户界面。图形用户界面是一种人与计算机通信的界面显示格式，允许用户使用鼠标等输入设备操纵屏幕上的图标或菜单选项，以选择命令、调用文件、启动程序或执行其它一些日常任务。与通过键盘输入文本或字符命令来完成例行任务的字符界面相比，图形用户界面有许多优点。 开源的优势是什么？问：开源的优势是什么？ 答：节省人力资源成本，同时有利于软件完善 答：开源允许你将软件（包括源代码）免费分发给任何感兴趣的人。然后，人们可以添加功能，甚至可以调试和更正源代码中的错误。它们甚至可以让它运行得更好，然后再次自由地重新分配这些增强的源代码。这最终使社区中的每个人受益。 GNU项目的重要性是什么？问：GNU项目的重要性是什么？ 答：这种所谓的自由软件运动具有多种优势，例如可以自由地运行程序以及根据你的需要自由学习和修改程序。它还允许你将软件副本重新分发给其他人，以及自由改进软件并将其发布给公众。 简单 Linux 文件系统？问：简单 Linux 文件系统？ 答： 123在 Linux 操作系统中，所有被操作系统管理的资源，例如网络接口卡、磁盘驱动器、打印机、输入输出设备、普通文件或是目录都被看作是一个文件。 也就是说在 Linux 系统中有一个重要的概念**：一切都是文件**。其实这是 Unix 哲学的一个体现，而 Linux 是重写 Unix 而来，所以这个概念也就传承了下来。在 Unix 系统中，把一切资源都看作是文件，包括硬件设备。UNIX系统把每个硬件都看成是一个文件，通常称为设备文件，这样用户就可以用读写文件的方式实现对硬件的访问。 Linux 支持 5 种文件类型，如下图所示： &lt;img src=&quot;https://ucc.alicdn.com/pic/developer-ecology/0e49bde8ba7249c2abbbe3a031b6a947.png&quot; alt=&quot;image.png&quot; /&gt; Linux 的目录结构是怎样的？问：Linux 的目录结构是怎样的？ 答： 12Linux 文件系统的结构层次鲜明，就像一棵倒立的树，最顶层是其根目录： &lt;img src=&quot;https://ucc.alicdn.com/pic/developer-ecology/61224ab457b144b08f514f6a720cc597.png&quot; alt=&quot;image.png&quot; /&gt; 常见目录说明： /bin： 存放二进制可执行文件(ls,cat,mkdir等)，常用命令一般都在这里； /etc： 存放系统管理和配置文件； /home： 存放所有用户文件的根目录，是用户主目录的基点，比如用户user的主目录就是/home/user，可以用~user表示； **/usr **： 用于存放系统应用程序； /opt： 额外安装的可选应用程序包所放置的位置。一般情况下，我们可以把tomcat等都安装到这里； /proc： 虚拟文件系统目录，是系统内存的映射。可直接访问这个目录来获取系统信息； /root： 超级用户（系统管理员）的主目录（特权阶级o）； /sbin: 存放二进制可执行文件，只有root才能访问。这里存放的是系统管理员使用的系统级别的管理命令和程序。如ifconfig等； /dev： 用于存放设备文件； /mnt： 系统管理员安装临时文件系统的安装点，系统提供这个目录是让用户临时挂载其他的文件系统； /boot： 存放用于系统引导时使用的各种文件； **/lib **： 存放着和系统运行相关的库文件 ； /tmp： 用于存放各种临时文件，是公用的临时文件存储点； /var： 用于存放运行时需要改变数据的文件，也是某些大文件的溢出区，比方说各种服务的日志文件（系统启动日志等。）等； /lost+found： 这个目录平时是空的，系统非正常关机而留下“无家可归”的文件（windows下叫什么.chk）就在这里。 什么是 inode ？问：什么是 inode ？ 答：理解inode，要从文件储存说起。文件储存在硬盘上，硬盘的最小存储单位叫做”扇区”（Sector）。每个扇区储存512字节（相当于0.5KB）。操作系统读取硬盘的时候，不会一个个扇区地读取，这样效率太低，而是一次性连续读取多个扇区，即一次性读取一个”块”（block）。这种由多个扇区组成的”块”，是文件存取的最小单位。”块”的大小，最常见的是4KB，即连续八个 sector组成一个 block。文件数据都储存在”块”中，那么很显然，我们还必须找到一个地方储存文件的元信息，比如文件的创建者、文件的创建日期、文件的大小等等。这种储存文件元信息的区域就叫做inode，中文译名为”索引节点”。每一个文件都有对应的inode，里面包含了与该文件有关的一些信息。 简述 Linux 文件系统通过 i 节点把文件的逻辑结构和物理结构转换的工作过程？问：简述 Linux 文件系统通过 i 节点把文件的逻辑结构和物理结构转换的工作过程？ 答：Linux 通过 inode 节点表将文件的逻辑结构和物理结构进行转换。inode 节点是一个 64 字节长的表，表中包含了文件的相关信息，其中有文件的大小、文件所有者、文件的存取许可方式以及文件的类型等重要信息。在 inode 节点表中最重要的内容是磁盘地址表。在磁盘地址表中有 13 个块号，文件将以块号在磁盘地址表中出现的顺序依次读取相应的块。 Linux 文件系统通过把 inode 节点和文件名进行连接，当需要读取该文件时，文件系统在当前目录表中查找该文件名对应的项，由此得到该文件相对应的 inode 节点号，通过该 inode 节点的磁盘地址表把分散存放的文件物理块连接成文件的逻辑结构。 什么是硬链接和软链接？问：什么是硬链接和软链接？ 答：1）硬链接由于 Linux 下的文件是通过索引节点(inode)来识别文件，硬链接可以认为是一个指针，指向文件索引节点的指针，系统并不为它重新分配 inode 。每添加一个一个硬链接，文件的链接数就加 1 。不足：1）不可以在不同文件系统的文件间建立链接；2）只有超级用户才可以为目录创建硬链接。 2）软链接软链接克服了硬链接的不足，没有任何文件系统的限制，任何用户可以创建指向目录的符号链接。因而现在更为广泛使用，它具有更大的灵活性，甚至可以跨越不同机器、不同网络对文件进行链接。不足：因为链接文件包含有原文件的路径信息，所以当原文件从一个目录下移到其他目录中，再访问链接文件，系统就找不到了，而硬链接就没有这个缺陷，你想怎么移就怎么移；还有它要系统分配额外的空间用于建立新的索引节点和保存原文件的路径。 实际场景下，基本是使用软链接。总结区别如下：硬链接不可以跨分区，软件链可以跨分区。 硬链接指向一个 inode 节点，而软链接则是创建一个新的 inode 节点。 删除硬链接文件，不会删除原文件，删除软链接文件，会把原文件删除。 RAID 是什么?问：RAID 是什么? 答：RAID 全称为独立磁盘冗余阵列(Redundant Array of Independent Disks)，基本思想就是把多个相对便宜的硬盘组合起来，成为一个硬盘阵列组，使性能达到甚至超过一个价格昂贵、 容量巨大的硬盘。RAID 通常被用在服务器电脑上，使用完全相同的硬盘组成一个逻辑扇区，因此操作系统只会把它当做一个硬盘。RAID 分为不同的等级，各个不同的等级均在数据可靠性及读写性能上做了不同的权衡。在实际应用中，可以依据自己的实际需求选择不同的 RAID 方案。 当然，因为很多公司都使用云服务，大家很难接触到 RAID 这个概念，更多的可能是普通云盘、SSD 云盘酱紫的概念。 一台 Linux 系统初始化环境后需要做一些什么安全工作？问：一台 Linux 系统初始化环境后需要做一些什么安全工作？ 答：1、添加普通用户登陆，禁止 root 用户登陆，更改 SSH 端口号。修改 SSH 端口不一定绝对哈。当然，如果要暴露在外网，建议改下。l2、服务器使用密钥登陆，禁止密码登陆。3、开启防火墙，关闭 SElinux ，根据业务需求设置相应的防火墙规则。4、装 fail2ban 这种防止 SSH 暴力破击的软件。5、设置只允许公司办公网出口 IP 能登陆服务器(看公司实际需要)也可以安装 VPN 等软件，只允许连接 VPN 到服务器上。6、修改历史命令记录的条数为 10 条。7、只允许有需要的服务器可以访问外网，其它全部禁止。8、做好软件层面的防护。8.1 设置 nginx_waf 模块防止 SQL 注入。 8.2 把 Web 服务使用 www 用户启动，更改网站目录的所有者和所属组为 www 。 什么叫 CC 攻击？什么叫 DDOS 攻击？问：什么叫 CC 攻击？什么叫 DDOS 攻击？ 答：ddos攻击又叫分布式拒绝服务攻击，可以使很多的计算机在同一时间遭受到攻击，使攻击的目标无法正常使用，分布式拒绝服务攻击已经出现了很多次，导致很多的大型网站都出现了无法进行操作的情况。 cc攻击在广泛定义上也属于ddos一种，cc攻击指的是攻击者控制某些主机不停地发大量数据包给对方服务器造成服务器资源耗尽，一直到宕机崩溃。 答：CC 攻击，主要是用来攻击页面的，模拟多个用户不停的对你的页面进行访问，从而使你的系统资源消耗殆尽。DDOS 攻击，中文名叫分布式拒绝服务攻击，指借助服务器技术将多个计算机联合起来作为攻击平台，来对一个或多个目标发动 DDOS 攻击。攻击，即是通过大量合法的请求占用大量网络资源，以达到瘫痪网络的目的。 怎么预防 CC 攻击和 DDOS 攻击？问：怎么预防 CC 攻击和 DDOS 攻击？ 答：防cc和ddos一般都是采用高防服务器或者云盾，游戏盾和web盾、高防服务器需要可以联系我，ddos无上限，cc事百分百防御，一旦被攻破都会赔偿。 答：防 CC、DDOS 攻击，这些只能是用硬件防火墙做流量清洗，将攻击流量引入黑洞。流量清洗这一块，主要是买 ISP 服务商的防攻击的服务就可以，机房一般有空余流量，我们一般是买服务，毕竟攻击不会是持续长时间。 什么是网站数据库注入？问：什么是网站数据库注入？ 答：由于程序员的水平及经验参差不齐，大部分程序员在编写代码的时候，没有对用户输入数据的合法性进行判断。 应用程序存在安全隐患。用户可以提交一段数据库查询代码，根据程序返回的结果，获得某些他想得知的数据，这就是所谓的 SQL 注入。 SQL注入，是从正常的 WWW 端口访问，而且表面看起来跟一般的 Web 页面访问没什么区别，如果管理员没查看日志的习惯，可能被入侵很长时间都不会发觉。 如何过滤与预防？数据库网页端注入这种，可以考虑使用 nginx_waf 做过滤与预防。 Shell 脚本是什么？问：Shell 脚本是什么？ 答：一个 Shell 脚本是一个文本文件，包含一个或多个命令。作为系统管理员，我们经常需要使用多个命令来完成一项任务，我们可以添加这些所有命令在一个文本文件(Shell 脚本)来完成这些日常工作任务。 什么是默认登录 Shell ？问：什么是默认登录 Shell ？ 答： 123456789在 Linux 操作系统，&quot;/bin/bash&quot; 是默认登录 Shell，是在创建用户时分配的。 使用 chsh 命令可以改变默认的 Shell 。示例如下所示： chsh &lt;用户名&gt; -s &lt;新shell&gt; chsh ThinkWon -s /bin/sh 1 2 在 Shell 脚本中，如何写入注释？ 注释可以用来描述一个脚本可以做什么和它是如何工作的。每一行注释以 # 开头。例子如下： #!/bin/bash This is a command echo “I am logged in as $USER” 可以在 Shell 脚本中使用哪些类型的变量？问：可以在 Shell 脚本中使用哪些类型的变量？ 答：在 Shell 脚本，我们可以使用两种类型的变量：系统定义变量系统变量是由系统系统自己创建的。这些变量通常由大写字母组成，可以通过 set 命令查看。用户定义变量用户变量由系统用户来生成和定义，变量的值可以通过命令 “echo $&lt;变量名&gt;” 查看。 Shell脚本中 $? 标记的用途是什么？问：Shell脚本中 $? 标记的用途是什么？ 答： 1234在写一个 Shell 脚本时，如果你想要检查前一命令是否执行成功，在 if 条件中使用 $? 可以来检查前一命令的结束状态。 如果结束状态是 0 ，说明前一个命令执行成功。例如： root@localhost:~## ls /usr/bin/shar /usr/bin/shar root@localhost:~## echo $? 0 1 2 3 4 如果结束状态不是0，说明命令执行失败。例如： root@localhost:~## ls /usr/bin/share ls: cannot access /usr/bin/share: No such file or directory root@localhost:~## echo $? 2 Bourne Shell(bash) 中有哪些特殊的变量？问：Bourne Shell(bash) 中有哪些特殊的变量？ 答： 12下面的表列出了 Bourne Shell 为命令行设置的特殊变量。 内建变量 解释 $0 命令行中的脚本名字 $1 第一个命令行参数 $2 第二个命令行参数 ….. ……. $9 第九个命令行参数 $## 命令行参数的数量 $* 所有命令行参数，以空格隔开 如何取消变量或取消变量赋值？问：如何取消变量或取消变量赋值？ 答：unset 命令用于取消变量或取消变量赋值。语法如下所示：unset &lt;变量名&gt; Shell 脚本中 if 语法如何嵌套?问：Shell 脚本中 if 语法如何嵌套? 答：if [ 条件 ] then 命令1 命令2 ….. else if [ 条件 ] then 命令1 命令2 …. else 命令1 命令2 ….. fi fi 在 Shell 脚本中如何比较两个数字？问：在 Shell 脚本中如何比较两个数字？ 答： 12在 if-then 中使用测试命令（ -gt 等）来比较两个数字。例如： #!/bin/bash x=10 y=20 if [ $x -gt $y ] then echo “x is greater than y” else echo “y is greater than x” fi Shell 脚本中 case 语句的语法?问：Shell 脚本中 case 语句的语法? 答：基础语法如下：case 变量 in 值1) 命令1 命令2 ….. 最后命令 !! 值2) 命令1 命令2 …… 最后命令 ;; esac Shell 脚本中 for 循环语法？问：Shell 脚本中 for 循环语法？ 答：for 变量 in 循环列表 do 命令1 命令2 …. 最后命令 done Shell 脚本中 while 循环语法？问：Shell 脚本中 while 循环语法？ 答：如同 for 循环，while 循环只要条件成立就重复它的命令块。 不同于 for循环，while 循环会不断迭代，直到它的条件不为真。基础语法：while [ 条件 ] do 命令… done do-while 语句的基本格式？问：do-while 语句的基本格式？ 答：do-while 语句类似于 while 语句，但检查条件语句之前先执行命令（LCTT 译注：意即至少执行一次。）。下面是用 do-while 语句的语法：do { 命令 } while (条件) Shell 脚本中 break 命令的作用？问：Shell 脚本中 break 命令的作用？ 答：break 命令一个简单的用途是退出执行中的循环。我们可以在 while 和 until 循环中使用 break 命令跳出循环。 Shell 脚本中 continue 命令的作用？问：Shell 脚本中 continue 命令的作用？ 答：continue 命令不同于 break 命令，它只跳出当前循环的迭代，而不是整个循环。continue 命令很多时候是很有用的，例如错误发生，但我们依然希望继续执行大循环的时候。 如何使脚本可执行?问：如何使脚本可执行? 答：使用 chmod 命令来使脚本可执行。例子如下：chmod a+x myscript.sh 。 #!/bin/bash 的作用？问：#!/bin/bash 的作用？ 答： 12#!/bin/bash 是 Shell 脚本的第一行，称为释伴（shebang）行。 这里 # 符号叫做 hash ，而 ! 叫做 bang。 它的意思是命令通过 /bin/bash 来执行。 如何调试 Shell脚本？问：如何调试 Shell脚本？ 答：使用 -x’ 数（sh -x myscript.sh）可以调试 Shell脚本。 另一个种方法是使用 -nv 参数(sh -nv myscript.sh)。 如何将标准输出和错误输出同时重定向到同一位置?问：如何将标准输出和错误输出同时重定向到同一位置? 答： 1方法一：2&gt;&amp;1 (如## ls /usr/share/doc &gt; out.txt 2&gt;&amp;1 ) 。 方法二：&amp;&gt; (如## ls /usr/share/doc &amp;&gt; out.txt ) 。 在 Shell 脚本中，如何测试文件？问：在 Shell 脚本中，如何测试文件？ 答：test 命令可以用来测试文件。基础用法如下表格：Test 用法 -d 文件名 如果文件存在并且是目录，返回true -e 文件名 如果文件存在，返回true -f 文件名 如果文件存在并且是普通文件，返回true -r 文件名 如果文件存在并可读，返回true -s 文件名 如果文件存在并且不为空，返回true -w 文件名 如果文件存在并可写，返回true -x 文件名 如果文件存在并可执行，返回true 在 Shell 脚本如何定义函数呢？问：在 Shell 脚本如何定义函数呢？ 答：函数是拥有名字的代码块。当我们定义代码块，我们就可以在我们的脚本调用函数名字，该块就会被执行。示例如下所示：$ diskusage () { df -h ; } 译注：下面是我给的shell函数语法，原文没有 [ function ] 函数名 [()] { 命令; [return int;] } 如何让 Shell 就脚本得到来自终端的输入?问：如何让 Shell 就脚本得到来自终端的输入? 答： 12345read 命令可以读取来自终端（使用键盘）的数据。read 命令得到用户的输入并置于你给出的变量中。例子如下： vi /tmp/test.sh #!/bin/bash echo ‘Please enter your name’ read name echo “My Name is $name” ./test.sh Please enter your name ThinkWon My Name is ThinkWon 如何执行算术运算？问：如何执行算术运算？ 答： 12有两种方法来执行算术运算： 1、使用 expr 命令：## expr 5 + 2 。 2、用一个美元符号和方括号（$[ 表达式 ]）：test=$[16 + 4] ; test=$[16 + 4] 。 判断一文件是不是字符设备文件，如果是将其拷贝到 /dev 目录下？问：判断一文件是不是字符设备文件，如果是将其拷贝到 /dev 目录下？ 答： 1#!/bin/bash read -p &quot;Input file name: &quot; FILENAME if [ -c &quot;$FILENAME&quot; ];then cp $FILENAME /dev fi 添加一个新组为 class1 ，然后添加属于这个组的 30 个用户，用户名的形式为 stdxx ，其问：添加一个新组为 class1 ，然后添加属于这个组的 30 个用户，用户名的形式为 stdxx ，其中 xx 从 01 到 30 ？ 答： 1#!/bin/bash groupadd class1 for((i=1;i&lt;31;i++)) do if [ $i -le 10 ];then useradd -g class1 std0$i else useradd -g class1 std$i fi done 编写 Shell 程序，实现自动删除 50 个账号的功能，账号名为stud1 至 stud50 ？问：编写 Shell 程序，实现自动删除 50 个账号的功能，账号名为stud1 至 stud50 ？ 答： 12345#!/bin/bashfor((i=1;i&lt;51;i++))do userdel -r stud$idone 写一个 sed 命令，修改 /tmp/input.txt 文件的内容？问：写一个 sed 命令，修改 /tmp/input.txt 文件的内容？ 要求：删除所有空行。 一行中，如果包含 “11111”，则在 “11111” 前面插入 “AAA”，在 “11111” 后面插入 “BBB” 。比如：将内容为 0000111112222 的一行改为 0000AAA11111BBB2222 。 答： 1234567891011121314151617181920212223242526272829303132[root@~]## cat -n /tmp/input.txt 1 000011111222 2 3 000011111222222 4 11111000000222 5 6 7 111111111111122222222222 8 2211111111 9 112222222 10 1122 11## 删除所有空行命令[root@~]## sed &apos;/^$/d&apos; /tmp/input.txt0000111112220000111112222221111100000022211111111111112222222222222111111111122222221122## 插入指定的字符[root@~]## sed &apos;s#\\(11111\\)#AAA\\1BBB#g&apos; /tmp/input.txt0000AAA11111BBB2220000AAA11111BBB222222AAA11111BBB000000222AAA11111BBBAAA11111BBB1112222222222222AAA11111BBB1111122222221122 如何选择 Linux 操作系统版本?问：如何选择 Linux 操作系统版本? 答：一般来讲，桌面用户首选 Ubuntu ；服务器首选 RHEL 或 CentOS ，两者中首选 CentOS 。根据具体要求：安全性要求较高，则选择 Debian 或者 FreeBSD 。需要使用数据库高级服务和电子邮件网络应用的用户可以选择 SUSE 。想要新技术新功能可以选择 Feddora ，Feddora 是 RHEL 和 CentOS 的一个测试版和预发布版本。【重点】根据现有状况，绝大多数互联网公司选择 CentOS 。现在比较常用的是 6 系列，现在市场占有大概一半左右。另外的原因是 CentOS 更侧重服务器领域，并且无版权约束。CentOS 7 系列，也慢慢使用的会比较多了。 如何规划一台 Linux 主机，步骤是怎样？问：如何规划一台 Linux 主机，步骤是怎样？ 答：1、确定机器是做什么用的，比如是做 WEB 、DB、还是游戏服务器。不同的用途，机器的配置会有所不同。2、确定好之后，就要定系统需要怎么安装，默认安装哪些系统、分区怎么做。3、需要优化系统的哪些参数，需要创建哪些用户等等的。 有哪些方面的因素会导致网站网站访问慢？问：有哪些方面的因素会导致网站网站访问慢？ 答：1、服务器出口带宽不够用本身服务器购买的出口带宽比较小。一旦并发量大的话，就会造成分给每个用户的出口带宽就小，访问速度自然就会慢。 跨运营商网络导致带宽缩减。例如，公司网站放在电信的网络上，那么客户这边对接是长城宽带或联通，这也可能导致带宽的缩减。 2、服务器负载过大，导致响应不过来可以从两个方面入手分析：分析系统负载，使用 w 命令或者 uptime 命令查看系统负载。如果负载很高，则使用 top 命令查看 CPU ，MEM 等占用情况，要么是 CPU 繁忙，要么是内存不够。 如果这二者都正常，再去使用 sar 命令分析网卡流量，分析是不是遭到了攻击。一旦分析出问题的原因，采取对应的措施解决，如决定要不要杀死一些进程，或者禁止一些访问等。 3、数据库瓶颈如果慢查询比较多。那么就要开发人员或 DBA 协助进行 SQL 语句的优化。 如果数据库响应慢，考虑可以加一个数据库缓存，如 Redis 等。然后，也可以搭建 MySQL 主从，一台 MySQL 服务器负责写，其他几台从数据库负责读。 4、网站开发代码没有优化好例如 SQL 语句没有优化，导致数据库读写相当耗时。 针对网站访问慢，怎么去排查？问：针对网站访问慢，怎么去排查？ 答：1、首先要确定是用户端还是服务端的问题。当接到用户反馈访问慢，那边自己立即访问网站看看，如果自己这边访问快，基本断定是用户端问题，就需要耐心跟客户解释，协助客户解决问题。不要上来就看服务端的问题。一定要从源头开始，逐步逐步往下。2、如果访问也慢，那么可以利用浏览器的调试功能，看看加载那一项数据消耗时间过多，是图片加载慢，还是某些数据加载慢。3、针对服务器负载情况。查看服务器硬件(网络、CPU、内存)的消耗情况。如果是购买的云主机，比如阿里云，可以登录阿里云平台提供各方面的监控，比如 CPU、内存、带宽的使用情况。4、如果发现硬件资源消耗都不高，那么就需要通过查日志，比如看看 MySQL慢查询的日志，看看是不是某条 SQL 语句查询慢，导致网站访问慢。 针对网站访问慢，怎么去解决？问：针对网站访问慢，怎么去解决？ 答：1、如果是出口带宽问题，那么久申请加大出口带宽。 2、如果慢查询比较多，那么就要开发人员或 DBA 协助进行 SQL 语句的优化。 3、如果数据库响应慢，考虑可以加一个数据库缓存，如 Redis 等等。然后也可以搭建MySQL 主从，一台 MySQL 服务器负责写，其他几台从数据库负责读。 4、申请购买 CDN 服务，加载用户的访问。 5、如果访问还比较慢，那就需要从整体架构上进行优化咯。做到专角色专用，多台服务器提供同一个服务。 Linux 性能调优都有哪几种方法？问：Linux 性能调优都有哪几种方法？ 答：1、Disabling daemons (关闭 daemons)。 2、Shutting down the GUI (关闭 GUI)。 3、Changing kernel parameters (改变内核参数)。 4、Kernel parameters (内核参数)。 5、Tuning the processor subsystem (处理器子系统调优)。 6、Tuning the memory subsystem (内存子系统调优)。 7、Tuning the file system (文件系统子系统调优)。 8、Tuning the network subsystem（网络子系统调优)。 cat 命令问：cat 命令 答：cat命令主要用来查看文件内容，创建文件，文件合并，追加文件内容等功能。eg:1、cat f1.txt，查看f1.txt文件的内容。 答：cat 命令用于连接文件并打印到标准输出设备上。cat 主要有三大功能：1.一次显示整个文件:cat filename 1 2.从键盘创建一个文件:cat &gt; filename 1 只能创建新文件，不能编辑已有文件。3.将几个文件合并为一个文件:cat file1 file2 &gt; file 1 -b 对非空输出行号 -n 输出所有行号 实例：（1）把 log2012.log 的文件内容加上行号后输入 log2013.log 这个文件里cat -n log2012.log log2013.log 1 （2）把 log2012.log 和 log2013.log 的文件内容加上行号（空白行不加）之后将内容附加到 log.log 里cat -b log2012.log log2013.log log.log 1 （3）使用 here doc 生成新文件cat &gt;log.txt &lt;&lt;EOF Hello World PWD=$(pwd) EOF ls -l log.txt cat log.txt Hello World PWD=/opt/soft/test 1 2 3 4 5 6 7 8 9 10 （4）反向列示 tac log.txt PWD=/opt/soft/test World Hello chmod 命令问：chmod 命令 答：改变一个或多个文件的存取模式(mode) 答：Linux/Unix 的文件调用权限分为三级 : 文件拥有者、群组、其他。利用 chmod 可以控制文件如何被他人所调用。用于改变 linux 系统文件或目录的访问权限。用它控制文件或目录的访问权限。该命令有两种用法。一种是包含字母和操作符表达式的文字设定法；另一种是包含数字的数字设定法。每一文件或目录的访问权限都有三组，每组用三位表示，分别为文件属主的读、写和执行权限；与属主同组的用户的读、写和执行权限；系统中其他用户的读、写和执行权限。可使用 ls -l test.txt 查找。以文件 log2012.log 为例：-rw-r—r— 1 root root 296K 11-13 06:03 log2012.log 1 第一列共有 10 个位置，第一个字符指定了文件类型。在通常意义上，一个目录也是一个文件。如果第一个字符是横线，表示是一个非目录的文件。如果是 d，表示是一个目录。从第二个字符开始到第十个 9 个字符，3 个字符一组，分别表示了 3 组用户对文件或者目录的权限。权限字符用横线代表空许可，r 代表只读，w 代表写，x 代表可执行。常用参数：-c 当发生改变时，报告处理信息 -R 处理指定目录以及其子目录下所有文件 1 2 权限范围：u ：目录或者文件的当前的用户 g ：目录或者文件的当前的群组 o ：除了目录或者文件的当前用户或群组之外的用户或者群组 a ：所有的用户及群组 1 2 3 4 权限代号：r ：读权限，用数字4表示 w ：写权限，用数字2表示 x ：执行权限，用数字1表示 - ：删除权限，用数字0表示 s ：特殊权限 1 2 3 4 5 实例：（1）增加文件 t.log 所有用户可执行权限chmod a+x t.log 1 （2）撤销原来所有的权限，然后使拥有者具有可读权限,并输出处理信息chmod u=r t.log -c 1 （3）给 file 的属主分配读、写、执行(7)的权限，给file的所在组分配读、执行(5)的权限，给其他用户分配执行(1)的权限chmod 751 t.log -c（或者：chmod u=rwx,g=rx,o=x t.log -c) 1 （4）将 test 目录及其子目录所有文件添加可读权限chmod u+r,g+r,o+r -R text/ -c chown 命令问：chown 命令 答：chown 将指定文件的拥有者改为指定的用户或组，用户可以是用户名或者用户 ID；组可以是组名或者组 ID；文件是以空格分开的要改变权限的文件列表，支持通配符。-c 显示更改的部分的信息 -R 处理指定目录及子目录 1 2 实例：（1）改变拥有者和群组 并显示改变信息chown -c mail:mail log2012.log 1 （2）改变文件群组chown -c :mail t.log 1 （3）改变文件夹及子文件目录属主及属组为 mailchown -cR mail: test/ cp 命令问：cp 命令 答：将源文件复制到你想移动到的文件 答：将源文件复制至目标文件，或将多个源文件复制至目标目录。注意：命令行复制，如果目标文件已经存在会提示是否覆盖，而在 shell 脚本中，如果不加 -i 参数，则不会提示，而是直接覆盖！-i 提示 -r 复制目录及目录内所有项目 -a 复制的文件与原文件时间一样 1 2 3 实例：（1）复制 a.txt 到 test 目录下，保持原文件时间，如果原文件存在提示是否覆盖。cp -ai a.txt test 1 （2）为 a.txt 建议一个链接（快捷方式）cp -s a.txt link_a.txt find 命令问：find 命令 答： 1234567891011用于在文件树中查找文件，并作出相应的处理。 命令格式： find pathname -options [-print -exec -ok ...] 1 命令参数： pathname: find命令所查找的目录路径。例如用.来表示当前目录，用/来表示系统根目录。 -print： find命令将匹配的文件输出到标准输出。 -exec： find命令对匹配的文件执行该参数所给出的shell命令。相应命令的形式为&apos;command&apos; &#123; &#125; ;，注意&#123; &#125;和\\；之间的空格。 -ok： 和-exec的作用相同，只不过以一种更为安全的模式来执行该参数所给出的shell命令，在执行每一个命令之前，都会给出提示，让用户来确定是否执行。 1 2 3 4 命令选项： -name 按照文件名查找文件 -perm 按文件权限查找文件 -user 按文件属主查找文件 -group 按照文件所属的组来查找文件。 -type 查找某一类型的文件，诸如： b - 块设备文件 d - 目录 c - 字符设备文件 l - 符号链接文件 p - 管道文件 f - 普通文件 1 2 3 4 5 6 7 8 9 10 11 实例： （1）查找 48 小时内修改过的文件 find -atime -2 1 （2）在当前目录查找 以 .log 结尾的文件。 . 代表当前目录 find ./ -name &apos;*.log&apos; 1 （3）查找 /opt 目录下 权限为 777 的文件 find /opt -perm 777 1 （4）查找大于 1K 的文件 find -size +1000c 1 查找等于 1000 字符的文件 find -size 1000c 1 -exec 参数后面跟的是 command 命令，它的终止是以 ; 为结束标志的，所以这句命令后面的分号是不可缺少的，考虑到各个系统中分号会有不同的意义，所以前面加反斜杠。&#123;&#125; 花括号代表前面find查找出来的文件名。 head 命令问：head 命令 答：head 用来显示档案的开头至标准输出中，默认 head 命令打印其相应文件的开头 10 行。常用参数：-n&lt;行数&gt; 显示的行数（行数为复数表示从最后向前数） 1 实例：（1）显示 1.log 文件中前 20 行head 1.log -n 20 1 （2）显示 1.log 文件前 20 字节head -c 20 log2014.log 1 （3）显示 t.log最后 10 行head -n -10 t.log less 命令问：less 命令 答：less 与 more 类似，但使用 less 可以随意浏览文件，而 more 仅能向前移动，却不能向后移动，而且 less 在查看之前不会加载整个文件。常用命令参数：-i 忽略搜索时的大小写 -N 显示每行的行号 -o &lt;文件名&gt; 将less 输出的内容在指定文件中保存起来 -s 显示连续空行为一行 /字符串：向下搜索“字符串”的功能 ?字符串：向上搜索“字符串”的功能 n：重复前一个搜索（与 / 或 ? 有关） N：反向重复前一个搜索（与 / 或 ? 有关） -x &lt;数字&gt; 将“tab”键显示为规定的数字空格 b 向后翻一页 d 向后翻半页 h 显示帮助界面 Q 退出less 命令 u 向前滚动半页 y 向前滚动一行 空格键 滚动一行 回车键 滚动一页 [pagedown]： 向下翻动一页 [pageup]： 向上翻动一页 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 实例：（1）ps 查看进程信息并通过 less 分页显示ps -aux | less -N 1 （2）查看多个文件less 1.log 2.log 1 可以使用 n 查看下一个，使用 p 查看前一个。 ln 命令问：ln 命令 答：功能是为文件在另外一个位置建立一个同步的链接，当在不同目录需要该问题时，就不需要为每一个目录创建同样的文件，通过 ln 创建的链接（link）减少磁盘占用量。链接分类：软件链接及硬链接软链接：1.软链接，以路径的形式存在。类似于Windows操作系统中的快捷方式 2.软链接可以 跨文件系统 ，硬链接不可以 3.软链接可以对一个不存在的文件名进行链接 4.软链接可以对目录进行链接 硬链接:1.硬链接，以文件副本的形式存在。但不占用实际空间。 2.不允许给目录创建硬链接 3.硬链接只有在同一个文件系统中才能创建 需要注意：第一：ln命令会保持每一处链接文件的同步性，也就是说，不论你改动了哪一处，其它的文件都会发生相同的变化； 第二：ln的链接又分软链接和硬链接两种，软链接就是ln –s 源文件 目标文件，它只会在你选定的位置上生成一个文件的镜像，不会占用磁盘空间，硬链接 ln 源文件 目标文件，没有参数-s， 它会在你选定的位置上生成一个和源文件大小相同的文件，无论是软链接还是硬链接，文件都保持同步变化。 第三：ln指令用在链接文件或目录，如同时指定两个以上的文件或目录，且最后的目的地是一个已经存在的目录，则会把前面指定的所有文件或目录复制到该目录中。若同时指定多个文件或目录，且最后的目的地并非是一个已存在的目录，则会出现错误信息。 常用参数：-b 删除，覆盖以前建立的链接 -s 软链接（符号链接） -v 显示详细处理过程 1 2 3 实例：（1）给文件创建软链接，并显示操作信息ln -sv source.log link.log 1 （2）给文件创建硬链接，并显示操作信息ln -v source.log link1.log 1 （3）给目录创建软链接ln -sv /opt/soft/test/test3 /opt/soft/test/test5 locate 命令问：locate 命令 答： 12345678locate 通过搜寻系统内建文档数据库达到快速找到档案，数据库由 updatedb 程序来更新，updatedb 是由 cron daemon 周期性调用的。默认情况下 locate 命令在搜寻数据库时比由整个由硬盘资料来搜寻资料来得快，但较差劲的是 locate 所找到的档案若是最近才建立或 刚更名的，可能会找不到，在内定值中，updatedb 每天会跑一次，可以由修改 crontab 来更新设定值 (etc/crontab)。 locate 与 find 命令相似，可以使用如 *、? 等进行正则匹配查找 常用参数： -l num（要显示的行数） -f 将特定的档案系统排除在外，如将proc排除在外 -r 使用正则运算式做为寻找条件 1 2 3 实例： （1）查找和 pwd 相关的所有文件(文件名中包含 pwd） locate pwd 1 （2）搜索 etc 目录下所有以 sh 开头的文件 locate /etc/sh 1 （3）查找 /var 目录下，以 reason 结尾的文件 locate -r &apos;^/var.*reason$&apos;（其中.表示一个字符，*表示任务多个；.*表示任意多个字符） more 命令问：more 命令 答：功能类似于 cat, more 会以一页一页的显示方便使用者逐页阅读，而最基本的指令就是按空白键（space）就往下一页显示，按 b 键就会往回（back）一页显示。命令参数：+n 从笫 n 行开始显示 -n 定义屏幕大小为n行 +/pattern 在每个档案显示前搜寻该字串（pattern），然后从该字串前两行之后开始显示 -c 从顶部清屏，然后显示 -d 提示“Press space to continue，’q’ to quit（按空格键继续，按q键退出）”，禁用响铃功能 -l 忽略Ctrl+l（换页）字符 -p 通过清除窗口而不是滚屏来对文件进行换页，与-c选项相似 -s 把连续的多个空行显示为一行 -u 把文件内容中的下画线去掉 1 2 3 4 5 6 7 8 9 常用操作命令：Enter 向下 n 行，需要定义。默认为 1 行 Ctrl+F 向下滚动一屏 空格键 向下滚动一屏 Ctrl+B 返回上一屏 = 输出当前行的行号 :f 输出文件名和当前行的行号 V 调用vi编辑器 !命令 调用Shell，并执行命令 q 退出more 1 2 3 4 5 6 7 8 9 实例：（1）显示文件中从第3行起的内容more +3 text.txt 1 （2）在所列出文件目录详细信息，借助管道使每次显示 5 行ls -l | more -5 1 按空格显示下 5 行。 mv 命令问：mv 命令 答： 12345678移动文件或修改文件名，根据第二参数类型（如目录，则移动文件；如为文件则重命令该文件）。 当第二个参数为目录时，第一个参数可以是多个以空格分隔的文件或目录，然后移动第一个参数指定的多个文件到第二个参数指定的目录中。 实例： （1）将文件 test.log 重命名为 test1.txt mv test.log test1.txt 1 （2）将文件 log1.txt,log2.txt,log3.txt 移动到根的 test3 目录中 mv llog1.txt log2.txt log3.txt /test3 1 （3）将文件 file1 改名为 file2，如果 file2 已经存在，则询问是否覆盖 mv -i log1.txt log2.txt 1 （4）移动当前文件夹下的所有文件到上一级目录 mv * ../ rm 命令问：rm 命令 答： 123456删除一个目录中的一个或多个文件或目录，如果没有使用 -r 选项，则 rm 不会删除目录。如果使用 rm 来删除文件，通常仍可以将该文件恢复原状。 rm [选项] 文件… 1 实例： （1）删除任何 .log 文件，删除前逐一询问确认： rm -i *.log 1 （2）删除 test 子目录及子目录中所有档案删除，并且不用一一确认： rm -rf test 1 （3）删除以 -f 开头的文件 rm -- -f* 答：删库跑路。。。。 tail 命令问：tail 命令 答：用于显示指定文件末尾内容，不指定文件时，作为输入信息进行处理。常用查看日志文件。常用参数：-f 循环读取（常用于查看递增的日志文件） -n&lt;行数&gt; 显示行数（从后向前） 1 2 （1）循环读取逐渐增加的文件内容ping 127.0.0.1 &gt; ping.log &amp; 1 后台运行：可使用 jobs -l 查看，也可使用 fg 将其移到前台运行。tail -f ping.log 1 （查看日志） touch 命令问：touch 命令 答： 12345678910111213141516Linux touch命令用于修改文件或者目录的时间属性，包括存取时间和更改时间。若文件不存在，系统会建立一个新的文件。 ls -l 可以显示档案的时间记录。 语法 touch [-acfm][-d&lt;日期时间&gt;][-r&lt;参考文件或目录&gt;] [-t&lt;日期时间&gt;][--help][--version][文件或目录…] 1 参数说明： a 改变档案的读取时间记录。 m 改变档案的修改时间记录。 c 假如目的档案不存在，不会建立新的档案。与 --no-create 的效果一样。 f 不使用，是为了与其他 unix 系统的相容性而保留。 r 使用参考档的时间记录，与 --file 的效果一样。 d 设定时间与日期，可以使用各种不同的格式。 t 设定档案的时间记录，格式与 date 指令相同。 –no-create 不会建立新档案。 –help 列出指令格式。 –version 列出版本讯息。 实例 使用指令&quot;touch&quot;修改文件&quot;testfile&quot;的时间属性为当前系统时间，输入如下命令： $ touch testfile #修改文件的时间属性 1 首先，使用ls命令查看testfile文件的属性，如下所示： $ ls -l testfile #查看文件的时间属性 #原来文件的修改时间为16:09 -rw-r--r-- 1 hdd hdd 55 2011-08-22 16:09 testfile 1 2 3 执行指令&quot;touch&quot;修改文件属性以后，并再次查看该文件的时间属性，如下所示： $ touch testfile #修改文件时间属性为当前系统时间 $ ls -l testfile #查看文件的时间属性 #修改后文件的时间属性为当前系统时间 -rw-r--r-- 1 hdd hdd 55 2011-08-22 19:53 testfile 1 2 3 4 使用指令&quot;touch&quot;时，如果指定的文件不存在，则将创建一个新的空白文件。例如，在当前目录下，使用该指令创建一个空白文件&quot;file&quot;，输入如下命令： $ touch file #创建一个名为“file”的新的空白文件 vim 命令问：vim 命令 答：Vim是从 vi 发展出来的一个文本编辑器。代码补完、编译及错误跳转等方便编程的功能特别丰富，在程序员中被广泛使用。打开文件并跳到第 10 行：vim +10 filename.txt 。 打开文件跳到第一个匹配的行：vim +/search-term filename.txt 。 以只读模式打开文件：vim -R /etc/passwd 。 基本上 vi/vim 共分为三种模式，分别是命令模式（Command mode），输入模式（Insert mode）和底线命令模式（Last line mode）。简单的说，我们可以将这三个模式想成底下的图标来表示： whereis 命令问：whereis 命令 答：whereis 命令只能用于程序名的搜索，而且只搜索二进制文件（参数-b）、man说明文件（参数-m）和源代码文件（参数-s）。如果省略参数，则返回所有信息。whereis 及 locate 都是基于系统内建的数据库进行搜索，因此效率很高，而find则是遍历硬盘查找文件。常用参数：-b 定位可执行文件。 -m 定位帮助文件。 -s 定位源代码文件。 -u 搜索默认路径下除可执行文件、源代码文件、帮助文件以外的其它文件。 1 2 3 4 实例：（1）查找 locate 程序相关文件whereis locate 1 （2）查找 locate 的源码文件whereis -s locate 1 （3）查找 lcoate 的帮助文件whereis -m locate which 命令问：which 命令 答：在 linux 要查找某个文件，但不知道放在哪里了，可以使用下面的一些命令来搜索：which 查看可执行文件的位置。 whereis 查看文件的位置。 locate 配合数据库查看文件位置。 find 实际搜寻硬盘查询文件名称。 1 2 3 4 which 是在 PATH 就是指定的路径中，搜索某个系统命令的位置，并返回第一个搜索结果。使用 which 命令，就可以看到某个系统命令是否存在，以及执行的到底是哪一个位置的命令。常用参数：-n 指定文件名长度，指定的长度必须大于或等于所有文件中最长的文件名。 1 实例：（1）查看 ls 命令是否存在，执行哪个which ls 1 （2）查看 whichwhich which 1 （3）查看 cdwhich cd（显示不存在，因为 cd 是内建命令，而 which 查找显示是 PATH 中的命令） 1 查看当前 PATH 配置：echo $PATH 1 或使用 env 查看所有环境变量及对应值 grep 命令问：grep 命令 答： 123456789101112131415161718192021强大的文本搜索命令，grep(Global Regular Expression Print) 全局正则表达式搜索。 grep 的工作方式是这样的，它在一个或多个文件中搜索字符串模板。如果模板包括空格，则必须被引用，模板后的所有字符串被看作文件名。搜索的结果被送到标准输出，不影响原文件内容。 命令格式： grep [option] pattern file|dir 1 常用参数： -A n --after-context显示匹配字符后n行 -B n --before-context显示匹配字符前n行 -C n --context 显示匹配字符前后n行 -c --count 计算符合样式的列数 -i 忽略大小写 -l 只列出文件内容符合指定的样式的文件名称 -f 从文件中读取关键词 -n 显示匹配内容的所在文件中行数 -R 递归查找文件夹 1 2 3 4 5 6 7 8 9 grep 的规则表达式: ^ #锚定行的开始 如：&apos;^grep&apos;匹配所有以grep开头的行。 $ #锚定行的结束 如：&apos;grep$&apos;匹配所有以grep结尾的行。 . #匹配一个非换行符的字符 如：&apos;gr.p&apos;匹配gr后接一个任意字符，然后是p。 * #匹配零个或多个先前字符 如：&apos;&lt;em&gt;grep&apos;匹配所有一个或多个空格后紧跟grep的行。 .&lt;/em&gt; #一起用代表任意字符。 [] #匹配一个指定范围内的字符，如&apos;[Gg]rep&apos;匹配Grep和grep。 [^] #匹配一个不在指定范围内的字符，如：&apos;[^A-FH-Z]rep&apos;匹配不包含A-R和T-Z的一个字母开头，紧跟rep的行。 (..) #标记匹配字符，如&apos;(love)&apos;，love被标记为1。 &lt; #锚定单词的开始，如:&apos;&lt;grep&apos;匹配包含以grep开头的单词的行。 &gt; #锚定单词的结束，如&apos;grep&gt;&apos;匹配包含以grep结尾的单词的行。 x&#123;m&#125; #重复字符x，m次，如：&apos;0&#123;5&#125;&apos;匹配包含5个o的行。 x&#123;m,&#125; #重复字符x,至少m次，如：&apos;o&#123;5,&#125;&apos;匹配至少有5个o的行。 x&#123;m,n&#125; #重复字符x，至少m次，不多于n次，如：&apos;o&#123;5,10&#125;&apos;匹配5--10个o的行。 \\w #匹配文字和数字字符，也就是[A-Za-z0-9]，如：&apos;G\\w*p&apos;匹配以G后跟零个或多个文字或数字字符，然后是p。 \\W #\\w的反置形式，匹配一个或多个非单词字符，如点号句号等。 \\b #单词锁定符，如: &apos;\\bgrep\\b&apos;只匹配grep。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 实例： （1）查找指定进程 ps -ef | grep svn 1 （2）查找指定进程个数 ps -ef | grep svn -c 1 （3）从文件中读取关键词 cat test1.txt | grep -f key.log 1 （4）从文件夹中递归查找以grep开头的行，并只列出文件 grep -lR &apos;^grep&apos; /tmp 1 （5）查找非x开关的行内容 grep &apos;^[^x]&apos; test.txt 1 （6）显示包含 ed 或者 at 字符的内容行 grep -E &apos;ed|at&apos; test.txt wc 命令问：wc 命令 答：wc(word count)功能为统计指定的文件中字节数、字数、行数，并将统计结果输出命令格式：wc [option] file.. 1 命令参数：-c 统计字节数 -l 统计行数 -m 统计字符数 -w 统计词数，一个字被定义为由空白、跳格或换行字符分隔的字符串 1 2 3 4 实例：（1）查找文件的 行数 单词数 字节数 文件名wc text.txt 1 结果：7 8 70 test.txt 1 （2）统计输出结果的行数cat test.txt | wc -l cd 命令问：cd 命令 答：进入某个目录 答：cd(changeDirectory) 命令语法：cd [目录名] 1 说明：切换当前目录至 dirName。实例：（1）进入要目录cd / 1 （2）进入 “home” 目录cd ~ 1 （3）进入上一次工作路径cd - 1 （4）把上个命令的参数作为cd参数使用。cd !$ df 命令问：df 命令 答：显示磁盘空间使用情况。获取硬盘被占用了多少空间，目前还剩下多少空间等信息，如果没有文件名被指定，则所有当前被挂载的文件系统的可用空间将被显示。默认情况下，磁盘空间将以 1KB 为单位进行显示，除非环境变量 POSIXLY_CORRECT 被指定，那样将以512字节为单位进行显示：-a 全部文件系统列表 -h 以方便阅读的方式显示信息 -i 显示inode信息 -k 区块为1024字节 -l 只显示本地磁盘 -T 列出文件系统类型 1 2 3 4 5 6 实例：（1）显示磁盘使用情况df -l 1 （2）以易读方式列出所有文件系统及其类型df -haT du 命令问：du 命令 答：du 命令也是查看使用空间的，但是与 df 命令不同的是 Linux du 命令是对文件和目录磁盘使用的空间的查看：命令格式：du [选项][文件] 1 常用参数：-a 显示目录中所有文件大小 -k 以KB为单位显示文件大小 -m 以MB为单位显示文件大小 -g 以GB为单位显示文件大小 -h 以易读方式显示文件大小 -s 仅显示总计 -c或—total 除了显示个别目录或文件的大小外，同时也显示所有目录或文件的总和 1 2 3 4 5 6 7 实例：（1）以易读方式显示文件夹内及子文件夹大小du -h scf/ 1 （2）以易读方式显示文件夹内所有文件大小du -ah scf/ 1 （3）显示几个文件或目录各自占用磁盘空间的大小，还统计它们的总和du -hc test/ scf/ 1 （4）输出当前目录下各个子目录所使用的空间du -hc —max-depth=1 scf/ ls命令问：ls命令 答：查看目录下有哪些东西 答：ls命令用来显示目标列表，在Linux中是使用率较高的命令。ls命令的输出信息可以进行彩色加亮显示，以分区不同类型的文件。 答： 123456789就是 list 的缩写，通过 ls 命令不仅可以查看 linux 文件夹包含的文件，而且可以查看文件权限(包括目录、文件夹、文件权限)查看目录信息等等。 常用参数搭配： ls -a 列出目录所有文件，包含以.开始的隐藏文件 ls -A 列出除.及..的其它文件 ls -r 反序排列 ls -t 以文件修改时间排序 ls -S 以文件大小排序 ls -h 以易读大小显示 ls -l 除了文件名之外，还将文件的权限、所有者、文件大小等信息详细列出来 1 2 3 4 5 6 7 实例： (1) 按易读方式按时间反序排序，并显示文件详细信息 ls -lhrt 1 (2) 按大小反序显示文件详细信息 ls -lrS 1 (3)列出当前目录中所有以&quot;t&quot;开头的目录的详细内容 ls -l t* 1 (4) 列出文件绝对路径（不包含隐藏文件） ls | sed &quot;s:^:pwd/:&quot; 1 (5) 列出文件绝对路径（包含隐藏文件） find $pwd -maxdepth 1 | xargs ls -ld mkdir 命令问：mkdir 命令 答：mkdir 命令用于创建文件夹。可用选项：-m: 对新建目录设置存取权限，也可以用 chmod 命令设置; -p: 可以是一个路径名称。此时若路径中的某些目录尚不存在,加上此选项后，系统将自动建立好那些尚不在的目录，即一次可以建立多个目录。 实例：（1）当前工作目录下创建名为 t的文件夹mkdir t 1 （2）在 tmp 目录下创建路径为 test/t1/t 的目录，若不存在，则创建：mkdir -p /tmp/test/t1/t pwd 命令问：pwd 命令 答：pwd 命令用于查看当前工作目录路径。实例：（1）查看当前路径pwd 1 （2）查看软链接的实际路径pwd -P rmdir 命令问：rmdir 命令 答：从一个目录中删除一个或多个子目录项，删除某目录时也必须具有对其父目录的写权限。注意：不能删除非空目录实例：（1）当 parent 子目录被删除后使它也成为空目录的话，则顺便一并删除：rmdir -p parent/child/child11 ifconfig 命令问：ifconfig 命令 答：ifconfig(interfaces config)。通常需要以root身份登录或使用sudo来使用ifconfig工具ifconfig 命令用来查看和配置网络设备。当网络环境发生改变时可通过此命令对网络进行相应的配置。备注：用ifconfig命令配置的网卡信息，在网卡重启后机器重启后，配置就不存在。要想将上述的配置信息永远的存的电脑里，那就要修改网卡的配置文件了。 答：ifconfig 用于查看和配置 Linux 系统的网络接口。 查看所有网络接口及其状态：ifconfig -a 。 使用 up 和 down 命令启动或停止某个接口：ifconfig eth0 up 和 ifconfig eth0 down 。 iptables 命令问：iptables 命令 答：iptables ，是一个配置 Linux 内核防火墙的命令行工具。功能非常强大，对于我们开发来说，主要掌握如何开放端口即可。例如：把来源 IP 为 192.168.1.101 访问本机 80 端口的包直接拒绝：iptables -I INPUT -s 192.168.1.101 -p tcp —dport 80 -j REJECT 。开启 80 端口，因为web对外都是这个端口iptables -A INPUT -p tcp —dport 80 -j ACCEP 1 另外，要注意使用 iptables save 命令，进行保存。否则，服务器重启后，配置的规则将丢失。 netstat 命令问：netstat 命令 答： 123456789101112Linux netstat命令用于显示网络状态。 利用netstat指令可让你得知整个Linux系统的网络情况。 语法 netstat [-acCeFghilMnNoprstuvVwx][-A&lt;网络类型&gt;][--ip] 1 参数说明： -a或–all 显示所有连线中的Socket。 -A&lt;网络类型&gt;或–&lt;网络类型&gt; 列出该网络类型连线中的相关地址。 -c或–continuous 持续列出网络状态。 -C或–cache 显示路由器配置的快取信息。 -e或–extend 显示网络其他相关信息。 -F或–fib 显示FIB。 -g或–groups 显示多重广播功能群组组员名单。 -h或–help 在线帮助。 -i或–interfaces 显示网络界面信息表单。 -l或–listening 显示监控中的服务器的Socket。 -M或–masquerade 显示伪装的网络连线。 -n或–numeric 直接使用IP地址，而不通过域名服务器。 -N或–netlink或–symbolic 显示网络硬件外围设备的符号连接名称。 -o或–timers 显示计时器。 -p或–programs 显示正在使用Socket的程序识别码和程序名称。 -r或–route 显示Routing Table。 -s或–statistice 显示网络工作信息统计表。 -t或–tcp 显示TCP传输协议的连线状况。 -u或–udp 显示UDP传输协议的连线状况。 -v或–verbose 显示指令执行过程。 -V或–version 显示版本信息。 -w或–raw 显示RAW传输协议的连线状况。 -x或–unix 此参数的效果和指定&quot;-A unix&quot;参数相同。 –ip或–inet 此参数的效果和指定&quot;-A inet&quot;参数相同。 实例 如何查看系统都开启了哪些端口？ [root@centos6 ~ 13:20 #55]# netstat -lnp Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1035/sshd tcp 0 0 :::22 :::* LISTEN 1035/sshd udp 0 0 0.0.0.0:68 0.0.0.0:* 931/dhclient Active UNIX domain sockets (only servers) Proto RefCnt Flags Type State I-Node PID/Program name Path unix 2 [ ACC ] STREAM LISTENING 6825 1/init @/com/ubuntu/upstart unix 2 [ ACC ] STREAM LISTENING 8429 1003/dbus-daemon /var/run/dbus/system_bus_socket 1 2 3 4 5 6 7 8 9 10 如何查看网络连接状况？ [root@centos6 ~ 13:22 #58]# netstat -an Active Internet connections (servers and established) Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN tcp 0 0 192.168.147.130:22 192.168.147.1:23893 ESTABLISHED tcp 0 0 :::22 :::* LISTEN udp 0 0 0.0.0.0:68 0.0.0.0:* 1 2 3 4 5 6 7 如何统计系统当前进程连接数？ 输入命令 netstat -an | grep ESTABLISHED | wc -l 。 输出结果 177 。一共有 177 连接数。 用 netstat 命令配合其他命令，按照源 IP 统计所有到 80 端口的 ESTABLISHED 状态链接的个数？ 严格来说，这个题目考验的是对 awk 的使用。 首先，使用 netstat -an|grep ESTABLISHED 命令。结果如下： tcp 0 0 120.27.146.122:80 113.65.18.33:62721 ESTABLISHED tcp 0 0 120.27.146.122:80 27.43.83.115:47148 ESTABLISHED tcp 0 0 120.27.146.122:58838 106.39.162.96:443 ESTABLISHED tcp 0 0 120.27.146.122:52304 203.208.40.121:443 ESTABLISHED tcp 0 0 120.27.146.122:33194 203.208.40.122:443 ESTABLISHED tcp 0 0 120.27.146.122:53758 101.37.183.144:443 ESTABLISHED tcp 0 0 120.27.146.122:27017 23.105.193.30:50556 ESTABLISHED ping 命令问：ping 命令 答：Linux ping命令用于检测主机。执行ping指令会使用ICMP传输协议，发出要求回应的信息，若远端主机的网络功能没有问题，就会回应该信息，因而得知该主机运作正常。指定接收包的次数ping -c 2 www.baidu.com telnet 命令问：telnet 命令 答：Linux telnet命令用于远端登入。执行telnet指令开启终端机阶段作业，并登入远端主机。语法telnet [-8acdEfFKLrx][-b&lt;主机别名&gt;][-e&lt;脱离字符&gt;][-k&lt;域名&gt;][-l&lt;用户名称&gt;][-n&lt;记录文件&gt;][-S&lt;服务类型&gt;][-X&lt;认证形态&gt;][主机名称或IP地址&lt;通信端口&gt;] 1 参数说明：-8 允许使用8位字符资料，包括输入与输出。 -a 尝试自动登入远端系统。 -b&lt;主机别名&gt; 使用别名指定远端主机名称。 -c 不读取用户专属目录里的.telnetrc文件。 -d 启动排错模式。 -e&lt;脱离字符&gt; 设置脱离字符。 -E 滤除脱离字符。 -f 此参数的效果和指定”-F”参数相同。 -F 使用Kerberos V5认证时，加上此参数可把本地主机的认证数据上传到远端主机。 -k&lt;域名&gt; 使用Kerberos认证时，加上此参数让远端主机采用指定的领域名，而非该主机的域名。 -K 不自动登入远端主机。 -l&lt;用户名称&gt; 指定要登入远端主机的用户名称。 -L 允许输出8位字符资料。 -n&lt;记录文件&gt; 指定文件记录相关信息。 -r 使用类似rlogin指令的用户界面。 -S&lt;服务类型&gt; 设置telnet连线所需的IP TOS信息。 -x 假设主机有支持数据加密的功能，就使用它。 -X&lt;认证形态&gt; 关闭指定的认证形态。 实例登录远程主机登录IP为 192.168.0.5 的远程主机telnet 192.168.0.5 date 命令问：date 命令 答：显示或设定系统的日期与时间。命令参数：-d&lt;字符串&gt; 显示字符串所指的日期与时间。字符串前后必须加上双引号。 -s&lt;字符串&gt; 根据字符串来设置日期与时间。字符串前后必须加上双引号。 -u 显示GMT。 %H 小时(00-23) %I 小时(00-12) %M 分钟(以00-59来表示) %s 总秒数。起算时间为1970-01-01 00:00:00 UTC。 %S 秒(以本地的惯用法来表示) %a 星期的缩写。 %A 星期的完整名称。 %d 日期(以01-31来表示)。 %D 日期(含年月日)。 %m 月份(以01-12来表示)。 %y 年份(以00-99来表示)。 %Y 年份(以四位数来表示)。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 实例：（1）显示下一天date +%Y%m%d —date=”+1 day” //显示下一天的日期 1 （2）-d参数使用date -d “nov 22” 今年的 11 月 22 日是星期三 date -d ‘2 weeks’ 2周后的日期 date -d ‘next monday’ (下周一的日期) date -d next-day +%Y%m%d（明天的日期）或者：date -d tomorrow +%Y%m%d date -d last-day +%Y%m%d(昨天的日期) 或者：date -d yesterday +%Y%m%d date -d last-month +%Y%m(上个月是几月) date -d next-month +%Y%m(下个月是几月) free 命令问：free 命令 答：显示系统内存使用情况，包括物理内存、交互区内存(swap)和内核缓冲区内存。命令参数：-b 以Byte显示内存使用情况 -k 以kb为单位显示内存使用情况 -m 以mb为单位显示内存使用情况 -g 以gb为单位显示内存使用情况 -s&lt;间隔秒数&gt; 持续显示内存 -t 显示内存使用总合 1 2 3 4 5 6 实例：（1）显示内存使用情况free free -k free -m 1 2 3 （2）以总和的形式显示内存的使用信息free -t 1 （3）周期性查询内存使用情况free -s 10 kill 命令问：kill 命令 答：发送指定的信号到相应进程。不指定型号将发送SIGTERM（15）终止指定进程。如果任无法终止该程序可用”-KILL” 参数，其发送的信号为SIGKILL(9) ，将强制结束进程，使用ps命令或者jobs 命令可以查看进程号。root用户将影响用户的进程，非root用户只能影响自己的进程。常用参数：-l 信号，若果不加信号的编号参数，则使用“-l”参数会列出全部的信号名称 -a 当处理当前进程时，不限制命令名和进程号的对应关系 -p 指定kill 命令只打印相关进程的进程号，而不发送任何信号 -s 指定发送信号 -u 指定用户 1 2 3 4 5 实例：（1）先使用ps查找进程pro1，然后用kill杀掉kill -9 $(ps -ef | grep pro1) ps 命令问：ps 命令 答：ps(process status)，用来查看当前运行的进程状态，一次性查看，如果需要动态连续结果使用 toplinux上进程有5种状态:运行(正在运行或在运行队列中等待)中断(休眠中, 受阻, 在等待某个条件的形成或接受到信号)不可中断(收到信号不唤醒和不可运行, 进程必须等待直到有中断发生)僵死(进程已终止, 但进程描述符存在, 直到父进程调用wait4()系统调用后释放)停止(进程收到SIGSTOP, SIGSTP, SIGTIN, SIGTOU信号后停止运行运行)ps 工具标识进程的5种状态码:D 不可中断 uninterruptible sleep (usually IO) R 运行 runnable (on run queue) S 中断 sleeping T 停止 traced or stopped Z 僵死 a defunct (”zombie”) process 1 2 3 4 5 命令参数：-A 显示所有进程 a 显示所有进程 -a 显示同一终端下所有进程 c 显示进程真实名称 e 显示环境变量 f 显示进程间的关系 r 显示当前终端运行的进程 -aux 显示所有包含其它使用的进程 1 2 3 4 5 6 7 8 实例：（1）显示当前所有进程环境变量及进程间关系ps -ef 1 （2）显示当前所有进程ps -A 1 （3）与grep联用查找某进程ps -aux | grep apache 1 （4）找出与 cron 与 syslog 这两个服务有关的 PID 号码ps aux | grep ‘(cron|syslog)’ rpm 命令问：rpm 命令 答：Linux rpm 命令用于管理套件。rpm(redhat package manager) 原本是 Red Hat Linux 发行版专门用来管理 Linux 各项套件的程序，由于它遵循 GPL 规则且功能强大方便，因而广受欢迎。逐渐受到其他发行版的采用。RPM 套件管理方式的出现，让 Linux 易于安装，升级，间接提升了 Linux 的适用度。查看系统自带jdkrpm -qa | grep jdk删除系统自带jdkrpm -e —nodeps 查看jdk显示的数据安装jdkrpm -ivh jdk-7u80-linux-x64.rpm top 命令问：top 命令 答：显示当前系统正在执行的进程的相关信息，包括进程 ID、内存占用率、CPU 占用率等常用参数：-c 显示完整的进程命令 -s 保密模式 -p &lt;进程号&gt; 指定进程显示 -n &lt;次数&gt;循环显示次数 1 2 3 4 实例：top - 14:06:23 up 70 days, 16:44, 2 users, load average: 1.25, 1.32, 1.35 Tasks: 206 total, 1 running, 205 sleeping, 0 stopped, 0 zombie Cpu(s): 5.9%us, 3.4%sy, 0.0%ni, 90.4%id, 0.0%wa, 0.0%hi, 0.2%si, 0.0%st Mem: 32949016k total, 14411180k used, 18537836k free, 169884k buffers Swap: 32764556k total, 0k used, 32764556k free, 3612636k cached PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 28894 root 22 0 1501m 405m 10m S 52.2 1.3 2534:16 java 1 2 3 4 5 6 7 前五行是当前系统情况整体的统计信息区。第一行，任务队列信息，同 uptime 命令的执行结果，具体参数说明情况如下：14:06:23 — 当前系统时间up 70 days, 16:44 — 系统已经运行了70天16小时44分钟（在这期间系统没有重启过的吆！）2 users — 当前有2个用户登录系统load average: 1.15, 1.42, 1.44 — load average后面的三个数分别是1分钟、5分钟、15分钟的负载情况。load average数据是每隔5秒钟检查一次活跃的进程数，然后按特定算法计算出的数值。如果这个数除以逻辑CPU的数量，结果高于5的时候就表明系统在超负荷运转了。第二行，Tasks — 任务（进程），具体信息说明如下：系统现在共有206个进程，其中处于运行中的有1个，205个在休眠（sleep），stoped状态的有0个，zombie状态（僵尸）的有0个。第三行，cpu状态信息，具体属性说明如下：5.9%us — 用户空间占用CPU的百分比。 3.4% sy — 内核空间占用CPU的百分比。 0.0% ni — 改变过优先级的进程占用CPU的百分比 90.4% id — 空闲CPU百分比 0.0% wa — IO等待占用CPU的百分比 0.0% hi — 硬中断（Hardware IRQ）占用CPU的百分比 0.2% si — 软中断（Software Interrupts）占用CPU的百分比 1 2 3 4 5 6 7 备注：在这里CPU的使用比率和windows概念不同，需要理解linux系统用户空间和内核空间的相关知识！第四行，内存状态，具体信息如下：32949016k total — 物理内存总量（32GB） 14411180k used — 使用中的内存总量（14GB） 18537836k free — 空闲内存总量（18GB） 169884k buffers — 缓存的内存量 （169M） 1 2 3 4 第五行，swap交换分区信息，具体信息说明如下：32764556k total — 交换区总量（32GB） 0k used — 使用的交换区总量（0K） 32764556k free — 空闲交换区总量（32GB） 3612636k cached — 缓冲的交换区总量（3.6GB） 1 2 3 4 第六行，空行。第七行以下：各进程（任务）的状态监控，项目列信息说明如下：PID — 进程id USER — 进程所有者 PR — 进程优先级 NI — nice值。负值表示高优先级，正值表示低优先级 VIRT — 进程使用的虚拟内存总量，单位kb。VIRT=SWAP+RES RES — 进程使用的、未被换出的物理内存大小，单位kb。RES=CODE+DATA SHR — 共享内存大小，单位kb S — 进程状态。D=不可中断的睡眠状态 R=运行 S=睡眠 T=跟踪/停止 Z=僵尸进程 %CPU — 上次更新到现在的CPU时间占用百分比 %MEM — 进程使用的物理内存百分比 TIME+ — 进程使用的CPU时间总计，单位1/100秒 COMMAND — 进程名称（命令名/命令行） 1 2 3 4 5 6 7 8 9 10 11 12 top 交互命令h 显示top交互命令帮助信息 c 切换显示命令名称和完整命令行 m 以内存使用率排序 P 根据CPU使用百分比大小进行排序 T 根据时间/累计时间进行排序 W 将当前设置写入~/.toprc文件中 o或者O 改变显示项目的顺序 yum 命令问：yum 命令 答： 123456yum（ Yellow dog Updater, Modified）是一个在Fedora和RedHat以及SUSE中的Shell前端软件包管理器。 基於RPM包管理，能够从指定的服务器自动下载RPM包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软体包，无须繁琐地一次次下载、安装。 yum提供了查找、安装、删除某一个、一组甚至全部软件包的命令，而且命令简洁而又好记。 1.列出所有可更新的软件清单命令：yum check-update 2.更新所有软件命令：yum update 3.仅安装指定的软件命令：yum install &lt;package_name&gt; 4.仅更新指定的软件命令：yum update &lt;package_name&gt; 5.列出所有可安裝的软件清单命令：yum list 6.删除软件包命令：yum remove &lt;package_name&gt; 7.查找软件包 命令：yum search 8.清除缓存命令: yum clean packages: 清除缓存目录下的软件包 yum clean headers: 清除缓存目录下的 headers yum clean oldheaders: 清除缓存目录下旧的 headers yum clean, yum clean all (= yum clean packages; yum clean oldheaders) :清除缓存目录下的软件包及旧的headers 实例 安装 pam-devel [root@www ~]# yum install pam-devel bzip2 命令问：bzip2 命令 答： 1创建 *.bz2 压缩文件：bzip2 test.txt 。 解压 *.bz2 文件：bzip2 -d test.txt.bz2 。 gzip 命令问：gzip 命令 答： 1创建一个 *.gz 的压缩文件：gzip test.txt 。 解压 *.gz 文件：gzip -d test.txt.gz 。 显示压缩的比率：gzip -l *.gz 。 tar 命令问：tar 命令 答： 1234567891011用来压缩和解压文件。tar 本身不具有压缩功能，只具有打包功能，有关压缩及解压是调用其它的功能来完成。 弄清两个概念：打包和压缩。打包是指将一大堆文件或目录变成一个总的文件；压缩则是将一个大的文件通过一些压缩算法变成一个小文件 常用参数： -c 建立新的压缩文件 -f 指定压缩文件 -r 添加文件到已经压缩文件包中 -u 添加改了和现有的文件到压缩包中 -x 从压缩包中抽取文件 -t 显示压缩文件中的内容 -z 支持gzip压缩 -j 支持bzip2压缩 -Z 支持compress解压文件 -v 显示操作过程 1 2 3 4 5 6 7 8 9 10 有关 gzip 及 bzip2 压缩: gzip 实例：压缩 gzip fileName .tar.gz 和.tgz 解压：gunzip filename.gz 或 gzip -d filename.gz 对应：tar zcvf filename.tar.gz tar zxvf filename.tar.gz bz2实例：压缩 bzip2 -z filename .tar.bz2 解压：bunzip filename.bz2或bzip -d filename.bz2 对应：tar jcvf filename.tar.gz 解压：tar jxvf filename.tar.bz2 1 2 3 4 5 实例： （1）将文件全部打包成 tar 包 tar -cvf log.tar 1.log,2.log 或tar -cvf log.* 1 （2）将 /etc 下的所有文件及目录打包到指定目录，并使用 gz 压缩 tar -zcvf /tmp/etc.tar.gz /etc 1 （3）查看刚打包的文件内容（一定加z，因为是使用 gzip 压缩的） tar -ztvf /tmp/etc.tar.gz 1 （4）要压缩打包 /home, /etc ，但不要 /home/dmtsai tar --exclude /home/dmtsai -zcvf myfile.tar.gz /home/* /etc unzip 命令问：unzip 命令 答： 1解压 *.zip 文件：unzip test.zip 。 查看 *.zip 文件的内容：unzip -l jasper.zip 。","categories":[{"name":"Linux","slug":"Linux","permalink":"https://shang.at/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://shang.at/tags/Linux/"}]},{"title":"java学习-基础2-Class的加载过程","slug":"java学习-基础2-Class的加载过程","date":"2020-08-24T22:54:32.000Z","updated":"2020-12-11T08:21:15.043Z","comments":true,"path":"post/java学习-基础2-Class的加载过程/","link":"","permalink":"https://shang.at/post/java学习-基础2-Class的加载过程/","excerpt":"简介：","text":"简介： Class加载过程 Loading 双亲委派，主要出于安全来考虑 LazyLoading 五种情况 new getstatic putstatic invokestatic指令，访问final变量除外 java.lang.reflect对类进行反射调用时 初始化子类的时候，父类首先初始化 虚拟机启动时，被执行的主类必须初始化 动态语言支持java.lang.invoke.MethodHandle解析的结果为REF_getstatic REF_putstatic REF_invokestatic的方法句柄时，该类必须初始化 ClassLoader的源码 findInCache -&gt; parent.loadClass -&gt; findClass() 自定义类加载器 extends ClassLoader overwrite findClass() -&gt; defineClass(byte[] -&gt; Class clazz) 加密 第一节课遗留问题：parent是如何指定的，打破双亲委派，学生问题桌面图片 用super(parent)指定 双亲委派的打破 如何打破：重写loadClass（） 何时打破过？ JDK1.2之前，自定义ClassLoader都必须重写loadClass() ThreadContextClassLoader可以实现基础类调用实现类代码，通过thread.setContextClassLoader指定 热启动，热部署 osgi tomcat 都有自己的模块指定classloader（可以加载同一类库的不同版本） 混合执行 编译执行 解释执行 检测热点代码：-XX:CompileThreshold = 10000 Linking Verification 验证文件是否符合JVM规定 Preparation 静态成员变量赋默认值 Resolution 将类、方法、属性等符号引用解析为直接引用常量池中的各种符号引用解析为指针、偏移量等内存地址的直接引用 Initializing 调用类初始化代码 &lt;clinit&gt;，给静态成员变量赋初始值 ClassLoader 任何一个Class被load到内存中实际上生成了两块内容： class文件的二进制内容被放置到内存的一块区域[Metaspace(1.8及以后)/PermGen(1.7及以前)] 同时[在堆内]生成了一个class类型的对象，指向该区域 其他的对象通过获取class类型对象的实例来访问class文件的内容 双亲委派的继承关系和父子关系，父加载器和子加载器没有继承关系，parent是在创建的时候指定的。ClassLoader有一个默认的无参的构造器，它会自动找到SystemClassLoader设置为自定义加载器的parent(SystemClassLoader默认为AppClassLoader) 什么叫做双亲委派：表示是一个从低到顶的过程和从顶到底的过程 下图是双亲委派的过程 为什么需要双亲委派？ 安全问题，比如不能让用户自定义加载器加载java.lang.String类型，可能带来安全问题。 防止重复加载 双亲委派的模式为什么能够避免安全问题？ 双亲委派把逻辑设计成了模板方法，即将整体流程规定死了，只将特定的步骤暴露给用户重写。这样的设计会保证classloader会先去parent加载器询问待加载的类是否已经被加载过，一直询问到Bootstrap Loader，如果每一层都返回空，那么返回子加载器，如果被加载过直接返回，否则抛出ClassNotFoundException异常 热加载原理：需要监测class文件的变化，如果发生变化，那么重新loading该class文件。其他的实例对象如何更新呢？还是会自动更新？ 要实现reloading，需要满足以下两个条件： 重写ClassLoader的loadClass方法，打破双亲委派机制 每次reload，都需要重新new一个ClassLoader出来，使用原来的ClassLoader对象进行loadClass会抛出LinkageError，表示不能reload重名的class 123456Exception in thread \"main\" java.lang.LinkageError: loader (instance of com/mashibing/jvm/c2_classloader/T012_ClassReloading2$MyLoader): attempted duplicate class definition for name: \"com/mashibing/jvm/Hello\" at java.lang.ClassLoader.defineClass1(Native Method) at java.lang.ClassLoader.defineClass(ClassLoader.java:763) at java.lang.ClassLoader.defineClass(ClassLoader.java:642) at com.mashibing.jvm.c2_classloader.T012_ClassReloading2$MyLoader.loadClass(T012_ClassReloading2.java:25) at com.mashibing.jvm.c2_classloader.T012_ClassReloading2.main(T012_ClassReloading2.java:40) 最终loadClass会一直调用到一个native的defineClass方法，在那里会检测同一个ClassLoader的load class的缓存. LazyLoading 执行模式 Linking Verification 验证文件是否符合JVM规定，检测class文件的格式以及内容是否符合class file format Preparation 静态成员变量赋默认值 Resolution 将类、方法、属性等符号引用解析为直接引用常量池中的各种符号引用解析为指针、偏移量等内存地址的直接引用 在Class File Format中我们知道，一个类文件中会记录了很多内容，其中包含fields信息，method信息，当前类信息，父类信息等。就拿记录的super_class这一项来说，一个没有明确声明的类肯定继承了Object，那么实际上super_class这一项是指向了常量池的某一条，而常量池的对应的那条记录是记录的Ljava/lang/Object这样的字符串，而在Runtime(运行时)，Ljava/lang/Object实际上是位于内存的某一个位置(Metaspace)，所谓的Resolution就是把super_class的这种字面引用转换成指向内存某个地址的直接引用 Initializing调用类初始化代码 &lt;clinit&gt;，给静态成员变量赋初始值 有一个非常有意思的情况，可以使用class的加载过程来解释： 代码一 12345678910111213141516171819public class T001_ClassLoadingProcedure &#123; public static void main(String[] args) &#123; System.out.println(T.count); &#125;&#125;class T &#123; public static T t = new T(); // null public static int count = 2; //0 //private int m = 8; private T() &#123; count ++; //System.out.println(\"--\" + count); &#125;&#125;// 打印结果： 2 代码二 12345678910111213141516171819public class T001_ClassLoadingProcedure &#123; public static void main(String[] args) &#123; System.out.println(T.count); &#125;&#125;class T &#123; public static int count = 2; //0 public static T t = new T(); // null //private int m = 8; private T() &#123; count ++; //System.out.println(\"--\" + count); &#125;&#125;// 打印结果：3 执行结束会发现上述两个代码的结果是不一样的，可以使用class的加载过程来解释： loading-&gt;linking(其中preparation：静态成员变量赋默认值)-&gt;initializing(调用类初始化代码 &lt;clinit&gt;，给静态成员变量赋初始值) 代码一：在linking-preparation阶段t=null；count=0，由于public static T t = new T();在前，所以会先执行它，那么就会执行new T()的过程，就会执行T的构造方法，那么此时count还是0，因此count++并没有生效，给count赋初始值之后，就把它给覆盖了，所以最终打印的count是2 代码二：这里public static int count = 2;在前，故会先给count的赋初始值2，然后执行new T()，count++是作用在了初始化之后的count上，故最终打印结果是3. 拓展：new一个对象的过程，也会分为两个阶段 开辟内存空间，这里会给成员变量赋默认值 给成员变量赋初始值 这里会牵扯到一个对象初始化过程中的CPU乱序执行的情况，DCL","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"Class的加载过程","slug":"Class的加载过程","permalink":"https://shang.at/tags/Class的加载过程/"}]},{"title":"数据结构与算法-左神学习笔记-高级5-Manacher算法及其扩展","slug":"数据结构与算法-左神学习笔记-高级5-Manacher算法及其扩展","date":"2020-08-19T07:55:51.000Z","updated":"2020-08-20T00:16:13.907Z","comments":true,"path":"post/数据结构与算法-左神学习笔记-高级5-Manacher算法及其扩展/","link":"","permalink":"https://shang.at/post/数据结构与算法-左神学习笔记-高级5-Manacher算法及其扩展/","excerpt":"简介：","text":"简介： 算法引子：计算一个字符串中的最大回文串的长度 什么是回文串：单个字符是回文串，’aba’是回文串，’aa’是回文串 暴力解求解：遍历字符串，寻找以每个字符为中心的所有回文子串。 时间复杂度是：O(N^2) 需要注意的点是回文串中有偶数个字符没法获取到，所以需要将原始字符串进行修改，开头和结尾以及每两个字符之间都插入一个辅助字符 暴力解法： 12 经历了暴力解法之后，Manacher闪亮登场，可以在O(N)的时间复杂度内解决该问题 概念： 回文半径、直径、区域 字符串为f#1#a#1#s，则以a为中心：回文直径为7，半径为4 回文半径数组，pArr[] 将每个字符为中心的回文子串的长度记录下来 放到pArr 回文最右右边界 R，初始值为-1 从左往右遍历字符串的时候，构成回文子串的最后边界的位置，如：#1#2#2#1#这个字符串，遍历到第一个位置时，R为0，因为#只有这一个字符；遍历到第二个字符的时候，R为2，因为从第二个字符可以往前往后扩 中心 C，初始值为-1 从左往右遍历字符串的时候，构成回文子串的中心位置，如：#1#2#2#1#这个字符串，遍历到第一个位置时，C为0，遍历到第二个字符的时候，C为1 Manacher就是基于这些概念的 假设有个字符串：#1#2#2#1#，长度为9，则pArr[9]， R=-1, C=-1 遍历到i个字符时，分为以下的情况： i在R外，含义是：位置i大于R 比如，初始情况下，i从0开始遍历，R=-1，这时0&gt;-1，表示i在R外 这种情况下，只能以i位置为中心，向两侧扩展，无法加速 i在R内，含义是：位置i小于等于R 那么一定会有如下的关系存在： C&lt;i&lt;=R，且存在一个以C为中心的对称点i’，如下图 i存在一个以C为中心的对称点i’ 讨论i’的位置情况： 以i’为中心的回文子串完全落在L…R之内：那么i位置的回文子串长度与i’一样 证明： 以i’为中心的回文子串的左边界落在了L的左侧：i位置为中心的回文子串的回文半径为R-i，则其长度为2*(R-i)+1 证明： 以i’为中心的回文子串的左边界正好落在L上：以i为中心的回文子串，需要从问号处继续向后扩，如果扩成功，同时更新R和C 证明： 伪代码： 时间复杂度的估计：把复杂度的估算绑定到R的变化上 延伸的问题：给定一个字符串str，要求给str后面补上一些字符串，要求补完的字符串整体四回文，问最少需要添加的字符串的个数。 思路：利用Manacher计算出来第一个包含最右字符的回文子串的回文半径和中心，然后将字符串补齐即可，如下图：","categories":[],"tags":[]},{"title":"数据结构与算法-左神学习笔记-公开课一","slug":"数据结构与算法-左神学习笔记-公开课一","date":"2020-08-12T08:00:42.000Z","updated":"2020-08-12T08:09:19.493Z","comments":true,"path":"post/数据结构与算法-左神学习笔记-公开课一/","link":"","permalink":"https://shang.at/post/数据结构与算法-左神学习笔记-公开课一/","excerpt":"简介：","text":"简介： 题目一给定一个有序数组arr，arr[i]表示i号点在x轴上的位置，所以整个arr表示一些点在x轴上从左往右的分布，给定一个正数len，表示绳子的长度，返回绳子最多能覆盖几个点？ 一、暴力解 O(n^2) 二、二分法O(N logN) 三、O(N) 题目二","categories":[],"tags":[]},{"title":"工具使用-编译工具-sbt","slug":"工具使用-编译工具-sbt","date":"2020-08-09T15:07:24.000Z","updated":"2021-04-13T03:28:30.549Z","comments":true,"path":"post/工具使用-编译工具-sbt/","link":"","permalink":"https://shang.at/post/工具使用-编译工具-sbt/","excerpt":"简介：使用sbt编译Scala项目","text":"简介：使用sbt编译Scala项目 SBT 是Scala 的构建工具，全称是Simple Build Tool， 类似Maven 或Gradle。 搭建一个SBT项目目录结构： 12345678910111213这是一个SBT项目的最简单的结构：atome-sg-apaylater-nightly-data-warehouse/ data-warehouse-core/ src/ main/ scala/ java/ resources/ test/ prject/ build.properties plugins.sbt build.sbt java语言和scala语言都是基于class的，都可以运行在JVM上，故在项目可以同时使用java语言和scala语言。 下面罗列一下相关的配置文件： build.sbt 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import sbt.Keys.dependencyOverrides// https://docs.aws.amazon.com/emr/latest/ReleaseGuide/images/emr-releases-5x.pngval emrMajorVersion = \"5.24\"val scalaFullVersion = \"2.11.12\"val scalaMajorVersion = \"2.11\"val sparkVersion = \"2.4.2\"val hadoopVersion = \"2.8.5\"val dataConsolidationRootVersion = \"1.0.0\"val dataConsolidationCoreVersion = \"1.0.0\"val dataOrganization = \"ai.advance\"val pureconfigVersion = \"0.10.0\"val commonSetting = Seq( compileOrder := CompileOrder.JavaThenScala, libraryDependencies ++= Seq( \"org.scalatest\" %% \"scalatest\" % \"3.2.0-SNAP10\" % \"test\", \"org.scalamock\" %% \"scalamock-scalatest-support\" % \"3.6.0\" % \"test\" ))val assemblySetting = Seq( assemblyOption in assembly := (assemblyOption in assembly).value.copy(includeScala = false), assemblyJarName in assembly := s\"$&#123;name.value&#125;_$scalaMajorVersion-$&#123;version.value&#125;-emr_$&#123;emrMajorVersion&#125;.jar\")lazy val core = (project in file(\"data-warehouse-core\")).settings( commonSetting ).settings( name := \"atome-sg-apaylater-nightly-data-warehouse\", organization := dataOrganization, version := dataConsolidationCoreVersion, scalaVersion := scalaFullVersion, test in assembly := &#123;&#125;, libraryDependencies ++= Seq( \"org.apache.spark\" % s\"spark-core_$&#123;scalaMajorVersion&#125;\" % sparkVersion % \"provided\", \"org.apache.spark\" % s\"spark-sql_$&#123;scalaMajorVersion&#125;\" % sparkVersion % \"provided\", \"org.scalacheck\" %% \"scalacheck\" % \"1.13.5\" % \"provided\", \"mysql\" % \"mysql-connector-java\" % \"5.1.38\" % \"provided\", \"com.github.pureconfig\" %% \"pureconfig\" % pureconfigVersion % \"provided\", \"com.typesafe.play\" %% \"play-json\" % \"2.6.9\" % \"provided\" ), dependencyOverrides += \"com.fasterxml.jackson.core\" % \"jackson-databind\" % \"2.6.7\" % \"provided\" ).settings( assemblySetting )lazy val root = (project in file(\".\")). settings( name := \"data-warehouse-root\", organization := dataOrganization, version := dataConsolidationRootVersion, scalaVersion := scalaFullVersion ).disablePlugins(sbtassembly.AssemblyPlugin) .aggregate(core) build.properties 1sbt.version = 0.13.18 plugins.sbt 12logLevel := Level.WarnaddSbtPlugin(\"com.eed3si9n\" % \"sbt-assembly\" % \"0.14.6\") 其中最主要的配置文件是build.sbt， 本地执行时，需要将lib的scope设置为compile，上线时再改成provided。 provided表示只在编译期有效，运行期不需要提供，所以运行时可能导致有些隐式转换找不到。","categories":[],"tags":[]},{"title":"Spark学习-源码一-wordcount","slug":"Spark学习-源码一-wordcount","date":"2020-08-09T00:12:46.000Z","updated":"2020-08-09T03:20:05.272Z","comments":true,"path":"post/Spark学习-源码一-wordcount/","link":"","permalink":"https://shang.at/post/Spark学习-源码一-wordcount/","excerpt":"简介：Spark源码学习-wordcount代码跟踪","text":"简介：Spark源码学习-wordcount代码跟踪 SparkContext：RPC、序列化、反序列化、零拷贝、堆内内存、堆外内存 sc.clean(f)：检查f是否可序列化 第一步数据文件testdata.txt 1234hello worldhello msbhello sparkgood spark 第二步spark脚本 1234567891011121314151617181920212223242526272829import org.apache.spark.rdd.RDDimport org.apache.spark.&#123;SparkConf, SparkContext&#125;object WordCountScala &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf() conf.setAppName(\"wordcount\") conf.setMaster(\"local\") //单击本地运行 val sc = new SparkContext(conf) val fileRDD: RDD[String] = sc.textFile(\"testdata.txt\") //hello world val words: RDD[String] = fileRDD.flatMap((x: String) =&gt; _.split(\" \")) //hello //world val pairWord: RDD[(String, Int)] = words.map((x: String) =&gt; (_, 1)) //(hello,1) //(hello,1) //(world,1) val res: RDD[(String, Int)] = pairWord.reduceByKey((x: Int, y: Int) =&gt; x + y) //X:oldValue Y:value //(hello,2) -&gt; (2,1) //(world,1) -&gt; (1,1) //(msb,2) -&gt; (2,1) res.foreach(println) &#125;&#125; 下面跟踪wordcount脚本的数据流的操作流程，从表面上看我们是调用了RDD的api，但实际上我们调用的这些个api只是构造了一个数据流程，并没有去实际的运行相关的逻辑，最后我们会引出，它底层实际上是基于我们最常见的一种编程模式——— 迭代器模式。 一个大数据处理框架，他为我们做了什么事情呢？ 1、读取数据，确定数据在集群中存储的位置 2、分配任务，任务具体会在哪里执行呢？遵循计算向数据移动的原则 3、根据我们写出来的脚本，生成任务的执行计划，DAG。 4、启动任务，调用我们的计算逻辑。 就像是我们平常所说的模板方法，函数回调(callback) 5、将结果写入目标文件 6、在这个过程中，框架还会帮助我们解决集群中的网络通信(RPC)、数据传输(编解码/序列化反序列化)、各种优化(零拷贝，内存管理、计算结果缓存) 第一步：val fileRDD: RDD[String] = sc.textFile(&quot;testdata.txt&quot;) 跟踪下去，他实际上是调用的SparkContext对象的textFile方法 123456789101112131415161718192021222324252627def textFile( path: String, minPartitions: Int = defaultMinPartitions): RDD[String] = withScope &#123; assertNotStopped() hadoopFile(path, classOf[TextInputFormat], classOf[LongWritable], classOf[Text], minPartitions).map(pair =&gt; pair._2.toString).setName(path)&#125;def hadoopFile[K, V]( path: String, inputFormatClass: Class[_ &lt;: InputFormat[K, V]], keyClass: Class[K], valueClass: Class[V], minPartitions: Int = defaultMinPartitions): RDD[(K, V)] = withScope &#123; assertNotStopped() FileSystem.getLocal(hadoopConfiguration) val confBroadcast = broadcast(new SerializableConfiguration(hadoopConfiguration)) val setInputPathsFunc = (jobConf: JobConf) =&gt; FileInputFormat.setInputPaths(jobConf, path) new HadoopRDD( this, confBroadcast, Some(setInputPathsFunc), inputFormatClass, keyClass, valueClass, minPartitions).setName(path)&#125; 发现最终实际上返回的一个HadoopRDD对象，并没有做什么具体的计算。OK，我们继续往下看： 第二步：val words: RDD[String] = fileRDD.flatMap((x: String) =&gt; _.split(&quot; &quot;))","categories":[],"tags":[]},{"title":"Java学习-IO学习4-C10K问题及NIO精讲和IO模型性能压测","slug":"Java学习-IO学习4-C10K问题及NIO精讲和IO模型性能压测","date":"2020-08-03T00:12:30.000Z","updated":"2020-08-03T00:12:30.028Z","comments":true,"path":"post/Java学习-IO学习4-C10K问题及NIO精讲和IO模型性能压测/","link":"","permalink":"https://shang.at/post/Java学习-IO学习4-C10K问题及NIO精讲和IO模型性能压测/","excerpt":"简介：","text":"简介：","categories":[],"tags":[]},{"title":"Java学习-IO学习3-Socket编程BIO及TCP参数","slug":"Java学习-IO学习3-Socket编程BIO及TCP参数","date":"2020-08-03T00:12:15.000Z","updated":"2021-02-22T23:54:09.361Z","comments":true,"path":"post/Java学习-IO学习3-Socket编程BIO及TCP参数/","link":"","permalink":"https://shang.at/post/Java学习-IO学习3-Socket编程BIO及TCP参数/","excerpt":"简介：","text":"简介：","categories":[],"tags":[]},{"title":"Java学习-IO学习2-PageCache-mmap","slug":"Java学习-IO学习2-PageCache-mmap","date":"2020-08-03T00:11:59.000Z","updated":"2020-08-03T00:11:59.864Z","comments":true,"path":"post/Java学习-IO学习2-PageCache-mmap/","link":"","permalink":"https://shang.at/post/Java学习-IO学习2-PageCache-mmap/","excerpt":"简介：","text":"简介：","categories":[],"tags":[]},{"title":"Java学习-IO学习1-虚拟文件系统-文件描述符-IO重定向","slug":"Java学习-IO学习1-虚拟文件系统-文件描述符-IO重定向","date":"2020-08-03T00:11:29.000Z","updated":"2021-02-18T10:58:08.047Z","comments":true,"path":"post/Java学习-IO学习1-虚拟文件系统-文件描述符-IO重定向/","link":"","permalink":"https://shang.at/post/Java学习-IO学习1-虚拟文件系统-文件描述符-IO重定向/","excerpt":"简介：","text":"简介： 操作系统宏观介绍 冯·诺依曼：计算器、控制器、主存储器、输入输出设备(I/O) 内存、CPU、IO设备（磁盘、网卡） kernel：操作系统内核，说白了就是一个程序，位于内核空间 kernel管理内存分配，磁盘IO VFS：虚拟 文件系统，是内存中的一个目录树，数上不同的节点映射到磁盘上不同的物理位置，每个物理位置，可以使不同的具体的文件系统。比如FAT，EXT4、或者网络节点，或者虚拟的一个块设备(后面有例子) 是一个位于底层硬件设备(不同文件系统会有不同的驱动)和上层应用程序之间的解耦层，是给用户空间程序暴露的一个统一接口，上层应用操作底层硬件设备的时候可以不用考虑具体的底层不同设备驱动的不同 inode：id，唯一标示一个文件(在linux中万物皆文件)，同一个文件的inode号是一样的 FD：文件描述符，指针-seek，每个应用程序获取各自的FD 以对文件进行读写，类似于java中迭代器模式，不同的应用程序可以自己控制读取文件的指针，而不会受到其他应用程序的影响 pagecache：也缓存，默认是4k大小，操作系统对pagecache是对所有上层应用程序打开的文件的一个统一的处理，并不是针对某一个特定的文件的。如果多个程序如果同时打开同一个文件，那么操作系统不会给每个应用程序需都重新加载一遍这个文件，他们会共用同一个pagecache。如果某个应用对文件内容进行了修改，那么操作系统会把该pagecache标记为dirty dirty：脏，如果app修改了pagecache的数据，pagecache就会被标记为dirty，dirty的标识是操作系统对所有上层打开文件的一个统一管理，并不是针对某一个文件的，会有一些系统参数来控制何时刷写dirty的数据 flush：文件内容在内存中发生了修改，什么时候刷写到磁盘。 应用程序控制flush，那么效率会非常低下 如果交由操作系统控制，比如当内存占用达到系统内存的n%，或者每隔n秒flush一次，则可能导致大量的数据丢失 了解了操作系统的这些bug，那么很多开源的系统他们在处理IO的时候都会有一些特殊的设计。 app：空户空间的程序，一般来说程序不能直接调用硬件设备，程序要想调用硬件设备，必须通过内核的系统调用 每个程序其实是一个逻辑的线性的内存地址，通过MMU给它映射到内存的物理地址。每个程序其实是模拟的使用整个内存空间 虚拟文件系统 /dev 目录比较特殊，他是设备挂载的目录 /dev/null /dev/zero 磁盘分区 如下图所示： 使用df命令可以查看当前系统的设备挂载情况 /dev/sda1是磁盘的1号分区 - boot分区 1该分区对应于/boot目录，约100MB.该分区存放Linux的Grub(bootloader)和内核源码。用户可通过访问/boot目录来访问该分区.换句话说，用户对/boot目录的操作就是操作该分区。 /dev/sda3是磁盘的2号分区 - 根分区 1在Linux操作系统中，除/boot目录外的其它所有目录都对应于该分区.因此，用户可通过访问除/boot目录外的其它所有目录来访问该分区。 tmpfs是交换分区 1234该分区没有对应的目录，故用户无法访问。Linux下的swap分区即为虚拟内存.虚拟内存用于当系统内存空间不足时，先将临时数据存放在swap分区，等待一段时间后，然后再将数据调入到内存中执行.所以说，虚拟内存只是暂时存放数据，在该空间内并没有执行。Ps:虚拟内存虚拟内存是指将硬盘上某个区域模拟为内存.因此虚拟内存的实际物理地址仍然在硬盘上.虚拟内存，或者说swap分区只能由系统访问，其大小为物理内存的2倍。 在上面的df的命令结果中，/下的所有目录除了/boot目录，其他目录都是在/dev/sda2分区下，/boot是独立挂载在了/dev/sda1分区下的，他覆盖了原始的/目录下的/boot目录 演示/boot的挂载过程： umount命令卸载/boot mount命令将/dev/sda1挂载到/boot上 在整个操作的过程中，我们发现结构目录树是趋向于稳定的，新挂载的设备不需要额外的操作。举个例子，可以给每个用户一个独立的硬盘，然后挂载在用户自己的目录下 文件类型： -：普通文件（可执行文件、图片、文本） REG d：目录 l：连接类型，分为软连接和硬链接。 共同的属性：修改任意一个文件的内容，其他的文件名对应的文件的内容都会发生变化 软连接：ln -s a.txt b.txt root前的数字1表示，该文件被引用的个数，发现都是1，说明软连接并不是直接指向物理文件的，如果把原始的xxoo.txt删掉，那么msb.txt就会报错，找不到原始文件 stat观察这两个文件的情况发现：这两个文件的inode号是不一样的 硬链接：ln a.txt b.txt，两个文件名指向了同一个物理位置 root前的数字2表示，该文件被引用的个数，发现都是2，说明软硬连接都是直接指向物理文件的，如果把原始的xxoo.txt删掉，那么对msb.txt并没有什么影响，只不过会导致该文件的被引用个数减为1 stat观察这两个文件的情况发现：这两个文件的inode号是一样的，且Links个数为2 b：读数据的时候，如果可以自由漂移的读，从任意位置读。如磁盘 c：读取数据的时候，不能读前面的，不能读到后面的，不能随便漂移的读，可能有时候还需要一些编解码和驱动的约束。如键盘、终端 CHR s：socket p：pipeline [eventpoll]： 演示镜像的创建过程： if：input file 使用dd命令创建一个镜像文件，if：input file,of：output file，bs写的基本单位是多少字节，count写多少个基本单位。这里发现创建了一个100M的镜像文件 losetup命令：设置循环设备 循环设备可以把文件虚拟成块设备（block device)，以便模拟整个文件系统，这样用户可以将其看作是硬盘驱动器，光驱或软驱等设备，并挂入当作目录来使用 loop好比是光驱，而文件就像光盘，放到loop中，之后挂载一下就可以访问 格式化虚拟磁盘 挂载虚拟磁盘到/mnt/ooxx这个目录，并指定文件系统为ext2 在mydisk.img这个镜像中，创建一个新的文件系统的命名空间 ldd跟着一个命令，可以找到该命令依赖的系统文件 花括号扩展用法如上 使用chroot命令把根目录切换到当前目录，会发现进入了一个新的命令行，echo $$查看当前的进程id 文件描述符，inodeid，脏读 lsof查看某个进程打开的文件，每个进程都存在三个特定的文件描述符：0 - 标准输入 1 - 标准输出 2 - 报错输出， 使用另一个进程，发现FD的偏移量并不会受到前一个进程的影响 socketexec命令可以实现重定向操作，如下操作，将/dev/tcp/www.baidu.com/80重定向到8号文件描述符 进入/proc/$$/fd目录下即可看到一个socket类型的文件 pipeline/proc - 是对内存中进程属性的一个映射目录，每个进程都有pid，对应一个独立的文件 重定向的标准符：&gt; 输出重定向 &lt; 输入重定向 cat 0&lt; ooxx.txt 1&gt;cat.out cat命令的标准输入来自于ooxx.txt文件，标准输出到cat.out文件 read a ： 标准输入来自键盘的输入，标准输出到变量a，对换行符敏感，遇到换行符则退出 read a 0&lt; cat.out 将标准输入重定向到cat.out文件 注意：0 1 2与重定向操作符之间不能有空白符， ls ./ /oososos 1&gt; ls01.out 2&gt; ls02.out 标准输出到ls01.out，错误输出到ls02.out ls ./ /oososos 1&gt; ls03.out 2&gt; ls03.out 标准输出和错误输出都到ls03.out，但是前面的输出会被后面的输出覆盖 ls ./ .ooosdfso 1&gt; ls04.out 2&gt;&amp; 1 将错误输出输出到标准输出，那么需要在&gt;紧跟一个&amp;符，同时要将1&gt; ls04.out放到2&gt;&amp; 1前面 管道 |如何读取文件的第8行，只显示第8行： head -8 test.txt | tail -1将前面的标准输出作为后面的标准输入 管道连接的两个指令块 是分别在两个子进程中执行的，操作系统会通过管道将第一个子进程的标准输出与第二个子进程的标准输入对接起来 比如：{ a=9; echo &quot;sdfs&quot; } | cat 进入正题： 当前bash进程的pid为4398 1&#123; echo $BASHPID; read x; &#125; | &#123; echo $BASHPID; read y; &#125; 指令块{ echo &quot;asdasd&quot;; echo &quot;123&quot; }，使用花括号标识一个代码块，一个代码块中可以有多条指令，使用分号隔开，他们会在同一个子进程中执行 父子进程 在父进程中定义的变量 在子进程中无法被读取，所以才会有一个export导出操作，在父进程中使用export导出的变量可以在子进程中被访问。 $和$BASHPID的区别$$$$优先级高于管道，bash是一个解释执行，它看到$$$$后，会直接把$$$$替换成当前进程的pid，开辟子进程的时候就已经被替换了 $$BASHPID优先级低于管道，会在开辟的子进程中被执行 PageCache 中断：告诉CPU现在该干什么事情了，触发中断的时候 会给CPU一个中断号，CPU会拿着这个中断号查询中断向量表，来得知该干什么事情了。","categories":[{"name":"IO","slug":"IO","permalink":"https://shang.at/categories/IO/"}],"tags":[{"name":"IO","slug":"IO","permalink":"https://shang.at/tags/IO/"}]},{"title":"Java学习-IO学习5-网络编程之多路复用器及Epoll","slug":"Java学习-IO学习5-网络编程之多路复用器及Epoll","date":"2020-08-03T00:08:15.000Z","updated":"2020-08-03T00:10:28.598Z","comments":true,"path":"post/Java学习-IO学习5-网络编程之多路复用器及Epoll/","link":"","permalink":"https://shang.at/post/Java学习-IO学习5-网络编程之多路复用器及Epoll/","excerpt":"简介：","text":"简介：","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"JAVA-IO","slug":"JAVA-IO","permalink":"https://shang.at/tags/JAVA-IO/"},{"name":"Epoll","slug":"Epoll","permalink":"https://shang.at/tags/Epoll/"}]},{"title":"Scala学习-0.3-抽象类","slug":"Scala学习-0-3-抽象类","date":"2020-07-31T00:34:32.000Z","updated":"2021-05-05T11:33:24.512Z","comments":true,"path":"post/Scala学习-0-3-抽象类/","link":"","permalink":"https://shang.at/post/Scala学习-0-3-抽象类/","excerpt":"简介：","text":"简介： 抽象类特质和抽象类可以包含一个抽象类型成员，意味着实际类型可由具体实现来确定。例如： 1234trait Buffer &#123; type T val element: T&#125; 这里定义的抽象类型T是用来描述成员element的类型的。通过抽象类来扩展这个特质后，就可以添加一个类型上边界来让抽象类型T变得更加具体。 12345abstract class SeqBuffer extends Buffer &#123; type U type T &lt;: Seq[U] def length = element.length&#125; 注意这里是如何借助另外一个抽象类型U来限定类型上边界的。通过声明类型T只可以是Seq[U]的子类（其中U是一个新的抽象类型），这个SeqBuffer类就限定了缓冲区中存储的元素类型只能是序列。 含有抽象类型成员的特质或类（classes）经常和匿名类的初始化一起使用。为了能够阐明问题，下面看一段程序，它处理一个涉及整型列表的序列缓冲区。 12345678910111213abstract class IntSeqBuffer extends SeqBuffer &#123; type U = Int&#125;def newIntSeqBuf(elem1: Int, elem2: Int): IntSeqBuffer = new IntSeqBuffer &#123; type T = List[U] val element = List(elem1, elem2) &#125;val buf = newIntSeqBuf(7, 8)println(\"length = \" + buf.length)println(\"content = \" + buf.element) 这里的工厂方法newIntSeqBuf使用了IntSeqBuf的匿名类实现方式，其类型T被设置成了List[Int]。 把抽象类型成员转成类的类型参数或者反过来，也是可行的。如下面这个版本只用了类的类型参数来转换上面的代码： 123456789101112131415abstract class Buffer[+T] &#123; val element: T&#125;abstract class SeqBuffer[U, +T &lt;: Seq[U]] extends Buffer[T] &#123; def length = element.length&#125;def newIntSeqBuf(e1: Int, e2: Int): SeqBuffer[Int, Seq[Int]] = new SeqBuffer[Int, List[Int]] &#123; val element = List(e1, e2) &#125;val buf = newIntSeqBuf(7, 8)println(\"length = \" + buf.length)println(\"content = \" + buf.element) 需要注意的是为了隐藏从方法newIntSeqBuf返回的对象的具体序列实现的类型，这里的型变标号（+T &lt;: Seq[U]）是必不可少的。此外要说明的是，有些情况下用类型参数替换抽象类型是行不通的。 类型边界实际上就是定义了父类与子类的关系 还是有点疑惑的… 类型上界 比如像T &lt;: A这样声明的类型上界表示类型变量T应该是类型A或者A的子类 类型下界 术语 B &gt;: A 表示类型参数 B 或抽象类型 B 是类型 A 的超类型。 在大多数情况下，A 将是类的类型参数，而 B 将是方法的类型参数。 类的构造辅助构造器(Auxiliary Constructor) 辅助构造器的名称为this 每个辅助构造器都必须以一个对先前已定义的其他辅助构造器或主构造器的调用开始 12345678910111213class Student &#123; private var name = \" \" private var age = 0 def this(name: String)&#123; //辅助构造器1 this() //调用主构造器 this.name = name &#125; def this(name: String,age: Int)&#123; //辅助构造器2 this(name) //调用前一个辅助构造器 this.age = age &#125;&#125; 现在有以下三种方式实例化对象: 1234567object Test1 &#123; def main(args: Array[String]): Unit = &#123; val s1 = new Student //主构造器 val s2 = new Student(\"KK\") //辅助构造器1 val s3 = new Student(\"Captian72\",18) //辅助构造器2 &#125;&#125; 主构造器(Primary Constructor)Scala中，每个类都有主构造器，但并不以this定义。 主构造器的参数直接放在类名之后 123class Student(val name: String,val age: Int)&#123; // 括号中的就是主构造器的参数 ......&#125; 只有主构造器的参数才会被编译成字段，其初始化的值为构造时传入的参数。 主构造器会执行类定义中的所有语句 123456789101112131415161718class Student (val name: String,val age: Int)&#123; println(\"Primary constructor and Auxiliary constructor!\") def fun = name + \" is \" + age +\" years old!\"&#125;object Test2 &#123; def main(args: Array[String]): Unit = &#123; val s1 = new Student(\"Captian72\",18) val s2 = new Student(\"KK\",20) println(s1.fun) println(s2.fun) &#125;&#125;// 输出为：Primary constructor and Auxiliary constructor!Primary constructor and Auxiliary constructor!Captian72 is 18 years old!KK is 20 years old! 说明：因为println语句是主构造器的一部分，所以在每次实例化的时候，会执行一次println。然后fun函数被调用2次，就执行两次fun函数的内容，即输出： 12Captian72 is 18 years old!KK is 20 years old! 在主构造器中使用默认参数，可防止辅助构造器使用过多 1class Student(val name:String=\"\",val age: Int = 0) 如果不带val或var的参数至少被一个方法使用，它将升格为字段 123class Dog(name:String, color:String)&#123; def fun = name + \" is \" + color + \" Dog!\"&#125; 上述代码声明并初始化了不可变字段name和color，并且这两个字段都是对象私有的。也就是说，类的方法，只能访问到当前对象的字段。 私有构造器(Private Constructor)想要让主构造器变成私有构造器，只需要加上private关键字即可。例如： 1class Dog private(val age: Int)&#123;...&#125; 这样做之后，就必须使用辅助构造器来构造Dog对象了。","categories":[],"tags":[]},{"title":"Scala学习-0.2-泛型","slug":"Scala学习-0-2-泛型","date":"2020-07-31T00:23:17.000Z","updated":"2020-08-05T08:42:04.288Z","comments":true,"path":"post/Scala学习-0-2-泛型/","link":"","permalink":"https://shang.at/post/Scala学习-0-2-泛型/","excerpt":"简介：","text":"简介： 与java不同，java中的泛型用&lt;&gt;包围，在scala中泛型用[]包围","categories":[{"name":"Scala学习","slug":"Scala学习","permalink":"https://shang.at/categories/Scala学习/"}],"tags":[{"name":"Scala泛型","slug":"Scala泛型","permalink":"https://shang.at/tags/Scala泛型/"}]},{"title":"Scala学习-0.1-import","slug":"Scala学习-0-1-import","date":"2020-07-31T00:23:01.000Z","updated":"2020-09-07T07:12:35.776Z","comments":true,"path":"post/Scala学习-0-1-import/","link":"","permalink":"https://shang.at/post/Scala学习-0-1-import/","excerpt":"简介：","text":"简介： Scala的import机制总结import 主要用于导入各种名字空间（package）或其包含的成员，使它们在声明的作用域里可见。 1 导入package 访问package的成员需要用导入的package名作为前缀，主要用于当package嵌套较多时，可以起到简化及隔离名字空间的作用。 当然任何时候使用完整package路径（full package path）都是可以的。如： 123456import java.utilnew util.ArrayList[String]() // 需要有util作为前缀，省略掉util的父package “java”new java.util.ArrayList[String] // 通过完整package路径访问import java.sqlnew sql.Date(new util.Date().getTime) // 使用package名隔离Date类的名字冲突 2 导入package下所有成员 相比Java中使用‘*’，Scala使用’_’表示导入package下所有成员. 如： 1234567891011121314151617scala&gt; val map = new HashMap[String,Int]&lt;console&gt;:7: error: not found: type HashMap val map = new HashMap[String,Int] ^scala&gt; import scala.collection.immutable.HashMapimport scala.collection.immutable.HashMapscala&gt; var map = new HashMap[String,Int]map: scala.collection.immutable.HashMap[String,Int] = Map()scala&gt; map += \"itang\" -&gt; 1scala&gt; mapres1: scala.collection.immutable.HashMap[String,Int] = Map(itang -&gt; 1)scala&gt; import java.util._scala&gt; new ArrayList[String]() 3 导入package下特定成员 指定package路径及要导入的类等的名称，如： 123import java.util.ArrayListnew ArrayList[Int] 4 导入package下特定成员，并重命名 在要导入package后使用”{}”,指定要导入的成员，并使用”=&gt;”重命名;如果在一个package下要导入多个成员，则使用逗号分隔.如: 12345678910111213141516scala&gt; import java.util.&#123;ArrayList =&gt; JList, HashMap =&gt; JMap&#125;scala&gt; new JList[Int]res: java.util.ArrayList[Int] = []scala&gt; type JStringList = JList[String]defined type alias JStringListscala&gt; new JStringListres: java.util.ArrayList[String] = []scala&gt; new JMap[String, String]res: java.util.HashMap[String,String] = &#123;&#125;scala&gt; import scala.Option.&#123;empty =&gt; jempty&#125;scala&gt; jemptyres: Option[Nothing] = None 5 导入package object的成员 scala中package除了充当命名空间之外，还可以通过package object可定义一些成员，形如： 123456789101112131415package testpackage object utils &#123; type JStringList = java.util.ArrayList[String] val Msg = \"Hello, World\" implicit def iw(target: Int) = new &#123; def times(proc: =&gt; Any) &#123; var i = 0 while (i &lt; target) &#123; proc i += 1 &#125; &#125; &#125;&#125; 123456import test.utils._object tt extends App &#123; println(Msg) new JStringList 10.times(println(\"hello\"))&#125; 6 导入object的成员 Scala中是没有Java中类中静态成员的概念，反之，Scala使用object class（对象类型）单例模式 Scala能导入object class的成员，也能导入运行时对象实例的成员。 在写spark应用的时候，import spark.implicits._的写法就是这种场景的一种应用，其中spark是SparkSession的对象。 12345678910scala&gt; import scala.Option._scala&gt; emptyres: Option[Nothing] = None case class User(name: String, age: Int)val a = new User(\"itang\", 18)println(\"%s's age is %d\" format (a.name, a.age))import a._println(\"%s's age is %d\", name, age) //output: itang's age is 18 7 import 可以位于表达式的能放的任意位置，而且导入的成员有其“可见”的作用域 如一下代码所示： 1234567891011121314151617181920212223import scala.actors.Actor._class TestPackagePosScope extends App &#123; actor &#123; println(\"Hello\") &#125; //new HashSet[String] ///not found type: HashSet import collection.immutable.HashSet new HashSet[String] def test1() &#123; import java.text.&#123; SimpleDateFormat =&gt; Sdf &#125; import java.util.Date new Sdf(\"yyyy-MM-dd\").format(new Date) &#125; //new Sdf(\"yyyy-MM-dd\").format(new Date) //not found type: Sdf &#123; import java.util._ val jlist = new ArrayList[String] &#125; //val badJlist = new ArrayList[String] // not found type: ArrayList&#125; import 使用上像表达式，但它不是表达式也不是函数，它没有返回值，存在于编译时，如 12345scala&gt; val some = import java.util._&lt;console&gt;:1: error: illegal start of simple expression val some = import java.util._ 8 scala默认已导入的名字空间及成员 scala包下成员默认可见 相当与每个编译单元里默认import scala._ 123scala&gt; Console.println(\"hello\") // scala.Consolescala&gt; Seq(1,2)scala&gt; scala.Seq(1,2) 注意到scala也是一个package object，里面定义了很多不同package的下类型别名，如此可以将众多的类型组织到scala名字空间。 java.lang包下成员默认可见 相当与每个编译单元里默认import java.lang._ 12scala&gt; Runtime.getRuntime.availableProcessors // java.lang.Runtimescala&gt; System.getProperty(\"user.home\") scala.Predef 这个object下成员默认可见（主要定义全局函数，类型别名，隐式转换等等) 相当与每个编译单元里默认import scala.Predef._ 1234scala&gt; println(\"hello\")scala&gt; scala.Predef.println(\"hell\")scala&gt; \"msg\" -&gt; \"Hello\"scala&gt; any2ArrowAssoc(\"msg\").-&gt;(\"Hello\") //scala.Predef.any2ArrowAssoc","categories":[{"name":"Scala学习","slug":"Scala学习","permalink":"https://shang.at/categories/Scala学习/"}],"tags":[{"name":"Scala-import","slug":"Scala-import","permalink":"https://shang.at/tags/Scala-import/"}]},{"title":"Spark学习-Encoder","slug":"Spark学习-Encoder","date":"2020-07-29T10:39:29.000Z","updated":"2020-07-29T10:39:44.621Z","comments":true,"path":"post/Spark学习-Encoder/","link":"","permalink":"https://shang.at/post/Spark学习-Encoder/","excerpt":"简介：","text":"简介：","categories":[],"tags":[{"name":"Encoder","slug":"Encoder","permalink":"https://shang.at/tags/Encoder/"}]},{"title":"Java学习-String","slug":"Java学习-String","date":"2020-07-29T10:23:44.000Z","updated":"2020-09-02T23:26:41.375Z","comments":true,"path":"post/Java学习-String/","link":"","permalink":"https://shang.at/post/Java学习-String/","excerpt":"简介：","text":"简介： intern方法介绍​ Oracle的开发文档上讲解的很详细：String类内部维护一个字符串池(strings pool)，当调用String的intern()方法时，如果字符串池中已经存在该字符串，则直接返回池中字符串引用，如果不存在，则将该字符串添加到池中，并返回该字符串对象的引用。执行过intern()方法的字符串，我们就说这个字符串被拘禁了(interned)。默认情况下，代码中的字符串字面量和字符串常量值都是被拘禁的，例如： 12345String s1 = \"abc\";String s2 =new String(\"abc\"); //返回trueSystem.out.println(s1 == s2.intern()); ​ 同值字符串的intern()方法返回的引用都相同，例如： 1234567String s2 = new String(\"abc\");String s3 = new String(\"abc\"); //返回trueSystem.out.println(s2.intern() == s3.intern());//返回falseSystem.out.println(s2 == s3);","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"String","slug":"String","permalink":"https://shang.at/tags/String/"}]},{"title":"Scala学习-函数式编程","slug":"Scala学习-函数式编程","date":"2020-07-29T08:51:56.000Z","updated":"2021-04-19T03:41:23.136Z","comments":true,"path":"post/Scala学习-函数式编程/","link":"","permalink":"https://shang.at/post/Scala学习-函数式编程/","excerpt":"简介：","text":"简介： 什么是函数式编程函数的职责是单一的 ——— 这和面向接口的编程模型是一致的 纯函数：没有副作用的函数 什么是副作用：从输入到输出的过程中还干了一些额外的事情 ​ 修改了某些值、与外部系统发生交互、直接修改数据结构 函数式编程的好处： 功能模块化、易测试、易复用、并行化、泛化、推导 定义函数式编程又称泛函编程是一种编程范型，它将计算机运算视为数学上的函数计算，并且避免使用程序状态以及易变对象。函数编程语言最重要的基础是λ演算（lambda calculus）。λ演算中最关键的要素就是函数被当作变量处理，能够参与运算。与之对应的，命令式编程是一种描述电脑所需作出的行为的编程范型。比起命令式编程，函数式编程更加强调执行的结果而非执行过程，倡导利用若干简单的执行单元让计算结果不断演进，抽丝剥茧逐层推导复杂的运算，而不是设计一个复杂的运算过程。总结一下就是 命令式编程(Imperative)命令“机器”如何去做事情(how)，这样不管你想要的是什么(what)，它都会按照你的命令实现。 函数式编程(functional)告诉“机器”你想要的是什么(what)，让机器想出如何去做(how)。 我们用一个使数组翻倍的例子来比较这两种编程范式。 Java Code \\ Imperative 123456Integer[] nums = new Integer[]&#123;1, 2, 3, 4&#125;;int i=0;for (Integer n : nums) &#123; n*=2; nums[i++]=n;&#125; 遍历整个数组，取出每个元素，乘以二，然后把翻倍后的值放回数组，每次都要操作这个数组，直到计算完所有元素。 Scala code \\ functional 12val nums = Array(1,2,3,4)val doubleNums = nums.map(_*2) 利用Array的map函数，将所有元素2，并生成了一个新的双倍数组。map函数所做的事情是把遍历整个数组的过程，归纳并抽离出来，让我们专注于描述我们想要的是什么”**_*2***”。我们传入map的是一个纯函数；它不具有任何副作用(不会改变外部状态)，它只是接收一个数字，返回乘以2后的值。PS： _ 字符是scala中的入参缩写形式。 正如封装、继承和多态是面向对象编程的三大特性。函数式编程也有自己的语言特性：数据不可变、函数是第一公民、引用透明和尾递归 *数据不可变（immutable data）：变量只赋值一次，如果想改变其值就创建一个新的。 *函数是第一公民（first class method）函数可以像普通变量一样去使用。函数可以像变量一样被创建，修改，并当成变量一样传递，返回或是在函数中嵌套函数。 *引用透明(referential transparency) 指的是函数的运行不依赖于外部变量或“状态”，只依赖于输入的参数，任何时候只要参数相同，调用函数所得到的返回值总是相同的。天然适应并发编程，因为调用函数的结果具有一致性，所以根本不需要加锁，也就不存在死锁的问题。 *尾递归（tail call optimization）:函数调用要压栈保存现场，递归层次过深的话，压栈过多会产生性能问题。所以引入尾递归优化，每次递归时都会重用栈，提升性能。 -————————————————————————————————————————————————————————————— 普通函数123def max(x: Int, y: Int): Int = &#123; if (x &gt; y) x else y&#125; def 是声明函数的关键字，小括号中的x:Int,y:int是函数的入参，紧跟着的:Int是函数返回值类型，花括号中的是函数体。(前一章介绍过，如果函数返回类型是非Unit，那么默认代码执行的最后一行的值，即为返回值，不需要显式的注明return关键字) 匿名函数123val increment = (x: Int) =&gt; x + 1increment(2)// output 3 匿名函数是指一类无需定义标识符的函数，定义匿名函数的语法很简单，=&gt;箭头左边是参数列表，右边是函数体。可以把命名函数赋值给一个变量使用。 另一种方式 123val increment =new Function1[Int, Int] &#123; def apply(x: Int): Int = x + 1&#125; 无参匿名函数123val sayHello = () =&gt; &#123; System.out.print(\"Hello World!\") &#125;sayHello()//output: Hello World! 高阶函数至少满足以下条件之一的函数，即为高阶函数 接受一个或多个函数作为输入 输出一个函数 在数学领域他们也叫做算子或泛函。微积分中的导数就是常见的例子，它映射一个函数到另一个函数。 1234val convert2String=(x:Int)=&gt;\"==[\" + x.toString() + \"]==\"def apply(f: Int =&gt; String, v: Int) = f(v)apply(convert2String,36)//output: ==[36]== 嵌入式函数(本地函数)在Scala中可以在函数体中再定义一个函数。 1234567def filter(xs: List[Int], threshold: Int) = &#123;def process(ys: List[Int]): List[Int] =if (ys.isEmpty) yselse if (ys.head &lt; threshold) ys.head :: process(ys.tail)else process(ys.tail) process(xs)&#125; 函数柯里化(Currying)在计算机科学中，柯里化是指把接受多个参数的函数变换成接受一个单一参数的函数，并且返回其余的参数和结果的函数。 123456789101112131415def curriedSum(x: Int)(y: Int)(z: Int) = x + y + zval sum = curriedSum(1)(2)(3)//output: 6//the calling procedure of currying functiondef first(x: Int) = (y: Int) =&gt; (z: Int) =&gt; x + y + zdef second = first(5)//output: Int =&gt; (Int =&gt; Int) = 5 + y + zdef third = second(6)//output: Int =&gt; Int = 5 + 6 + zval fourth = third(7)//output: 18first(5)(6)(7)//output: 18 代码范例中，curriedSum是一个currying函数的Scala写法，而first、second、third、fourth展示了currying函数调用过程。通过代码注释可看到second、third返回的都是函数，直到fourth才返回求和值。而这种特性也正是惰性求值的秘密所在。 部分应用函数假如有一个函数可以表示很多的含义，比如有一个函数fun(m,n)求$m^n$，我们更常用的可能是求$m^2$或者$m^3$，那么我们就可以固定次数为2或3，来创建一个新的函数，如fun2=fun(m, 2)和fun3=fun(m,3)。 123456def sum(x: Int, y: Int, z: Int) = &#123;x + y + z&#125;val sum2 = sum(10, _: Int, 15)sum2(2)//output: 27 偏函数(Partial Function)Scala定义了一个特殊的偏函数(Partial Function)。首先，我先来理解偏函数的概念。 从输入集合X到可能的输出集合Y的函数f(记作f(X)=&gt;Y)是X与Y的关系，满足如下条件： f是完全的即对于集合X中的任一元素x都有集合Y中的元素y，从而满足f(x)=y,换句话说就是每一个输入值x都有y与之对应。 f是多对一的即存在 且 并且 。也就是多个输入可以映射一个输出，但一个输入不能映射多个输出。 那么结论来了，X与Y的关系f满足条件1，则为多值函数。X与Y的关系f满足条件2，则为偏函数。 1234567891011val pf: PartialFunction[Int, String] = &#123; case 1 =&gt; \"One\" case 2 =&gt; \"Two\" case 3 =&gt; \"Three\" case 9 =&gt; \"Nine\" case _=&gt; \"Nothing\"&#125;pf(1) // output: Onepf(2) // output: Twopf(5) // output: Nothingpf(9) // output: Nine 示例代码中，我们定义了一个Scala的偏函数，那么只有1、2、3、9有对应的输出值，而输入其它值只会打印 Nothing。 那么偏函数有什么应用价值呢？函数式的编程思想是以一种“演绎法”而非“归纳法”去寻求解决空间。也就是说，它并不是要去归纳问题然后分解问题并解决问题，而是看请问题本质，定义最初的想法和组合规则，面对问题时，可以通过组合各种函数去解决问题，这也正是“组合子（combinator）”的含义。偏函数则更进一步，将函数求解空间中各个分支也分离出来，形成可以被组合的偏函数。 e.g. 偏函数使用orElse组成新的函数，得到的偏函数反映了是否对给定参数进行了定义。 123456789101112131415161718192021222324val one: PartialFunction[Int, String] = &#123; case 1 =&gt; \"one\"&#125;val two: PartialFunction[Int, String] = &#123;case 2 =&gt; \"two\"&#125;val three: PartialFunction[Int, String] = &#123;case 3 =&gt; \"three\"&#125;val wildcard: PartialFunction[Int, String] = &#123;case _ =&gt; \"something else\"&#125;val partial = one orElse two orElse three orElse wildcardpartial(5)//output: something elsepartial(3)//output: threepartial(2)//output: twopartial(1)//output: = onepartial(0)//output: something else 此外，在集合类的很多函数都带有这种偏函数的能力。e.g. map 函数 123456val L1 = List(1, 2, 3, \"A\", 4, \"B\", 5)val L2 = L1 map &#123; case x: Int =&gt; x * xcase x: String =&gt; s\"$x*$x\"&#125;//L2: List[Any] = List(1, 4, 9, A*A, 16, B*B, 25) 闭包又称函数闭包，是引用了自由变量的函数。这个被引用的自由变量将和这个函数一同存在，换句话说，闭包是由函数和与其相关的引用环境组合成的。 PS:另外一种解释，对象是带有函数的数据，闭包是带有数据的函数。 123val more = 2val addMore = (x: Int) =&gt; x + moreaddMore(1) 传值调用函数(call by value)先计算参数表达式的值，再应用到函数内部即为传值调用。 12345def assert(predicate: () =&gt; Boolean) =if (!predicate()) throw new AssertionErrorassert(() =&gt; (1 &gt; 3)) 传名调用函数(call by name)将未计算的参数表达式直接应用到函数内部即为传名调用。 123456789def assert(predicate: =&gt; Boolean) =if (!predicate) throw new AssertionErrorassert(1 &gt; 3)def compare(x: Int, y: Int): Boolean = &#123; if (x &gt; y) true else false&#125;assert(compare(1, 3)) 可变参函数12345def echo(args:String*)=&#123; for(arg &lt;- args) println(arg)&#125;echo(\"Hello\",\"World\",\"!\")//output: Hello World! 管道(pipeline)函数123456789101112def even(x:Array[Int]):Array[Int]=&#123; x.filter(_%2==0)&#125;def square(x:Array[Int]):Array[Int]=&#123; x.map(e=&gt;e*e)&#125;def total(x:Array[Int]):Int=&#123; x.reduce(_+_)&#125;val nums = Array(1,2,3,4,5,6,7,8,9,10)val pipeline = total(square(even(nums)))//output: 220 递归函数 1234567def fun(i:Int):Unit= &#123; if (i &lt; 10 &amp;&amp; i &gt;= 0) &#123; println(s\"i:$i\"); fun(i + 1); &#125;&#125;fun(1) Scala编译器优化了尾递归调用，递归调用和执行一个while循环没有性能上区别。 方法与函数在Scala中函数如果是某个对象的成员，那么称这种函数为方法。 12345class Sample&#123; def sayHello():Unit=&#123; print(\"Hello\") &#125;&#125; 方法的访问权限和Java有些不同，没有标注访问权限关键字的方法即为public。 private 12345678910object Sample&#123; def apply()=new Sample //Visible Sample().sayHello()&#125;class Sample &#123; private def sayHello():Unit=&#123; print(\"Hello\") &#125;&#125; 标注了private关键字，伴生对象和本对象可以访问。 private[this] 12345678910object Sample&#123; def apply()=new Sample //Invisible Sample().sayHello()&#125;class Sample &#123; private[this] def sayHello():Unit=&#123; print(\"Hello\") &#125;&#125; 为private标注了this限定词,则只有本对象可以访问。 protected 12345678910111213141516object Sample&#123; def apply()=new Sample&#125;class Sample &#123; protected[scala] def sayHello():Unit=&#123; print(\"Hello\") &#125;&#125;object OtherSample extends Sample&#123; //Visible Sample().sayHello()&#125;object ThirdSample &#123; //Invisible Sample().sayHello()&#125; Scala的方法被标注了protedted，就只比private多了能在子类中访问。 protected[packageName] 12345678910111213package com.stanley.scalaobject Sample&#123; def apply()=new Sample&#125;class Sample &#123; protected[stanley] def sayHello():Unit=&#123; print(\"Hello\") &#125;&#125;object OtherSample&#123; //Visible Sample().sayHello()&#125; 通过指定protected的限定词，可以指明在包中可见。样例代码中，标明了在 stanley 包中可见。 添加限定词后 。当限定词是Sample时，private[Sample] 等于 private。而protected[Sample]等于protected。当限定词是this时，protected[this] 子类中可见，伴生对象中不可见，private[this]只有本对象可见。当限定词是包名时，private和protected没有区别。 -———————————————————————————————————————————————————— 补充：Scala的lambda演算为了更好的理解函数式编程，还是补充一部分关于lambda的知识和大家交流。 lambda演算是一套用于研究函数定义、函数应用和递归的形式系统。它由阿隆佐·邱奇和他的学生斯蒂芬·科尔·克莱尼在20世纪30年代引入。邱奇运用λ演算在1936年给出判定性问题的一个否定的答案。这种演算可以用来清晰地定义什么是一个可计算函数。关于两个lambda演算表达式是否等价的命题无法通过一个“通用的算法”来解决，这是不可判定性能够证明的头一个问题，甚至还在停机问题之先。Lambda演算对函数式编程语言有巨大的影响，比如Lisp语言、ML语言和Haskell语言，包括Scala语言。 Lambda演算可以被称为最小的通用程序设计语言。它包括一条变换规则（变量替换）和一条函数定义方式，Lambda演算之通用在于，任何一个可计算函数都能用这种形式来表达和求值。因而，它是等价于图灵机的。尽管如此，Lambda演算强调的是变换规则的运用，而非实现它们的具体机器。可以认为这是一种更接近软件而非硬件的方式。 非形式化的描述在lambda演算中，每个表达式都代表一个函数，这个函数有一个参数，并且返回一个值。不论是参数和返回值，也都是一个单参的函数。可以这么说，lambda演算中，只有一种“类型”，那就是这种单参函数。 函数是通过lambda表达式匿名地定义的，这个表达式说明了此函数将对其参数进行什么操作。e.g.，“加2”函数def f(x)= x + 2可以用lambda演算表示为。 Scala的表达式 1(x)=&gt;x+2 考虑这么一个函数：它把一个函数作为参数，这个函数将被作用在3上 1f3 = (x:Int=&gt;Int)=&gt;x(3) 。如果把这个（用函数作参数的）函数作用于我们先前的“加2”函数上 1f3(f) ，则明显地，这三个表达式：、、是等价的。 1f3(f)、f(3)、3+2 有两个参数的函数可以通过lambda演算这么表达：一个单一参数的函数的返回值又是一个单一参数的函数（Currying）。例如，函数可以写作。 123def f(x:Int, y:Int) = x - y//Can this(x:Int,y:Int)=&gt;x-y 这三个表达式：作、、也是等价的。 123((x:Int,y:Int)=&gt;x-y)(7,2)((y:Int)=&gt;7-y)(2)7-2 然而这种lambda表达式之间的等价性无法找到一个通用的函数来判定。 并非所有的lambda表达式都可以规约至上述那样的确定值，考虑或 123((x:Int)=&gt;x)(((x:Int)=&gt;x)(((x:Int)=&gt;x)(((x:Int)=&gt;x)(0))))//OR((x:Int)=&gt;x)(((x:Int)=&gt;x)(((x:Int)=&gt;x)(((x:Int)=&gt;x)(((x:Int)=&gt;x)(((x:Int)=&gt;x)(0)))))) （注意:0只是个参数） 然后试图把第一个函数作用在它的参数上。被称为ω 组合子，被称为Ω，而被称为Ω2，以此类推。 lambda演算中的算术在lambda演算中有许多方式都可以定义自然数，但最常见的是Church数。邱奇整数是一个高阶函数，以单一参数函数f为参数，返回另一个单一参数的函数。利用函数被调用的次数标识自然数。 12345678910111213//Scala Sampledef zero[A](f: A =&gt; A) = (x: A) =&gt; xdef one[A](f: A =&gt; A) = (x: A) =&gt; f(x)def two[A](f: A =&gt; A) = (x: A) =&gt; f(f(x))def three[A](f: A =&gt; A) = (x: A) =&gt; f(f(f(x)))def four[A](f: A =&gt; A) = (x: A) =&gt; f(f(f(f(x))))def five[A](f: A =&gt; A) = (x: A) =&gt; f(f(f(f(f(x)))))println(\"zero: \" + zero[Int](i =&gt; i + 1)(0))println(\"one: \" + one[Int](i =&gt; i + 1)(0))println(\"two: \" + two[Int](i =&gt; i + 1)(0))println(\"three: \" + three[Int](i =&gt; i + 1)(0))println(\"four: \" + four[Int](i =&gt; i + 1)(0))println(\"four: \" + five[Int](i =&gt; i + 1)(0)) Church数定义的基础上，我们可以定义一个后继函数，它以n为参数，返回n + 1 12def succ[A](n: (A =&gt; A) =&gt; A =&gt; A) = (f: A =&gt; A) =&gt; (x: A) =&gt; f(n(f)(x))println(\"succ: \" + succ[Int](two)(i =&gt; i + 1)(0)) 定义加法 12def plus[A](m: (A =&gt; A) =&gt; A =&gt; A) = (n: (A =&gt; A) =&gt; A =&gt; A) =&gt; (f: A =&gt; A) =&gt; (x: A) =&gt; m(f)(n(f)(x))println(\"plus: \" + plus[Int](three)(two)(i =&gt; i + 1)(0)) 乘法可以这样定义 原始链接：https://zhuanlan.zhihu.com/p/25484213 集合数据操作scala.collection.TraversableOnce：定义了可遍历集合的操作 12// 是否为空def isEmpty: Boolean 12// 是否不为空def nonEmpty: Boolean = !isEmpty 12345678// 对满足p条件的元素计数，默认直接传True即可 def count(p: A =&gt; Boolean): Int = &#123; var cnt = 0 for (x &lt;- this) if (p(x)) cnt += 1 cnt &#125; 12// 对集合中的每个元素施加f操作，没有返回值def foreach[U](f: A =&gt; U): Unit 12// 检查某个元素在集合中是否存在def exists(p: A =&gt; Boolean): Boolean 12// 找到第一个满足p条件的元素，返回一个Option对象，表示可能找不到def find(p: A =&gt; Boolean): Option[A] 12// foldLeft的别名def /:[B](z: B)(op: (B, A) =&gt; B): B = foldLeft(z)(op) 12345678910 // 最重要的一个方法。很多方法基本都是基于此方法演化出来的// 对当前的集合的每一个元素 从左到右的施加一个有初始值(z)二值操作(op)// 返回类型B与初始值z的类型一致// op接收的两个参数类型分别为：B和A，A为原始集合元素的类型；B是op每一次处理的结果；初始情况：B=z // 最终的效果是：1-&gt;2-&gt;3-&gt;4 z op(op(op(op(z, 1), 2), 3), 4)def foldLeft[B](z: B)(op: (B, A) =&gt; B): B = &#123; var result = z this foreach (x =&gt; result = op(result, x)) result &#125; 12// foldRight的别名def :\\[B](z: B)(op: (A, B) =&gt; B): B = foldRight(z)(op) 12def foldRight[B](z: B)(op: (A, B) =&gt; B): B = reversed.foldLeft(z)((x, y) =&gt; op(y, x)) 1234567891011121314151617181920// 将集合中的元素从左到右的施加一个二值操作(op)// 返回结果类型是集合元素本身的类型或者是其父类型// 1-&gt;2-&gt;3-&gt;4 op(op(op(1, 2), 3), 4)// 实质上就是没有初始值的foldLeftdef reduceLeft[B &gt;: A](op: (B, A) =&gt; B): B = &#123; if (isEmpty) throw new UnsupportedOperationException(\"empty.reduceLeft\") var first = true var acc: B = 0.asInstanceOf[B] for (x &lt;- self) &#123; if (first) &#123; acc = x first = false &#125; else acc = op(acc, x) &#125; acc&#125; 123456def reduceRight[B &gt;: A](op: (A, B) =&gt; B): B = &#123; if (isEmpty) throw new UnsupportedOperationException(\"empty.reduceRight\") reversed.reduceLeft[B]((x, y) =&gt; op(y, x))&#125; 123456// 集合为空的时候返回None，不为空的时候返回执行结果def reduceLeftOption[B &gt;: A](op: (B, A) =&gt; B): Option[B] = if (isEmpty) None else Some(reduceLeft(op))def reduceRightOption[B &gt;: A](op: (A, B) =&gt; B): Option[B] = if (isEmpty) None else Some(reduceRight(op)) 123def reduce[A1 &gt;: A](op: (A1, A1) =&gt; A1): A1 = reduceLeft(op)def reduceOption[A1 &gt;: A](op: (A1, A1) =&gt; A1): Option[A1] = reduceLeftOption(op) 123def fold[A1 &gt;: A](z: A1)(op: (A1, A1) =&gt; A1): A1 = foldLeft(z)(op)def aggregate[B](z: =&gt;B)(seqop: (B, A) =&gt; B, combop: (B, B) =&gt; B): B = foldLeft(z)(seqop) 123456789// 使用foldLeft来实现求和 必须是数字类型的元素// z=num.zero// op=num.plusdef sum[B &gt;: A](implicit num: Numeric[B]): B = foldLeft(num.zero)(num.plus)// z=num.one// op=num.timesdef product[B &gt;: A](implicit num: Numeric[B]): B = foldLeft(num.one)(num.times)// 这里会自动寻找Numeric的隐式转换对象，使用sun和product时只需要直接调用即可 12345678910111213141516// 这里的op=(x, y) =&gt; if (cmp.lteq(x, y)) x else y// 会自动去寻找Ordering的隐式转换对象，如果是原始元素就是数字类型的，那么这个参数可以不用传，但是如果原始元素类型不是数字这样可比较的类型，那么需要自顶一个Ordering规则，作为参数传递进来// 或者原始的元素类型实现了java.util.Comparator接口def min[B &gt;: A](implicit cmp: Ordering[B]): A = &#123; if (isEmpty) throw new UnsupportedOperationException(\"empty.min\") reduceLeft((x, y) =&gt; if (cmp.lteq(x, y)) x else y)&#125;def max[B &gt;: A](implicit cmp: Ordering[B]): A = &#123; if (isEmpty) throw new UnsupportedOperationException(\"empty.max\") reduceLeft((x, y) =&gt; if (cmp.gteq(x, y)) x else y)&#125; scala.collection.TraversableLike 1234567891011121314// 对集合中的每个元素施加一个f操作，数据类型发生了从A到B的变化，// 返回结果是一个size一样的集合，但是元素类型不一样// [1,2,3,4] [f(1),f(2),f(3),f(4)]// [[1,2,3],[4,5,6],[7,8]] [f([1,2,3]),f([4,5,6]),f([7,8])]def map[B, That](f: A =&gt; B)(implicit bf: CanBuildFrom[Repr, B, That]): That = &#123; def builder = &#123; val b = bf(repr) b.sizeHint(this) b &#125; val b = builder for (x &lt;- this) b += f(x) b.result&#125; 123456789// 原理与map类似，但是增加了一个扁平化的处理：// 效果如下：[[1,2,3],[4,5,6],[7,8]] // [f([1,2,3])[0],f([1,2,3])[1],f([1,2,3])[2],f([4,5,6])[0],f([4,5,6])[1],f([4,5,6])[2],f([7,8])[0],f([7,8])[1],f([7,8])[2]]def flatMap[B, That](f: A =&gt; GenTraversableOnce[B])(implicit bf: CanBuildFrom[Repr, B, That]): That = &#123; def builder = bf(repr) val b = builder for (x &lt;- this) b ++= f(x).seq b.result&#125; 123456789// 过滤出来集合中元素满足p条件的元素，结果为一个新的集合private def filterImpl(p: A =&gt; Boolean, isFlipped: Boolean): Repr = &#123; val b = newBuilder for (x &lt;- this) if (p(x) != isFlipped) b += x b.result &#125;def filter(p: A =&gt; Boolean): Repr = filterImpl(p, isFlipped = false)","categories":[{"name":"Scala学习","slug":"Scala学习","permalink":"https://shang.at/categories/Scala学习/"}],"tags":[{"name":"函数式编程","slug":"函数式编程","permalink":"https://shang.at/tags/函数式编程/"}]},{"title":"Scala学习-编程Tips","slug":"Scala学习-编程Tips","date":"2020-07-27T17:53:33.000Z","updated":"2021-04-11T01:23:52.378Z","comments":true,"path":"post/Scala学习-编程Tips/","link":"","permalink":"https://shang.at/post/Scala学习-编程Tips/","excerpt":"简介：","text":"简介： 和python一样不需要分号，但是如果在一行有多条语句，则需要加分号。如 1val s = \"hello\"; println(s) scala中的Symbol 符号，参考java-String-intern 在scala中可以使用&#39;abc这样的形式创建一个Symbol对象，它是使用的Scala&#39;s built-in quote mechanism Symbol的优点： 节省内存：在Scala中，Symbol类型的对象是被拘禁的(interned)，任意的同名symbols都指向同一个Symbol对象，避免了因冗余而造成的内存开销。而对于String类型，只有编译时确定的字符串是被拘禁的(interned)。Scala测试代码如下： 12345val s = 'aSymbol//输出trueprintln( s == 'aSymbol)//输出trueprintln( s == Symbol(\"aSymbol\")) 快速比较：由于Symbol类型的对象是被拘禁的(interned)，任意的同名symbols都指向同一个Symbol对象，而不同名的symbols一定指向不同的Symbol对象，所以symbols对象之间可以使用操作符==快速地进行相等性比较，常数时间内便可以完成，而字符串的equals方法需要逐个字符比较两个字符串，执行时间取决于两个字符串的长度，速度很慢。(实际上，String.equals方法会先比较引用是否相同，但是在运行时产生的字符串对象，引用一般是不同的) 比如：Symbol类型一般用于快速比较，例如用于Map类型：Map,根据一个Symbol对象，可以快速查询相应的Data, 而Map的查询效率则低很多。 相较于String.intern()的优点：利用String的intern方法也可以实现Map的键值快速比较，但是由于需要显式地调用intern()方法，在编码时会造成很多的麻烦，而且如果忘了调用intern()方法，还会造成难以寻找的bug。从这个角度看，Scala的Symbol类型不仅有效率上的提升，而且也简化了编码的复杂度。 scala中的self =&gt;写法 其他的一些情况参考 看scala的源码的话很发现很多源码开头都有一句：self =&gt; 。这句相当于给this起了一个别名为self 12345678class SparkSession private( @transient val sparkContext: SparkContext, @transient private val existingSharedState: Option[SharedState], @transient private val parentSessionState: Option[SessionState], @transient private[sql] val extensions: SparkSessionExtensions) extends Serializable with Closeable with Logging &#123; self =&gt;// ... &#125; 作为this关键字的别名，如 1234567891011121314151617181920class Self &#123; self =&gt; //代表this指针 ，也就是说 self 就 this的别名 // anc =&gt; // 作用与self =&gt; 一样，只不过人们习惯于用self =&gt; 的写法 val tmp = \"scala\" def foo: String = self.tmp + this.tmp&#125;// 内部类class Outer &#123; out =&gt; val v1 = \"spark\" class Inner &#123; println(out.v1) // 用outer表示外部类，相当于Outer.this v1 &#125;&#125; 自身类型：混入trait，强制要求 S2的实现类 去实现 S1，如： 123456789101112131415trait S1/** * this 不能称当别名，这里this:S1 是一个整体，就是说实例化S2时，要求S1必须混入 */class S2 &#123; this: S1 =&gt;&#125; // 混入trait S1，强制要求 S2的实现类 去实现 S1class S3 extends S2 with S1trait T &#123; this: S1 =&gt;&#125;object S4 extends T with S1 this:S1 =&gt; 要求S2在实例化时或定义S2的子类时，必须混入指定的S1 类型，这个X类型也可以指定为当前类型 自身类型的存在相当于让当前类变得抽象了，它假设当前对象(this)也符合指定的类型，因为自身类型 this:S1 =&gt;的存在，当前类构造实例时需要同时满足S1类型 scala中的=&gt;，参考资料，例子来自于这里 In a value, it introduces a function literal, or lambda. e.g. the bit inside the curly braces in List(1,2,3).map { (x: Int) =&gt; x * 2 } 用在函数声明上：连接参数和函数体 In a type, with symbols on both sides of the arrow (e.g. A =&gt; T, (A,B) =&gt; T, (A,B,C) =&gt; T, etc.) it’s sugar for Function&lt;n&gt;[A[,B,...],T], that is, a function that takes parameters of type A[,B...], and returns a value of type T. Empty parens on the left hand side (e.g. () =&gt; T) indicate that the function takes no parameters (also sometimes called a “thunk”); Empty parens on the right hand side denote that it returns ()—the sole value of type Unit, whose name can also be written ()—confused yet? :) A function that returns Unit is also known as a procedure, normally a method that’s called only for its side effect. 用在类型声明上：表示类型的转换，一般用来表示函数的签名 In the type declaration for a method or function parameter, with no symbol on the left hand side (e.g. def f(param: =&gt; T)) it’s a “by-name parameter”, meaning that is evaluated every time it’s used within the body of the function, and not before. Ordinary “by-value” parameters are evaluated before entry into the function/method. 用在传名参数上：这种用法和其他三种都不一样，比较特殊，讲的是 用=&gt;修饰函数参数时，意味着只有该参数在函数体内第一次被使用时，才会去计算它(lazy initializing)(且每进入一次函数体就会被计算)，也被称为传名参数(by-name parameters)。与之对应的是传值参数(by-value parameters)，传值参数会被立刻计算一次，且只会计算一次。 如下面的例子 1234567891011121314151617181920212223object TestFuncParameter &#123; def whileLoop(condition: =&gt; Boolean)(body: =&gt; Unit): Unit = &#123; println(condition) if (condition) &#123; body whileLoop(condition)(body) &#125; &#125; def main(args: Array[String]): Unit = &#123; var i = 2 whileLoop(i &gt; 0) &#123; // body println(i) i -= 1 &#125; &#125;&#125;// prints // true// 2 // false// 1 正常情况下会打印上面写到的结果。如果把condition: =&gt; Boolean改成condition: Boolean，则会永远死循环下去，因为condition永远都是true，它只被计算了一次。 其中(body: =&gt; Unit)必须加=&gt;修饰，这里body指向的是{println(i);i-=1}，它是一个Unit类型的变量，可以理解成它指向了一个代码块，但是他不是一个函数，就是一个变量。再加上=&gt;关键字的修饰，body在whileLoop中第一次遇见body的时候，就会计算它一次(执行一次它所指向的代码块)，所以这里可以看成scala中的一种特殊的语法糖。 在spark中可以看到这种用法：rdd的几乎所有方法都有withScope这么一个调用，如 1234def map[U: ClassTag](f: T =&gt; U): RDD[U] = withScope &#123; val cleanF = sc.clean(f) new MapPartitionsRDD[U, T](this, (context, pid, iter) =&gt; iter.map(cleanF))&#125; withScope是这样定义的： body: =&gt; U 1private[spark] def withScope[U](body: =&gt; U): U = RDDOperationScope.withScope[U](sc)(body) 再来看一下org.apache.spark.rdd.RDDOperationScope#withScope的定义： body: =&gt; T 1234private[spark] def withScope[T]( sc: SparkContext, allowNesting: Boolean = false)(body: =&gt; T): T = &#123;&#125; 关于body: =&gt; Unit 做了如下的测试： 这的Unit是根据body的具体类型可以推断出来，body指向了一个代码块，这个代码块如果有返回值，那么body就是那个返回的类型，如果没有返回值，那么body就是Unit类型。上面的写法就是没有返回值，下面的写法返回了一个Int类型的值，且这个值可以在函数体内自由使用，=&gt; 则使在函数内第一次调用的时候被计算 12345678910111213141516171819202122232425262728object TestFuncParameter &#123; def whileLoop(condition: =&gt; Boolean)(body: =&gt; Int): Unit = &#123; println(condition) if (condition) &#123; val tmp: Int = body println(\"tmp:\" + tmp) whileLoop(condition)(body) &#125; &#125; def main(args: Array[String]): Unit = &#123; var i: Int = 2 whileLoop(i &gt; 0) &#123; println(i) i -= 1 i &#125; // prints 2 1 &#125;&#125;// true// 2// tmp:1// true// 1// tmp:0// false 传名参数的优点： 如果参数是计算密集型或长时间运行的代码块，如获取 URL资源，这种延迟计算参数直到它被使用时才计算的能力可以帮助提高性能。 In a case clause, they separate the pattern (and optional guard) from the result expression, e.g. case x =&gt; y. 用在模式匹配上：可以理解成和函数一样的作用，用来连接模式和具体的逻辑块 除此之外，=&gt;还有一个使用场景：用在类的声明中，用作this别名或者自身类型(self-type)，详情请参考上一条的总结","categories":[{"name":"Scala学习","slug":"Scala学习","permalink":"https://shang.at/categories/Scala学习/"}],"tags":[{"name":"Scala编程Tips","slug":"Scala编程Tips","permalink":"https://shang.at/tags/Scala编程Tips/"}]},{"title":"Scala学习-9-Iterator设计模式","slug":"Scala学习-9-Iterator设计模式","date":"2020-07-26T08:26:23.000Z","updated":"2021-05-13T01:55:05.640Z","comments":true,"path":"post/Scala学习-9-Iterator设计模式/","link":"","permalink":"https://shang.at/post/Scala学习-9-Iterator设计模式/","excerpt":"简介：Iterator的数据处理模式贯穿了大数据处理流程的始终，是一个非常值得借鉴的设计模式","text":"简介：Iterator的数据处理模式贯穿了大数据处理流程的始终，是一个非常值得借鉴的设计模式 现象使用scala编写了一段wordcount的代码如下： 123456789101112131415object TestIterator &#123; def main(args: Array[String]): Unit = &#123; val listStr = List( \"hello world\", \"hello msb\", \"good idea\" ) val flatMap: List[String] = listStr.flatMap((x: String) =&gt; x.split(\" \")) flatMap.foreach(println) val mapList: List[(String, Int)] = flatMap.map((_, 1)) mapList.foreach(println) &#125;&#125; 分析一下上面程序的执行过程： 创建listStr对象 listStr调用flatMap，作用是将listStr的每个元素使用空格分隔，然后合并成一个大的list 继续调用map，将每个元素转换成一个[String,Int]的二元组 打印结果 以上代码有一个致命的问题：在数据量非常的的时候，会急剧的消耗内存空间。为什么？简单分析下：第一步空间复杂度为O(N)，第二步又生成了一个全新的List[String]对象，又是O(N)的空间复杂度，第三步中又生成了一个List[(String,Int)]对象，空间复杂度依然是O(N)。通过分析可知，在数据统计的过程中，貌似并没有必要将中间过程的数据存储下来，不但占用空间，还没有任何用处。 于是，我们想到了一种设计模式-迭代器模式。迭代器模式在内部维护了一个指针，实际上并不会存储数据，在遍历数据集的时候，不断的消耗当前指针。 现在开始复习一下迭代器模式迭代器一定有两个方法： 12345public interface Iterator&lt;E&gt; &#123; //Element E //Type T //Key K //Value V boolean hasNext(); // 是否还有下一个元素 E next(); // 获取下一个元素&#125; 集合类一定有一个返回迭代器的函数 123456public interface Collection&lt;E&gt; &#123; void add(E o); int size(); Iterator iterator();&#125; 具体的集合类实现Collection接口 1234567891011121314151617181920212223242526272829303132333435363738394041class ArrayList&lt;E&gt; implements Collection&lt;E&gt; &#123; E[] objects = (E[]) new Object[10]; private int index = 0; public void add(E o) &#123; if (index == objects.length) &#123; E[] newObjects = (E[]) new Object[objects.length * 2]; System.arraycopy(objects, 0, newObjects, 0, objects.length); objects = newObjects; &#125; objects[index] = o; index++; &#125; public int size() &#123; return index; &#125; @Override public Iterator&lt;E&gt; iterator() &#123; return new ArrayListIterator(); &#125; private class ArrayListIterator&lt;E&gt; implements Iterator&lt;E&gt; &#123; private int currentIndex = 0; @Override public boolean hasNext() &#123; if (currentIndex &gt;= index) return false; return true; &#125; @Override public E next() &#123; E o = (E) objects[currentIndex]; currentIndex++; return o; &#125; &#125;&#125; 可以发现，迭代器中不会存数据，只是保存了一个指针，指向当前遍历到了哪一个索引，只有真正开始遍历的时候，指针才会开始移动，并且没有回退的方法，即迭代器只能遍历一次(另有设计的除外)。 解决方案使用迭代器实现上面的功能 123456789101112131415161718192021object TestIterator &#123; def main(args: Array[String]): Unit = &#123; val listStr = List( \"hello world\", \"hello msb\", \"good idea\" ) val iter: Iterator[String] = listStr.iterator //什么是迭代器，为什么会有迭代器模式？ 迭代器里不存数据！ val iterFlatMap = iter.flatMap((x: String) =&gt; x.split(\" \")) // iterFlatMap.foreach(println) // 中途不能打印，否则后续就读取不到数据了 val iterMapList = iterFlatMap.map((_, 1)) while (iterMapList.hasNext) &#123; val tuple: (String, Int) = iterMapList.next() println(tuple) &#125; //1.listStr真正的数据集，有数据的 //2.iter.flatMap 没有发生计算，返回了一个新的迭代器 &#125;&#125; 分析：基于迭代器的方案中，除了listStr中存储了数据，中间的过程中只有计算逻辑没有存储数据(faltMap会有一点少少的数据缓冲存储)。这和spark中的算子的思想一样啊，也可以说spark是借鉴了迭代器的编程模式。 Spark的transformation算子：类比这里的flatMap/map Spark的action算子：类比这里的foreach 关于scala中flatMap、map、foreach的过程分析 iter = listStr.iterator 12345678def iterator: Iterator[A] = new AbstractIterator[A] &#123; var these = self def hasNext: Boolean = !these.isEmpty def next(): A = if (hasNext) &#123; val result = these.head; these = these.tail; result &#125; else Iterator.empty.next()&#125; iterator返回的是一个AbstractIterator对象，重写了hasNext和next函数： hasNext：调用listStr的isEmpty方法，如果为空则返回false next： 先检测是否有元素，有的话返回listStr的头结点，并且移动these到剩余部分的头部； 否则返回空。按理说不会为空，因为都是先判断了hasNext为true才会调用next iterFlatMap = iter.flatMap((x:String) =&gt; x.split(“ “)) 1234567891011121314def flatMap[B](f: A =&gt; GenTraversableOnce[B]): Iterator[B] = new AbstractIterator[B] &#123; private var cur: Iterator[B] = empty private def nextCur() &#123; cur = f(self.next()).toIterator &#125; def hasNext: Boolean = &#123; // Equivalent to cur.hasNext || self.hasNext &amp;&amp; &#123; nextCur(); hasNext &#125; // but slightly shorter bytecode (better JVM inlining!) while (!cur.hasNext) &#123; if (!self.hasNext) return false nextCur() &#125; true &#125; def next(): B = (if (hasNext) cur else empty).next()&#125; flatMap返回的也是一个AbstractIterator，也重写了hasNext和next函数： 它在这里维护了一个cur的小迭代器，之所以说小，是因为它会缓存上一个调用节点的一条记录经过f处理之后的结果 hasNext：重点看这个函数 这里会先判断cur是否有元素，有的话直接返回true； 否则的话，调用父类的hasNext，判断是否还有值，没有的话，返回false； 否则的话调用父类的next获取一条新的记录，并交给处理函数f处理，处理完之后交给cur缓存起来 next：如果有值，则直接从cur中取值，而且永远都只从cur中取值 iterMapList = iterFlatMap.map((_, 1)) 1234def map[B](f: A =&gt; B): Iterator[B] = new AbstractIterator[B] &#123; def hasNext = self.hasNext def next() = f(self.next())&#125; map返回的也是一个AbstractIterator，也重写了hasNext和next函数，不过他这里的逻辑比较简单了，因为map只是完成了一个映射的过程 iterMapList.foreach(println) 打印收工。","categories":[{"name":"Scala学习","slug":"Scala学习","permalink":"https://shang.at/categories/Scala学习/"}],"tags":[{"name":"Scala-Iterator设计模式","slug":"Scala-Iterator设计模式","permalink":"https://shang.at/tags/Scala-Iterator设计模式/"}]},{"title":"Scala学习-隐式转换","slug":"Scala学习-8-隐式转换","date":"2020-07-26T08:21:03.000Z","updated":"2021-04-10T23:21:32.090Z","comments":true,"path":"post/Scala学习-8-隐式转换/","link":"","permalink":"https://shang.at/post/Scala学习-8-隐式转换/","excerpt":"简介：一个从类型 S 到类型 T 的隐式转换由一个函数类型 S =&gt; T 的隐式值来定义，或者由一个可转换成所需值的隐式方法来定义。 隐式转换是scala中一个非常重要的特性，但同时也是挺难理解的一个特性。 隐式转换使用不当将会导致很难检查的bug","text":"简介：一个从类型 S 到类型 T 的隐式转换由一个函数类型 S =&gt; T 的隐式值来定义，或者由一个可转换成所需值的隐式方法来定义。 隐式转换是scala中一个非常重要的特性，但同时也是挺难理解的一个特性。 隐式转换使用不当将会导致很难检查的bug 现象例如在scala项目使用创建了一个LinkedList对象，我们想要对该对象调用foreach方法，但是却发现调用的时候会报错，因为它没有该方法 12345678import java.utilobject TestImplicit &#123; def main(args: Array[String]): Unit = &#123; val list = new util.LinkedList[Int]() list.foreach(println) // 这里会报：Cannot resolve symbol foreach &#125;&#125; 那么我们应该怎么办呢？实际上我们可以有以下几种解决方案 不需要修改LinkedList的源代码(也修改不了) 解决方案分析一下list.foreach(println)，发现这一句中包含了三个内容：数据集list、遍历行为foreach、处理函数println，那么我们只需要通过某种方案将这三个元素组装好就可以了 方案一定义一个foreach函数帮助完成遍历 123456789101112131415161718object TestImplicit &#123; def main(args: Array[String]): Unit = &#123; val list = new util.LinkedList[Int]() list.add(3) list.add(4) list.add(5) // list.foreach(println) def foreach[T](list: util.LinkedList[T], f: (T) =&gt; Unit): Unit = &#123; val iter: util.Iterator[T] = list.iterator() while (iter.hasNext) f(iter.next()) &#125; foreach(list, println) &#125;&#125; 方案二通过方案一看似可以满足我们的需求，但是我们不想通过把list当做foreach的参数来完成遍历，我们就想通过调用obj.foreach的形式来完成这个操作。那么我们可以通过定义一个class来包装下list对象，即在新定义的class中提供foreach的方法调用，然后有一个list的属性，那么就可以这样做 123456789101112131415161718class XXX[T](list: util.LinkedList[T]) &#123; def foreach(f: (T) =&gt; Unit): Unit = &#123; val iter: util.Iterator[T] = list.iterator() while (iter.hasNext) f(iter.next()) &#125;&#125;object TestImplicit &#123; def main(args: Array[String]): Unit = &#123; val list = new util.LinkedList[Int]() list.add(3) list.add(4) list.add(5) val xx = new XXX(list) xx.foreach(println) &#125;&#125; 方案三方案二中通过一个新的class做到了类似obj.foreach的调用形式，但是我们还不满足，我们不想用其他的对象来调用foreach，就是想直接list.foreach，用list对象本身直接调用，那该怎么办呢？首先在原始的LinkedList类中是没有foreach方法的，我们又不能去修改LinkedList的源码。实际上在scala中为我们提供了一种更加方便快捷的方案，可以在不侵入源码的情况下，给一个对象赋予新的方法。那就是隐式转换(implicit)，实际上scala中的隐式转换在某种程度上与我们上面的两种方案有异曲同工之妙，但是它在编译器层面做了优化，是语言层面的支持，即我们可以通过implicit关键字直接告诉编译器：我这个对象不存在当前我要调用的方法，但是我有在其他地方定义了一个方法转换，可以让我拥有这个方法。 1234567891011121314151617181920212223object TestImplicit &#123; def main(args: Array[String]): Unit = &#123; val list = new util.LinkedList[Int]() list.add(3) list.add(4) list.add(5) //隐式转换： 隐式转换方法:接受一个类型，返回另外一个包含特定方法的类型 implicit def sdfsdf[T](list: util.LinkedList[T]) = &#123; new XXX(list) &#125; list.foreach(println) &#125;&#125;class XXX[T](list: util.LinkedList[T]) &#123; def foreach(f: (T) =&gt; Unit): Unit = &#123; val iter: util.Iterator[T] = list.iterator() while (iter.hasNext) f(iter.next()) &#125;&#125; 你可以发现我们使用了一个新的关键字implicit，它修饰了一个函数，并且我们新创建的这个函数，它的名字很特殊，只是一个乱写的字符串，也许你可以猜出来了，这个函数的名字对我们来说没有用处，对的，在scala中的隐式转换只关心类型，并不关心名称。但是在实际应用中一定不要如此随便的命名，这里只是做演示。 implicit关键字如注释中说的那样：接受一个类型，返回另一个包含特定方法的类型，但是我们不需要使用新的类型对象来调用foreach，这是因为scala在编译的阶段帮我们做了改写，其执行过程大致如下： scala编译器发现list对象并没有foreach方法 但是并不会立马报错 它回去寻找有没有implicit定义的方法，且方法的参数正好是list对象对应的类型 如果找到了，那么就可以编译通过；否则就会报错 总而言之，scala编译器帮助我们完成了list类型的转换，也可以理解成编译器帮我们把代码改写了，但是这一步骤是我们所看不到，所以说是隐式的(个人这么理解) 方案三介绍的只是scala的一种隐式转换：隐式转换方法 除此之外，scala中还有其他隐式转换形式：隐式转换类、隐式转换参数 隐式转换类12345678910111213141516171819object TestImplicit &#123; //隐式转换类 implicit class KKK[T](list: util.LinkedList[T]) &#123; def foreach(f: (T) =&gt; Unit): Unit = &#123; val iter: util.Iterator[T] = list.iterator() while (iter.hasNext) f(iter.next()) &#125; &#125; def main(args: Array[String]): Unit = &#123; val list = new util.LinkedList[Int]() list.add(3) list.add(4) list.add(5) list.foreach(println) &#125;&#125; 注：这里需要注意的是：implicit只能用在类内部或者函数内部，不能使用在top-level的objects上。否则会报如下的错误&#39;implicit&#39; modifier cannot be used for top-level objects 隐式转换参数最后这一种隐式转换的形式实际上对这个例子没有作用，他是在另外一些使用场景有使用 方法可以具有 隐式 参数列表，由参数列表开头的 implicit 关键字标记。 如果参数列表中的参数没有像往常一样传递， Scala 将查看它是否可以获得正确类型的隐式值，如果可以，则自动传递。 implicit如果出现在函数的参数列表中，如def func(implicit name:String)，那么表示在调用func函数的时候，name参数可以不传，也可以传。传与不传的区别如下： 传：那么直接使用传递的参数，和普通的函数没有任何区别 不传：这种情况下，必须存在一个隐式的String类型的变量被声明，否则会编译报错 使用方式如下： 12345678910111213object TestImplicit &#123; def main(args: Array[String]): Unit = &#123; implicit val str: String = \"lisi\" def func(implicit name: String): Unit = &#123; println(name) &#125; func // 打印 lisi func(\"zhangsan\") // 打印 zhangsan &#125;&#125; 隐式转换参数使用的时候需要注意以下几点 如果参数列表中有implicit，那么所有的参数要么都传要么都定义隐式转换变量 123def func(implicit name: String, age: Int): Unit = &#123; println(name)&#125; 同一种类型的隐私转换变量只能定义一个，否则会报错，例如下面定义了两个字符串类型的隐式变量 12345678implicit val a:String = \"wangwu\"implicit val b:String = \"lisi\"// 生命期间不会报错，但是代码执行期间，系统寻找隐式转换的时候会报错，如下&lt;console&gt;:15: error: ambiguous implicit values: both value a of type =&gt; String and value b of type =&gt; String match expected type String 如果有些参数有隐式转换变量定义，但是其他的参数不需要定义，该如何解决呢？ 解决方案：使用scala的函数柯里化(CURRYING)特性 123456implicit val a:String = \"123\"def func(age: Int)(implicit name: String): Unit = &#123; println(name + \" \" + age)&#125;func(66)(\"jkljkl\") // jkljkl 66func(66) // 123 66 隐式转换的作用域StackOverFlow 隐式转换本身是一种代码查找机制，所以下面会介绍隐式转换的查找范围： -当前代码作用域。最直接的就是隐式定义和当前代码处在同一作用域中。 -当第一种解析方式没有找到合适的隐式转换时，编译器会继续在隐式参数类型的隐式作用域里查找。一个类型的隐式作用域指的是与该类型相关联的所有的伴生对象。 对于一个类型T它的隐式搜索区域包括如下： -假如T是这样定义的：T extends A with B with C，那么A, B, C的伴生对象都是T的搜索区域。 -如果T是类型参数，那么参数类型和基础类型都是T的搜索部分。比如对于类型List[Foo]，List和Foo都是搜索区域 -如果T是一个单例类型p.T，那么p和T都是搜索区域。 -如果T是类型注入p#T，那么p和T都是搜索区域。 所以，只要在上述的任何一个区域中搜索到合适的隐式转换，编译器都可以使编译通过。 scala中预定义的隐式转换scala.Predef：scala的预定义对象，预定义了很多类型 (例如 Pair) 和方法 (例如 assert)，同时也声明了一些隐式转换。","categories":[{"name":"Scala学习","slug":"Scala学习","permalink":"https://shang.at/categories/Scala学习/"}],"tags":[{"name":"Scala隐式转换","slug":"Scala隐式转换","permalink":"https://shang.at/tags/Scala隐式转换/"}]},{"title":"Scala学习-偏函数","slug":"Scala学习-7-偏函数","date":"2020-07-26T08:20:55.000Z","updated":"2020-11-27T02:22:59.475Z","comments":true,"path":"post/Scala学习-7-偏函数/","link":"","permalink":"https://shang.at/post/Scala学习-7-偏函数/","excerpt":"简介：","text":"简介： 123456789101112131415object TestPartialFunction &#123; def main(args: Array[String]): Unit = &#123; def xxx: PartialFunction[Any, String] = &#123; case \"hello\" =&gt; \"val is hello\" case x: Int =&gt; s\"$x...is int\" case _ =&gt; \"none\" &#125; val str: String = xxx(44) println(str) println(xxx(\"hello\")) println(xxx(\"hi\")) &#125;&#125;","categories":[{"name":"Scala学习","slug":"Scala学习","permalink":"https://shang.at/categories/Scala学习/"}],"tags":[{"name":"Scala偏函数","slug":"Scala偏函数","permalink":"https://shang.at/tags/Scala偏函数/"}]},{"title":"Scala学习-模式匹配","slug":"Scala学习-6-模式匹配","date":"2020-07-26T08:20:13.000Z","updated":"2021-05-19T02:16:09.986Z","comments":true,"path":"post/Scala学习-6-模式匹配/","link":"","permalink":"https://shang.at/post/Scala学习-6-模式匹配/","excerpt":"简介：scala的模式匹配功能非常强大，可以支持值匹配，类型匹配等","text":"简介：scala的模式匹配功能非常强大，可以支持值匹配，类型匹配等 12345678910111213141516171819202122object Lesson06_match &#123; def main(args: Array[String]): Unit = &#123; val tup: (Double, Int, String, Boolean, Int, Option[Int]) = (1.0, 88, \"abc\", false, 44, Option(1)) val iter: Iterator[Any] = tup.productIterator val res: Iterator[Unit] = iter.map( (x) =&gt; &#123; x match &#123; case 1 =&gt; println(s\"$x...is 1\") case 88 =&gt; println(s\"$x ...is 88\") case false =&gt; println(s\"$x...is false\") case w: Int if w &gt; 50 =&gt; println(s\"$w...is &gt; 50\") case Some(n) =&gt; println(s\"$x... is Option(1)\") case _ =&gt; println(\"wo ye bu zhi dao sha lei xing \") &#125; &#125; ) while (res.hasNext) println(res.next()) &#125;&#125; Option类型中的模式匹配 123456789101112131415161718192021222324/** * Option类型中的模式匹配 * Option类型有两个子类，分别是Some和None（单例对象） * None是一个case object，它同Some一样都extends Option类，只不过Some是case class， * 对于case class我们已经很熟悉了，那case object又是怎么样的呢？ * 对比反编译之后的代码可以看出，case object与case class所不同的是： * case object对应反编译后的CaseObject$.class中不存在apply、unapply方法， * 这是因为None不需要创建对象及进行内容提取，从这个角度讲，它被定义为case object是十分合理的。 */object OptionDemo &#123; def main(args: Array[String]): Unit = &#123; val map = Map(\"hadoop\" -&gt; 1, \"spark\" -&gt; 2, \"flink\" -&gt; 3) //scala语言可以在一个函数中定义另外一个函数 def mapPattern(key: String): Unit = &#123; //get(key)方法中，通过key值获取与之相对应的value值 map.get(key) match &#123; case Some(x)=&gt;println(x);x case None=&gt;println(\"None\");-1 &#125; &#125; println(mapPattern(\"hive\")) println(mapPattern(\"spark\")) &#125;&#125; https://stackoverflow.com/questions/7519140/pattern-matching-on-class-type https://stackoverflow.com/questions/7157143/how-can-i-match-classes-in-a-scala-match-statement","categories":[{"name":"Scala学习","slug":"Scala学习","permalink":"https://shang.at/categories/Scala学习/"}],"tags":[{"name":"Scala模式匹配","slug":"Scala模式匹配","permalink":"https://shang.at/tags/Scala模式匹配/"}]},{"title":"Scala学习-案例类","slug":"Scala学习-5-案例类","date":"2020-07-26T08:19:58.000Z","updated":"2021-04-12T04:37:57.808Z","comments":true,"path":"post/Scala学习-5-案例类/","link":"","permalink":"https://shang.at/post/Scala学习-5-案例类/","excerpt":"简介：","text":"简介： 123456789101112131415case class Dog(name:String,age:Int)&#123;&#125;object TestCaseClass &#123; def main(args: Array[String]): Unit = &#123; val dog1 = Dog(\"hsq\",18) val dog2 = Dog(\"hsq\",18) println(dog1.equals(dog2)) println(dog1 == dog2) val dog3: Dog = dog2.copy(age=10) println(dog3) &#125;&#125; case class感觉上类似于tuple，默认情况下：属性都是public的val的(不可变)，但是case class可以将val改成var 12345678scala&gt; christina.nameres0: String = Christina// can't mutate the `name` fieldscala&gt; christina.name = \"Fred\"&lt;console&gt;:10: error: reassignment to val christina.name = \"Fred\" ^ case class构造的时候不需要使用new关键字也可以，因为case class有一个默认的apply方法来负责对象的创建。 12case class Person(name: String, relation: String)val christina = Person(\"Christina\", \"niece\") case class 自带unapply方法，可以直接用于模式匹配中 123456789101112trait Person &#123; def name: String&#125;case class Student(name: String, year: Int) extends Personcase class Teacher(name: String, specialty: String) extends Persondef getPrintableString(p: Person): String = p match &#123; case Student(name, year) =&gt; s\"$name is a student in Year $year.\" case Teacher(name, whatTheyTeach) =&gt; s\"$name teaches $whatTheyTeach.\"&#125; case class在比较的时候是按值比较而非按引用比较：在消息传递的时候非常有用，不关心引用，只关心值 1234567891011scala&gt; case class Person(name: String, relation: String)defined class Personscala&gt; val christina = Person(\"Christina\", \"niece\")christina: Person = Person(Christina,niece)scala&gt; val hannah = Person(\"Hannah\", \"niece\")hannah: Person = Person(Hannah,niece)scala&gt; christina == hannahres1: Boolean = false 可以通过copy方法创建一个案例类实例的浅拷贝，同时可以指定构造参数来做一些改变。 12345678scala&gt; case class BaseballTeam(name: String, lastWorldSeriesWin: Int)defined class BaseballTeamscala&gt; val cubs1908 = BaseballTeam(\"Chicago Cubs\", 1908)cubs1908: BaseballTeam = BaseballTeam(Chicago Cubs,1908)scala&gt; val cubs2016 = cubs1908.copy(lastWorldSeriesWin = 2016)cubs2016: BaseballTeam = BaseballTeam(Chicago Cubs,2016) toString方法 12scala&gt; christinares0: Person = Person(Christina,niece) The biggest advantage While all of these features are great benefits to functional programming, as they write in the book, Programming in Scala (Odersky, Spoon, and Venners), “the biggest advantage of case classes is that they support pattern matching.” Pattern matching is a major feature of FP languages, and Scala’s case classes provide a simple way to implement pattern matching in match expressions and other areas","categories":[{"name":"Scala学习","slug":"Scala学习","permalink":"https://shang.at/categories/Scala学习/"}],"tags":[{"name":"Scala案例类","slug":"Scala案例类","permalink":"https://shang.at/tags/Scala案例类/"}]},{"title":"Scala学习-特质和抽象类型","slug":"Scala学习-4-特质和抽象类型","date":"2020-07-26T08:19:37.000Z","updated":"2021-04-11T03:55:08.507Z","comments":true,"path":"post/Scala学习-4-特质和抽象类型/","link":"","permalink":"https://shang.at/post/Scala学习-4-特质和抽象类型/","excerpt":"简介：特质 (Traits) 用于在类 (Class)之间共享程序接口 (Interface)和字段 (Fields)。 它们类似于Java 8的接口。 类和对象 (Objects)可以扩展特质，但是特质不能被实例化，因此特质没有参数。","text":"简介：特质 (Traits) 用于在类 (Class)之间共享程序接口 (Interface)和字段 (Fields)。 它们类似于Java 8的接口。 类和对象 (Objects)可以扩展特质，但是特质不能被实例化，因此特质没有参数。 12345678910111213141516171819202122232425262728293031323334353637trait God&#123; def say(): Unit =&#123; println(\"god...say\") &#125;&#125;trait Mg&#123; def ku(): Unit =&#123; println(\"mg...say\") &#125; def haiRen():Unit&#125;class Person(name:String) extends God with Mg&#123; def hello(): Unit =&#123; println(s\"$name say hello\") &#125; override def haiRen(): Unit = &#123; println(\"ziji shixian ....\") &#125;&#125;object Lesson04_trait &#123; def main(args: Array[String]): Unit = &#123; val p = new Person(\"zhangsan\") p.hello() p.say() p.ku() p.haiRen() &#125;&#125; trait就类似于java8中的接口，特性和接口的性质相似 一次继承多次实现：只能继承一个父类，但是可以实现多个trait，使用关键字extends(和java中不同) trait可以有默认实现","categories":[{"name":"Scala学习","slug":"Scala学习","permalink":"https://shang.at/categories/Scala学习/"}],"tags":[{"name":"Scala特质和抽象类型","slug":"Scala特质和抽象类型","permalink":"https://shang.at/tags/Scala特质和抽象类型/"}]},{"title":"Scala学习-集合","slug":"Scala学习-3-集合","date":"2020-07-26T08:19:27.000Z","updated":"2021-04-11T03:54:16.716Z","comments":true,"path":"post/Scala学习-3-集合/","link":"","permalink":"https://shang.at/post/Scala学习-3-集合/","excerpt":"简介：","text":"简介： 在scala中分为两种集合类型：immutable(不可变的)、mutable(可变的) 默认创建出来的集合类如： immutable的API都是创建一个新的对象返回 mutable的API具有add set put等操作，可以修改对象内容 scala中的队列和栈： 1.使用队列队列是一种那个先进先出的队列。1)创建一个队列。 123456789101112scala&gt; import scala.collection.mutable.Queuescala&gt; var fruits = Queue[String]()fruits: scala.collection.mutable.Queue[String] = Queue() scala&gt; fruits += \"apple\"res66: scala.collection.mutable.Queue[String] = Queue(apple) scala&gt; fruits += (\"orange\",\"banana\")res67: scala.collection.mutable.Queue[String] = Queue(apple, orange, banana) scala&gt; fruits ++= List(\"cherry\",\"cocount\")res68: scala.collection.mutable.Queue[String] = Queue(apple, orange, banana, cherry, cocount) scala&gt; fruits.enqueue(\"pine\") scala&gt; fruitsres70: scala.collection.mutable.Queue[String] = Queue(apple, orange, banana, cherry, cocount, pine) 2)dequeue每次从队列头部删除一个元素。 1234scala&gt; val next = fruits.dequeuenext: String = apple scala&gt; fruitsres72: scala.collection.mutable.Queue[String] = Queue(orange, banana, cherry, cocount, pine) 3)dequeueFirst和dequeueAll方法可以从队列中删除多个元素。 12345678910111213scala&gt; var fruits = Queue[String]()fruits: scala.collection.mutable.Queue[String] = Queue() scala&gt; fruits ++= List(\"cherry\",\"cocount\")res76: scala.collection.mutable.Queue[String] = Queue(cherry, cocount) scala&gt; fruits ++= List(\"orange\",\"apple\")res77: scala.collection.mutable.Queue[String] = Queue(cherry, cocount, orange, apple) scala&gt; fruits.dequeueFirst(_.startsWith(\"a\"))res79: Option[String] = Some(apple) scala&gt; fruitsres80: scala.collection.mutable.Queue[String] = Queue(cherry, cocount, orange)scala&gt; fruits.dequeueAll(_.length &gt; 6)res81: scala.collection.mutable.Seq[String] = ArrayBuffer(cocount)scala&gt; fruitsres82: scala.collection.mutable.Queue[String] = Queue(cherry, orange) 2.使用栈栈是一种后进先出的数据结构。用push方法将元素入栈，用pop方法将元素出栈。1)创建一个任意数据类型空的可变栈。 123scala&gt; import scala.collection.mutable.Stackscala&gt; var ints = Stack[Int]()ints: scala.collection.mutable.Stack[Int] = Stack() 2)在创建时给栈初始元素。 12scala&gt; val ints = Stack(1,2,3)ints: scala.collection.mutable.Stack[Int] = Stack(1, 2, 3) 3)用push方法向栈中填充元素。 123456scala&gt; var fruits = Stack[String]()fruits: scala.collection.mutable.Stack[String] = Stack() scala&gt; fruits.push(\"apple\")res2: scala.collection.mutable.Stack[String] = Stack(apple)scala&gt; fruits.push(\"apple\",\"orange\",\"banana\")res3: scala.collection.mutable.Stack[String] = Stack(banana, orange, apple, apple) 4)用pop方法将元素出栈。 1234scala&gt; val next = fruits.popnext: String = bananascala&gt; fruitsres4: scala.collection.mutable.Stack[String] = Stack(orange, apple, apple) 5)用top方法查看下一个元素。 1234scala&gt; fruits.topres5: String = orangescala&gt; fruitsres6: scala.collection.mutable.Stack[String] = Stack(orange, apple, apple) 6)用clear清空一个可变栈。 123scala&gt; fruits.clearscala&gt; fruitsres8: scala.collection.mutable.Stack[String] = Stack() https://www.cnblogs.com/zhaohadoopone/p/9534996.html#)","categories":[{"name":"Scala学习","slug":"Scala学习","permalink":"https://shang.at/categories/Scala学习/"}],"tags":[{"name":"Scala集合","slug":"Scala集合","permalink":"https://shang.at/tags/Scala集合/"}]},{"title":"Scala学习-方法","slug":"Scala学习-2-方法","date":"2020-07-26T08:19:20.000Z","updated":"2020-11-26T02:24:10.038Z","comments":true,"path":"post/Scala学习-2-方法/","link":"","permalink":"https://shang.at/post/Scala学习-2-方法/","excerpt":"简介：","text":"简介： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203import java.utilimport java.util.Dateobject Lesson02_Functions &#123; //成员方法 def ooxx(): Unit = &#123; println(\"hello object\") &#125; def main(args: Array[String]): Unit = &#123; // 方法 函数 println(\"-------1.basic----------\") //返回值，参数，函数体 def fun01() &#123; println(\"hello world\") &#125; fun01() var x = 3 var y = fun01() println(y) //想有返回 // public void sdfsd()&#123;&#125; // public String sdfsdf()&#123;&#125; //有return必须给出返回类型 def fun02() = &#123; new util.LinkedList[String]() &#125; //参数：必须给出类型，是val //class 构造，是var，val def fun03(a: Int): Unit = &#123; println(a) &#125; fun03(33) println(\"-------2.递归函数----------\") //递归先写触底！ 触发什么报错呀 def fun04(num: Int): Int = &#123; if (num == 1) &#123; num &#125; else &#123; num * fun04(num - 1) &#125; &#125; val i: Int = fun04(4) println(i) println(\"-------3.默认值函数----------\") def fun05(a: Int = 8, b: String = \"abc\"): Unit = &#123; println(s\"$a\\t$b\") &#125; // fun05(9,\"def\") fun05(22) fun05(b = \"ooxx\") println(\"-------4.匿名函数----------\") //函数是第一类值 //函数： //1，签名 ：(Int,Int)=&gt;Int ： （参数类型列表）=&gt; 返回值类型 //2，匿名函数： (a:Int,b:Int) =&gt; &#123; a+b &#125; ：（参数实现列表）=&gt; 函数体 var xx: Int = 3 var yy: (Int, Int) =&gt; Int = (a: Int, b: Int) =&gt; &#123; a + b &#125; val w: Int = yy(3, 4) println(w) println(\"--------5.嵌套函数---------\") def fun06(a: String): Unit = &#123; def fun05(): Unit = &#123; println(a) &#125; fun05() &#125; fun06(\"hello\") println(\"--------6.偏应用函数---------\") def fun07(date: Date, tp: String, msg: String): Unit = &#123; println(s\"$date\\t$tp\\t$msg\") &#125; fun07(new Date(), \"info\", \"ok\") var info = fun07(_: Date, \"info\", _: String) var error = fun07(_: Date, \"error\", _: String) info(new Date, \"ok\") error(new Date, \"error...\") println(\"--------7.可变参数---------\") def fun08(a: Int*): Unit = &#123; for (e &lt;- a) &#123; println(e) &#125; // def foreach[U](f: A =&gt; U): Unit // a.foreach( (x:Int)=&gt;&#123;println(x)&#125; ) // a.foreach( println(_) ) a.foreach(println) &#125; fun08(2) fun08(1, 2, 3, 4, 5, 6) println(\"--------将数组中的元素当做变长参数-----------\") def max(values: Int*) = values.foldLeft(values(0)) &#123; Math.max &#125; val numbers = Array(2, 5, 3, 7, 1, 6) println(max(numbers:_*)) // 需要使用:_*符号组合 println(\"--------8.高阶函数---------\") //函数作为参数，函数作为返回值 //函数作为参数 def computer(a: Int, b: Int, f: (Int, Int) =&gt; Int): Unit = &#123; val res: Int = f(a, b) println(res) &#125; computer(3, 8, (x: Int, y: Int) =&gt; &#123; x + y &#125;) computer(3, 8, (x: Int, y: Int) =&gt; &#123; x * y &#125;) computer(3, 8, _ * _) //函数作为返回值： def factory(i: String): (Int, Int) =&gt; Int = &#123; def plus(x: Int, y: Int): Int = &#123; x + y &#125; if (i.equals(\"+\")) &#123; plus &#125; else &#123; (x: Int, y: Int) =&gt; &#123; x * y &#125; &#125; &#125; computer(3, 8, factory(\"-\")) println(\"--------9.柯里化---------\") def fun09(a: Int)(b: Int)(c: String): Unit = &#123; println(s\"$a\\t$b\\t$c\") &#125; fun09(3)(8)(\"sdfsdf\") def fun10(a: Int*)(b: String*): Unit = &#123; a.foreach(println) b.foreach(println) &#125; fun10(1, 2, 3)(\"sdfs\", \"sss\") println(\"--------*.方法---------\") //方法不想执行，赋值给一个引用 方法名+空格+下划线 val funa = ooxx println(funa) val func = ooxx _ func() //语法 -&gt; 编译器 -&gt; 字节码 &lt;- jvm规则 //编译器，衔接 人 机器 //java 中 +： 关键字 //scala中+： 方法/函数 //scala语法中，没有基本类型，所以你写一个数字 3 编辑器/语法，其实是把 3 看待成Int这个对象 // 3 + 2 // 3.+(2) // 3:Int &#125;&#125;","categories":[{"name":"Scala学习","slug":"Scala学习","permalink":"https://shang.at/categories/Scala学习/"}],"tags":[{"name":"Scala方法","slug":"Scala方法","permalink":"https://shang.at/tags/Scala方法/"}]},{"title":"Scala学习-for-yield","slug":"Scala学习-1-控制语句-for-yield","date":"2020-07-26T08:19:07.000Z","updated":"2021-04-18T23:38:15.852Z","comments":true,"path":"post/Scala学习-1-控制语句-for-yield/","link":"","permalink":"https://shang.at/post/Scala学习-1-控制语句-for-yield/","excerpt":"简介：","text":"简介： scala语言的for语法很灵活. 除了普通的直接对集合的循环, 以及循环中的判断和值返回. 非常灵活. for 可以通过yield(生产)返回值, 最终组成for循环的对象类型.for 循环中的 yield 会把当前的元素记下来，保存在集合中，循环结束后将返回该集合。如果被循环的是 Map，返回的就是Map，被循环的是 List，返回的就是List，以此类推。 守卫( guards) (for loop ‘if’ conditions) 可以在 for 循环结构中加上 ‘if’ 表达式, 和yield联合起来用. 普通对集合或迭代循环1234567891011scala&gt; for(i&lt;- 1 to 5) println(i)12345scala&gt; for(i&lt;- 1 until 5) println(i)1234 yield返回值1234567891011121314151617181920212223scala&gt; for (i &lt;- 1 to 5) yield ires0: scala.collection.immutable.IndexedSeq[Int] = Vector(1, 2, 3, 4, 5)scala&gt; val a= for (i &lt;- 1 to 5) yield ia: scala.collection.immutable.IndexedSeq[Int] = Vector(1, 2, 3, 4, 5)scala&gt; ares1: scala.collection.immutable.IndexedSeq[Int] = Vector(1, 2, 3, 4, 5)scala&gt; val a= for (i &lt;- 1 until 5) yield ia: scala.collection.immutable.IndexedSeq[Int] = Vector(1, 2, 3, 4)scala&gt; ares2: scala.collection.immutable.IndexedSeq[Int] = Vector(1, 2, 3, 4)scala&gt; val a= for (i &lt;- 1 until 5) yield i*2a: scala.collection.immutable.IndexedSeq[Int] = Vector(2, 4, 6, 8)scala&gt; val a = Array(1, 2, 3, 4, 5)a: Array[Int] = Array(1, 2, 3, 4, 5)scala&gt; for ( e &lt;- a) yield eres3: Array[Int] = Array(1, 2, 3, 4, 5) 循环过滤 if 判断, 并返回值123456789101112131415161718192021222324252627282930313233343536scala&gt; for ( e &lt;- a if e%2 == 0) yield eres4: Array[Int] = Array(2, 4)scala&gt; ares10: Array[Int] = Array(1, 2, 3, 4, 5)scala&gt; val b = 6 to 7b: scala.collection.immutable.Range.Inclusive = Range 6 to 7scala&gt; for &#123; | x &lt;-a | y &lt;-b | &#125; yield (x,y)res11: Array[(Int, Int)] = Array((1,6), (1,7), (2,6), (2,7), (3,6), (3,7), (4,6), (4,7), (5,6), (5,7))scala&gt; for &#123; | y &lt;- b | x &lt;- a | &#125; yield (x,y)res12: scala.collection.immutable.IndexedSeq[(Int, Int)] = Vector((1,6), (2,6), (3,6), (4,6), (5,6), (1,7), (2,7), (3,7), (4,7), (5,7))val m = Map[String,String](\"a\"-&gt;1, \"b\"-&gt;2)for &#123; item &lt;- m&#125; yield &#123; println(item) (item._1+\"4\", item._2+1)&#125;res4: scala.collection.immutable.Map[String,Int] = Map(a4 -&gt; 2, b4 -&gt; 3)for &#123; item &lt;- m&#125; yield &#123; println(item) item._2+1&#125;res5: scala.collection.immutable.Iterable[Int] = List(2, 3) for 复杂实例找出.txt后缀文件 12345678910scala&gt; def getTextFile(path:String) : Array[java.io.File] = | for &#123; | file &lt;- new File(path).listFiles | if file.isFile | if file.getName.endsWith(\".txt\") | &#125; yield filegetTextFile: (path: String)Array[java.io.File]scala&gt; getTextFile(\".\")res9: Array[java.io.File] = Array(./a.txt, ./test.txt) 摘要：本文详细介绍 Scala 的 For 推导式的内部原理 基础知识123456789101112131415161718192021222324252627282930// src/main/scala/progscala2/forcomps/RemoveBlanks.scalapackage progscala2.forcompsobject RemoveBlanks &#123; /** * Remove blank lines from the specified input file. */ def apply(path: String, compressWhiteSpace: Boolean = false): Seq[String] = for &#123; // 这是一个生成器 line &lt;- scala.io.Source.fromFile(path).getLines.toSeq // &lt;1&gt; if line.matches(\"\"\"^\\s*$\"\"\") == false // &lt;2&gt; line2 = if (compressWhiteSpace) line replaceAll (\"\\\\s+\", \" \") // &lt;3&gt; else line &#125; yield line2 // &lt;4&gt; /** * Remove blank lines from the specified input files and echo the remaining * lines to standard output, one after the other. * @param args list of file paths. Prefix each with an optional \"-\" to * compress remaining whitespace in the file. */ def main(args: Array[String]) = for &#123; path2 &lt;- args // &lt;5&gt; (compress, path) = if (path2 startsWith \"-\") (true, path2.substring(1)) else (false, path2) // &lt;6&gt; line &lt;- apply(path, compress) &#125; println(line) // &lt;7&gt;&#125; 由于 for 推导式无法返回 Iterator 对象， for 推导式的返回类型由初始的生成器所决定，因此我们必须将其转化成一个序列。 过滤空行，也就是说这句话是使用上一句的输出结果 是一个 guard， 通过某个条件对进过进行处理，并赋值给另外的临时变量 输出最后的结果 For 推导式原理For 推导式实际上是语法糖，是组合了 map withFilter flatMap 的基础操作，下面给出一个等价的表示方法： 案例1123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263case class Person(name:String, isMale:Boolean, children: Person*)val lara = Person(\"Lara\", isMale = false)val bob = Person(\"Bob\", isMale = true)val julie = Person(\"Julie\", false, lara, bob)val gulie = Person(\"Gulie\", false, julie)val persons = List(lara, bob, julie, gulie)var temp1 = persons filter (p =&gt; !p.isMale)var temp11 = temp1 flatMap (k =&gt; k.name)var temp12 = temp1 flatMap (k =&gt; k.children)var temp2 = temp1 flatMap (p =&gt; p.children map (c =&gt; (p.name, c.name)))println(temp1)println(temp11)println(temp12)println(temp2)val temp3 = persons withFilter (p =&gt; !p.isMale) flatMap (p =&gt; p.children map (c =&gt; (p.name, c.name)))println(temp3)val temp4 = for &#123; p &lt;- persons if !p.isMale c &lt;- p.children&#125; yield (p.name, c.name)println(temp4)val temp44 = for &#123; p &lt;- persons if !p.isMale n = p.name if n startsWith \"Gu\" c &lt;- p.children&#125; yield (p.name, c.name)val temp5 = for &#123; p &lt;- persons // a generator n = p.name // a definition if n startsWith \"To\" // a filter&#125; yield nprintln(temp5)val temp6 = for ( x &lt;- List(1, 2); y &lt;- List(\"one\", \"two\");) yield (x, y)println(temp6)val temp6Equivalence = List(1, 2) flatMap (p =&gt; &#123; List(\"one\", \"two\") map ( k =&gt; (p,k))&#125;)println(temp6Equivalence)val temp7 = for ( x &lt;- List(1, 2); y &lt;- List(\"one\", \"two\"); z &lt;- List(\"a\", \"b\")) yield (x, y, z)println(temp7)val temp7Equivalence = List(1, 2) flatMap (p =&gt; &#123; List(\"one\", \"two\") flatMap ( k =&gt; &#123; List(\"a\", \"b\") map (l =&gt; (p,k,l))&#125;)&#125;)println(temp7Equivalence) 案例2没有 yield12345678910val states = List(\"Alabama\", \"Alaska\", \"Virginia\", \"Wyoming\")for &#123;s &lt;- states&#125; println(s)// 结果值:// Alabama// Alaska// Virginia// Wyomingstates foreach println 含有 yield1234567val states = List(\"Alabama\", \"Alaska\", \"Virginia\", \"Wyoming\")for &#123;s &lt;- states&#125; yield s.toUpperCase// 结果值: List(ALABAMA, ALASKA, VIRGINIA, WYOMING)states map (_.toUpperCase) // 有点类似于 Java Lambda 的 ::Method 操作，如果在 `map` 操作中只执行了一个方法。// 结果值: List(ALABAMA, ALASKA, VIRGINIA, WYOMING) 多个生成器123456789101112val states = List(\"Alabama\", \"Alaska\", \"Virginia\", \"Wyoming\")val temp8 = for &#123; s &lt;- states c &lt;- s&#125; yield s\"$c-$&#123;c.toUpper&#125;\"// 结果值: List(\"A-A\", \"l-L\", \"a-A\", \"b-B\", ...)val temp8Equivalence = states flatMap (_.toSeq map (c =&gt; s\"$c-$&#123;c.toUpper&#125;\"))val temp8Equivalence2 = states flatMap (p =&gt; p map (c =&gt; s\"$c-$&#123;c.toUpper&#125;\"))// 结果值: List(\"A-A\", \"l-L\", \"a-A\", \"b-B\", ...)println(temp8)println(temp8Equivalence)println(temp8Equivalence2) 加入保护式1234567891011121314val temp9 = for &#123; s &lt;- states c &lt;- s if c.isLower&#125; yield s\"$c-$&#123;c.toUpper&#125;\"// 结果值: List(\"l-L\", \"a-A\", \"b-B\", ...)val temp9Equivalence = states flatMap (_.toSeq withFilter (_.isLower) map (c =&gt; s\"$c-$&#123;c.toUpper&#125;\"))val temp9Equivalence2 = states flatMap (p =&gt; p withFilter (k =&gt; k.isLower) map (c =&gt; s\"$c-$&#123;c.toUpper&#125;\"))val temp9Equivalence3 = states flatMap (p =&gt; p filter (k =&gt; k.isLower) map (c =&gt; s\"$c-$&#123;c.toUpper&#125;\"))println(temp9)println(temp9Equivalence)println(temp9Equivalence2)println(temp9Equivalence3)// 结果值: List(\"l-L\", \"a-A\", \"b-B\", ...) 加入定义12345678910111213val states = List(\"Alabama\", \"Alaska\", \"Virginia\", \"Wyoming\")for &#123; s &lt;- states c &lt;- s if c.isLower c2 = s\"$c-$&#123;c.toUpper&#125; \"&#125; yield c2// 结果值: List(\"l-L\", \"a-A\", \"b-B\", ...)states flatMap (_.toSeq withFilter (_.isLower) map &#123; c =&gt; val c2 = s\"$c-$&#123;c.toUpper&#125; \" c2&#125;)// 结果值: List(\"l-L\", \"a-A\", \"b-B\", ...) 总结内部机制结合 《Scala 程序设计第2版》 7.2章节和上面的例子，我们总结 for 推导式的内部机制如下： withFilter 效率比 filter 高， 因为它不会生成一次中间的临时的容器，而是和后续操作结合使用。而且 withFilter 会限制传递给后续组合器的元素类型域。 yield 实际是一次 map 操作，具体 map 的操作方法是根据 yield 的表达式，比如案例1中是yield 元组，而案例2中是 yield 一个 toUpperCase 操作。 多个生成器，除了最后一个，其他的生成器都会转化为 flatMap，最后一个生成器对应这 map 调用。 toSeq 可以用来简化针对 Seq 的 map 后面的操作，实际上就是 p =&gt; p do something 定义变量将会在 map 表达式中定义 （flatmap 也可以吗？） 深层理解在像 pat &lt;- expr 这样的生成器表达式中，pat 实际上是一个模式表达式（ pattern expression），例如： (x,y) &lt;- List((1,2),(3,4))。 Scala 会以类似的方式对值定义语句 pat2 = expr 进行处理，该语句也会被视为某一模式。 12// pat &lt;- exprpat &lt;- expr.withFilter &#123; case pat =&gt; true; case _ =&gt; false &#125; Scala 在转化 for 推导式时，要做的第一件事便是将 pat &lt;- expr 语句转化为上述语句，然后， Scala 将重复执行下列转化规则，直到所有的推导表达式都被替换掉。值得一提的是，某些转化会生成新的 for 推导式，而后续的迭代则会负责对这些推导式进行转化。 如果 for 推导式中包含了一个生成器和一个 yield 表达式，那么该表达式将被转化为下列语句： 12// for ( pat &lt;- expr1 ) yield expr2expr map &#123; case pat =&gt; expr2 &#125; 如果 for 循环中未使用 yield 语句，但执行的代码具有副作用，那么该语句将被转化为： 12// for ( pat &lt;- expr1 ) expr2expr foreach &#123; case pat =&gt; expr2 &#125; 包含多个生成器（同时包含 yield 表达式）的 for 推导式将被转化成下列语句： 12// for ( pat1 &lt;- expr1; pat2 &lt;- expr2; ... ) yield exprNexpr1 flatMap &#123; case pat1 =&gt; for (pat2 &lt;- expr2 ...) yield exprN &#125; 请留意，嵌套的生成器会被转化成嵌套的 for 推导式。这些嵌套的 for 推导式会在下一次执行转化规则时被转化成方法调用。上面示例中 (…) 代表了省略的表达式，这些表达式可能是其他的生成器，也可能是值定义或保护式（ guard）。 包含多个生成器的 for 循环将被翻译成下列语句： 12// for ( pat1 &lt;- expr1; pat2 &lt;- expr2; ... ) exprNexpr1 foreach &#123; case pat1 =&gt; for (pat2 &lt;- expr2 ...) yield exprN &#125; 我们之前所见的示例中包含保护式（ guard）表达式，该表达式被编写在单独的一行中。事实上， guard 以及上一行中的代码可以编写在一行中，例如： pat1 &lt;- expr1 if guard。 后面跟着保护式的生成器会被翻译成下列语句： 12// pat1 &lt;- expr1 if guardpat1 &lt;- expr1 withFilter ((arg1, arg2, ...) =&gt; guard) 此处，变量 argN 代表了传递给 withFilter 方法的参数。对于大多数的容器而言，传入的方法中只含有一个参数 生成器后尾随一个值定义如果生成器后面尾随着一个值定义，那么转化这个生成器的复杂度会令人惊奇。如下所示： 1234567// pat1 &lt;- expr1; pat2 = expr2(pat1, pat2) &lt;- for &#123; // ➊ x1 @ pat1 &lt;- expr1 // ➋ &#125; yield &#123; val x2 @ pat2 = expr2 // ➌ (x1, x2) // ➍&#125; ➊ for 推导式将返回包含两个模式的 pair 对象。➋ x1 @ pat1 语句会将整个表达式中 pat1 所匹配的值赋给变量 x1，该值可能包含另一个变量的某一部分。假如 pat1 是一个不可变变量名， x1 和 pat1 的赋值将会是冗余的。➌ 将 pat2 值赋给 x2。➍ 返回元组。 下面的 REPL 会话中包含了 x @ pat = expr 语句的对应示例： 1234scala&gt; val z @ (x, y) = (1 -&gt; 2)z: (Int, Int) = (1,2)x: Int = 1y: Int = 2 变量 z 的值为元组 (1,2)，而变量 x 和变量 y 则对应了元组中各个组成部分的值。 一个具体案例1234567891011121314151617val map = Map(\"one\" -&gt; 1, \"two\" -&gt; 2)val list1 = for &#123; (key, value) &lt;- map // 本行和下一行将会被翻译成什么语句呢？ i10 = value + 10&#125; yield (i10)// 执行结果值: list1: scala.collection.immutable.Iterable[Int] = List(11, 12)// 翻译后的语句:val list2 = for &#123; (i, i10) &lt;- for &#123; x1 @ (key, value) &lt;- map &#125; yield &#123; val x2 @ i10 = value + 10 (x1, x2) &#125;&#125; yield (i10)// 执行结果: list2: scala.collection.immutable.Iterable[Int] = List(11, 12) 书中给出的解释就到这一步，这实际上还是一个嵌套的 for 表达式，下面我们进一步把它转化为 map。 首先我们需要知道如何把 map 中的元素映射为元组，很简答，只需要使用 val (x,y) = p，其中 p 是 map 的元素，x和y就是元组值，后面就可以使用 x 和 y 进一步计算了。 我们这里分两步转化，首先转化内部的 for 推导式 123456789101112131415161718192021222324252627val list2Inter = for &#123; x1@(key, value) &lt;- map&#125; yield &#123; val x2@i10 = value + 10 (x1, x2)&#125;val z@(x, y) = 1 -&gt; 2println(\"z: \" + z)val list2InterEqual = map map &#123; p =&gt; &#123; val (x, y) = p (x, y) -&gt; (y + 10) &#125;&#125;val list2InterEqual2 = map map &#123; p =&gt; &#123; val (x, y) = p ((x, y), y + 10) &#125;&#125;println(list2Inter)println(list2InterEqual)println(list2InterEqual2) 我们可以看到，内部的推导式 yield 了一个元组，所以还是返回了一个 map，所以我们的 map 的返回值也应是一个map 或元组 （代码中有两个等价表达式）。 在内部的 for 推导式的基础上，我们进一步映射 123456789101112131415val list2Equal = map map &#123; p =&gt; &#123; val (x, y) = p (x, y) -&gt; (y + 10)&#125;&#125; map &#123; k =&gt; &#123; val (m, n) = k n&#125;&#125;val list2Equal2 = map map &#123; p =&gt; &#123; val (_, y) = p y + 10&#125;&#125; 第一个是纯粹使用上一步的的内部推导式的转换结果再次进行 map 得到的。外层的 for 推导式只是将元组的第二个值返回。所以我们第二个表达式简化了过程，直接在内部的 map 就只返回元组的第二个值。当然如果第二层 for 推导式更加复杂，就仍然需要使用第一个表达式来等价翻译，而且对于 scala 本身应该也是使用第一个表达式进行翻译。 For 的应用容器一般我们都是使用明显的容器，比如 List, Array 以及 Map，实际上只要容器支持 foreach，map，flat 和 withFilter 的操作都可以使用 for 推导式。比如 Option， Either， Try 等等。","categories":[{"name":"Scala学习","slug":"Scala学习","permalink":"https://shang.at/categories/Scala学习/"}],"tags":[{"name":"Scala-for-yield","slug":"Scala-for-yield","permalink":"https://shang.at/tags/Scala-for-yield/"}]},{"title":"Scala学习-类型","slug":"Scala学习-0-类型","date":"2020-07-26T08:18:55.000Z","updated":"2021-04-14T08:22:37.397Z","comments":true,"path":"post/Scala学习-0-类型/","link":"","permalink":"https://shang.at/post/Scala学习-0-类型/","excerpt":"简介：","text":"简介： W3CSchool Scala的教程 在Scala中，所有的值都有类型，包括数值和函数 Scala类型层次结构Any是所有类型的超类型，也称为顶级类 型。它定义了一些通用的方法如equals、hashCode和toString。Any有两个直接子类：AnyVal和AnyRef。 AnyVal代表值类型。有9个预定义的非空的值类型分别是：Double、Float、Long、Int、Short、Byte、Char、Unit和Boolean。Unit是不带任何意义的值类型，它仅有一个实例可以像这样声明：()。所有的函数必须有返回，所以说有时候Unit也是有用的返回类型。 AnyRef代表引用类型。所有非值类型都被定义为引用类型。在Scala中，每个用户自定义的类型都是AnyRef的子类型。如果Scala被应用在Java的运行环境中，AnyRef相当于java.lang.Object。 类型转换值类型可以按照下面的方向进行转换： scala中默认存在以下的值类型的隐式转换 Nothing和NullNothing是所有类型的子类型，也称为底部类型。没有一个值是Nothing类型的。它的用途之一是给出非正常终止的信号，如抛出异常、程序退出或者一个无限循环（可以理解为它是一个不对值进行定义的表达式的类型，或者是一个不能正常返回的方法）。 Null是所有引用类型的子类型（即AnyRef的任意子类型）。它有一个单例值由关键字null所定义。Null主要是使得Scala满足和其他JVM语言的互操作性，但是几乎不应该在Scala代码中使用。我们将在后面的章节中介绍null的替代方案。","categories":[{"name":"Scala学习","slug":"Scala学习","permalink":"https://shang.at/categories/Scala学习/"}],"tags":[{"name":"Scala类型","slug":"Scala类型","permalink":"https://shang.at/tags/Scala类型/"}]},{"title":"工具使用-编译工具-maven","slug":"工具使用-编译工具-maven","date":"2020-07-24T10:06:19.000Z","updated":"2021-04-12T05:34:55.736Z","comments":true,"path":"post/工具使用-编译工具-maven/","link":"","permalink":"https://shang.at/post/工具使用-编译工具-maven/","excerpt":"简介：","text":"简介： maven配置settings123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;settings&gt; &lt;!--需要改成自己的maven的本地仓库地址--&gt; &lt;localRepository&gt;~/.m2/repository&lt;/localRepository&gt; &lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt; &lt;/mirrors&gt; &lt;profiles&gt; &lt;profile&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;name&gt;local private nexus&lt;/name&gt; &lt;url&gt;http://maven.oschina.net/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;name&gt;local private nexus&lt;/name&gt; &lt;url&gt;http://maven.oschina.net/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt; &lt;/profile&gt; &lt;/profiles&gt;&lt;/settings&gt; localRepository ​ 本地maven仓库路径 pluginGroups proxies ​ 代理设置 servers mirrors profiles activeProfiles pom.xmlScope compile：默认的scope，运行期有效，需要打入包中 provided：编译期有效，运行期不需要提供，不会打入包中 runtime：编译不需要，在运行期有效，需要导入包中。（接口与实现分离） test：测试需要，不会打入包中 system：非本地仓库引入、存在系统的某个路径下的jar。（一般不使用） maven的常见命令项目编译：mvn clean package -Dmaven.test.skip 使用指定的setting.xml文件：mvn clean package --settings ~/.m2/settings.xml.fast -Dmaven.test.skip 收集项目所依赖的lib：mvn dependency:copy-dependencies -DoutputDirectory=lib 下载源码：mvn dependency:sources 将jar包安装到本地仓库mvn install:install-file -Dfile=... -DgroupId=... -DartifactId=... -Dversion=... -Dpackaging=jar 如：mvn install:install-file -Dfile=IKAnalyzer2012_FF.jar -DgroupId=org.wltea.ik-analyzer -DartifactId=ik-analyzer -Dversion=5.0.0 -Dpackaging=jar 将jar包发布到私服mvn deploy:deploy-file -DgroupId=... -DartifactId=... -Dversion=... -Dgenertatepom=true -Dpackaging=jar -Dfile=... -Durl=... mvn deploy:deploy-file --settings ~/.m2/settings.xml -DgroupId=sqlline -DartifactId=sqlline -Dversion=1.1.8 -Dgenertatepom=true -Dpackaging=jar -Dfile=/Users/wangshang/.m2/repository/sqlline/sqlline/1.1.8/sqlline-1.1.8.jar -Durl=... -DrepositoryId=archiva.internal 如果是本地项目打包上传需要到私服，需要在pom.xml文件和maven的settings.xml配置文件中做类似如下的配置： pom.xml 配置要上传的私服地址 123456789101112&lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;archiva.internal&lt;/id&gt; &lt;name&gt;Internal Release Repository&lt;/name&gt; &lt;url&gt;...&lt;/url&gt; &lt;/repository&gt; &lt;snapshotRepository&gt; &lt;id&gt;archiva.snapshots&lt;/id&gt; &lt;name&gt;Internal Snapshot Repository&lt;/name&gt; &lt;url&gt;...&lt;/url&gt; &lt;/snapshotRepository&gt; &lt;/distributionManagement&gt; settings.xml 配置私服的认证信息 123456789101112&lt;servers&gt; &lt;server&gt; &lt;id&gt;archiva.internal&lt;/id&gt; &lt;username&gt;...&lt;/username&gt; &lt;password&gt;...&lt;/password&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;archiva.snapshots&lt;/id&gt; &lt;username&gt;...&lt;/username&gt; &lt;password&gt;...&lt;/password&gt; &lt;/server&gt; &lt;/servers&gt;","categories":[{"name":"工具使用","slug":"工具使用","permalink":"https://shang.at/categories/工具使用/"}],"tags":[{"name":"maven","slug":"maven","permalink":"https://shang.at/tags/maven/"}]},{"title":"数据结构与算法-左神学习笔记12-暴力递归到动态规划1-4","slug":"数据结构与算法-左神学习笔记12-暴力规划到动态规划1-4","date":"2020-07-23T04:19:14.000Z","updated":"2020-07-23T06:00:30.532Z","comments":true,"path":"post/数据结构与算法-左神学习笔记12-暴力规划到动态规划1-4/","link":"","permalink":"https://shang.at/post/数据结构与算法-左神学习笔记12-暴力规划到动态规划1-4/","excerpt":"简介：","text":"简介：","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"}],"tags":[{"name":"左神学习笔记","slug":"左神学习笔记","permalink":"https://shang.at/tags/左神学习笔记/"}]},{"title":"数据结构与算法-左神学习笔记11-动态规划","slug":"数据结构与算法-左神学习笔记11-动态规划","date":"2020-07-23T04:18:52.000Z","updated":"2020-07-23T06:00:26.672Z","comments":true,"path":"post/数据结构与算法-左神学习笔记11-动态规划/","link":"","permalink":"https://shang.at/post/数据结构与算法-左神学习笔记11-动态规划/","excerpt":"简介：","text":"简介：","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"}],"tags":[{"name":"左神学习笔记","slug":"左神学习笔记","permalink":"https://shang.at/tags/左神学习笔记/"}]},{"title":"数据结构与算法-左神学习笔记10-暴力递归","slug":"数据结构与算法-左神学习笔记10-暴力递归","date":"2020-07-23T04:18:43.000Z","updated":"2020-07-23T06:00:24.432Z","comments":true,"path":"post/数据结构与算法-左神学习笔记10-暴力递归/","link":"","permalink":"https://shang.at/post/数据结构与算法-左神学习笔记10-暴力递归/","excerpt":"简介：","text":"简介：","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"}],"tags":[{"name":"左神学习笔记","slug":"左神学习笔记","permalink":"https://shang.at/tags/左神学习笔记/"}]},{"title":"数据结构与算法-左神学习笔记9-并查集&图","slug":"数据结构与算法-左神学习笔记9-并查集-图","date":"2020-07-23T04:18:31.000Z","updated":"2021-01-21T01:13:06.122Z","comments":true,"path":"post/数据结构与算法-左神学习笔记9-并查集-图/","link":"","permalink":"https://shang.at/post/数据结构与算法-左神学习笔记9-并查集-图/","excerpt":"简介：","text":"简介： 并查集：可以解决连通性问题，","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"}],"tags":[{"name":"左神学习笔记","slug":"左神学习笔记","permalink":"https://shang.at/tags/左神学习笔记/"}]},{"title":"数据结构与算法-左神学习笔记8-贪心算法","slug":"数据结构与算法-左神学习笔记8-贪心算法","date":"2020-07-23T04:18:15.000Z","updated":"2020-07-23T06:00:18.411Z","comments":true,"path":"post/数据结构与算法-左神学习笔记8-贪心算法/","link":"","permalink":"https://shang.at/post/数据结构与算法-左神学习笔记8-贪心算法/","excerpt":"简介：","text":"简介：","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"}],"tags":[{"name":"左神学习笔记","slug":"左神学习笔记","permalink":"https://shang.at/tags/左神学习笔记/"}]},{"title":"数据结构与算法-左神学习笔记7-二叉树的递归套路","slug":"数据结构与算法-左神学习笔记7-二叉树的递归套路","date":"2020-07-23T04:18:05.000Z","updated":"2020-07-23T06:00:15.894Z","comments":true,"path":"post/数据结构与算法-左神学习笔记7-二叉树的递归套路/","link":"","permalink":"https://shang.at/post/数据结构与算法-左神学习笔记7-二叉树的递归套路/","excerpt":"简介：","text":"简介：","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"}],"tags":[{"name":"左神学习笔记","slug":"左神学习笔记","permalink":"https://shang.at/tags/左神学习笔记/"}]},{"title":"数据结构与算法-左神学习笔记6-链表相关面试题","slug":"数据结构与算法-左神学习笔记6-链表相关面试题","date":"2020-07-23T04:17:43.000Z","updated":"2020-07-24T10:06:39.139Z","comments":true,"path":"post/数据结构与算法-左神学习笔记6-链表相关面试题/","link":"","permalink":"https://shang.at/post/数据结构与算法-左神学习笔记6-链表相关面试题/","excerpt":"简介：","text":"简介：","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"}],"tags":[{"name":"左神学习笔记","slug":"左神学习笔记","permalink":"https://shang.at/tags/左神学习笔记/"}]},{"title":"数据结构与算法-左神学习笔记5-Trie&桶排序&排序总结","slug":"数据结构与算法-左神学习笔记5-Trie-桶排序-排序总结","date":"2020-07-23T04:17:29.000Z","updated":"2020-07-23T06:00:10.925Z","comments":true,"path":"post/数据结构与算法-左神学习笔记5-Trie-桶排序-排序总结/","link":"","permalink":"https://shang.at/post/数据结构与算法-左神学习笔记5-Trie-桶排序-排序总结/","excerpt":"简介：","text":"简介：","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"}],"tags":[{"name":"左神学习笔记","slug":"左神学习笔记","permalink":"https://shang.at/tags/左神学习笔记/"}]},{"title":"数据结构与算法-左神学习笔记4-比较器&堆","slug":"数据结构与算法-左神学习笔记4-比较器-堆","date":"2020-07-23T04:17:04.000Z","updated":"2020-12-04T02:47:43.775Z","comments":true,"path":"post/数据结构与算法-左神学习笔记4-比较器-堆/","link":"","permalink":"https://shang.at/post/数据结构与算法-左神学习笔记4-比较器-堆/","excerpt":"简介：","text":"简介： 堆是一种特殊的二叉树结构-完全二叉树，它可以使用连续的数组来表示 完全二叉树:二叉树的每一层都是从左到右填充","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"}],"tags":[{"name":"左神学习笔记","slug":"左神学习笔记","permalink":"https://shang.at/tags/左神学习笔记/"}]},{"title":"数据结构与算法-左神学习笔记3-归并&随机快排","slug":"数据结构与算法-左神学习笔记3-归并-随机快排","date":"2020-07-23T04:16:44.000Z","updated":"2020-07-23T06:00:06.044Z","comments":true,"path":"post/数据结构与算法-左神学习笔记3-归并-随机快排/","link":"","permalink":"https://shang.at/post/数据结构与算法-左神学习笔记3-归并-随机快排/","excerpt":"简介：","text":"简介：","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"}],"tags":[{"name":"左神学习笔记","slug":"左神学习笔记","permalink":"https://shang.at/tags/左神学习笔记/"}]},{"title":"数据结构与算法-左神学习笔记2-链表&栈&队列&递归&哈希表","slug":"数据结构与算法-左神学习笔记2-链表-栈-队列-递归-哈希表","date":"2020-07-23T04:16:10.000Z","updated":"2020-07-23T06:00:03.558Z","comments":true,"path":"post/数据结构与算法-左神学习笔记2-链表-栈-队列-递归-哈希表/","link":"","permalink":"https://shang.at/post/数据结构与算法-左神学习笔记2-链表-栈-队列-递归-哈希表/","excerpt":"简介：","text":"简介：","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"}],"tags":[{"name":"左神学习笔记","slug":"左神学习笔记","permalink":"https://shang.at/tags/左神学习笔记/"}]},{"title":"数据结构与算法-左神学习笔记1-复杂度&对数器&二分法&位运算","slug":"数据结构与算法-左神学习笔记1-复杂度-对数器-二分法-位运算","date":"2020-07-23T04:15:02.000Z","updated":"2020-11-06T09:30:44.702Z","comments":true,"path":"post/数据结构与算法-左神学习笔记1-复杂度-对数器-二分法-位运算/","link":"","permalink":"https://shang.at/post/数据结构与算法-左神学习笔记1-复杂度-对数器-二分法-位运算/","excerpt":"简介：","text":"简介： 复杂度对数器二分法位运算","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"}],"tags":[{"name":"左神学习笔记","slug":"左神学习笔记","permalink":"https://shang.at/tags/左神学习笔记/"}]},{"title":"大数据-aws","slug":"大数据-aws","date":"2020-07-23T01:08:29.000Z","updated":"2020-07-23T01:13:53.160Z","comments":true,"path":"post/大数据-aws/","link":"","permalink":"https://shang.at/post/大数据-aws/","excerpt":"简介：","text":"简介： AWS常见命令aws s3 [commend] help aws s3 ls [—recursive] [—human-readable] [—summarize] aws s3 rm s3_path [—recursive] aws s3 cp s3_path target_path [—recursive]","categories":[{"name":"大数据","slug":"大数据","permalink":"https://shang.at/categories/大数据/"}],"tags":[{"name":"aws","slug":"aws","permalink":"https://shang.at/tags/aws/"}]},{"title":"Python学习-文件操作","slug":"Python学习-文件操作","date":"2020-07-22T03:24:24.000Z","updated":"2020-07-22T06:20:32.279Z","comments":true,"path":"post/Python学习-文件操作/","link":"","permalink":"https://shang.at/post/Python学习-文件操作/","excerpt":"简介：","text":"简介： 打开文件：open(filename, &#39;r|w|x|a|b|t|+&#39;, buffering=4096,) 检测路径是否存在：os.path.exists(path) 创建目录：os.mkdir(path) 递归创建路径：os.makedirs(path) 删除目录：os.rmdir(path) 递归删除目录：shutil.rmtree(path) 获取当前的工作目录：os.getcwd() 判断给定的路径是否为目录：os.path.isdir(source) 获取给定文件全路径的一级目录：os.path.dirname(source) 获取给定文件全路径的文件名：os.path.basename(source) 修改工作目录：os.chdir(source_dir) 拼接路径：os.path.join(path1, path2) 执行shell命令：os.system(cmd) 循环遍历指定目录下 的(直属的)所有文件(夹)：os.listdir(path) 递归遍历指定目录下的所有文件(夹)：os.walk(path)，返回的是一个generator 12345#conding=utf8 import os for path,dir_list,file_list in os.walk(path): for file_name in file_list: print(os.path.join(path, file_name))","categories":[{"name":"Python","slug":"Python","permalink":"https://shang.at/categories/Python/"}],"tags":[{"name":"文件操作","slug":"文件操作","permalink":"https://shang.at/tags/文件操作/"}]},{"title":"操作系统-网络的配置","slug":"操作系统-网络的配置","date":"2020-07-15T14:24:58.000Z","updated":"2020-12-11T07:58:56.809Z","comments":true,"path":"post/操作系统-网络的配置/","link":"","permalink":"https://shang.at/post/操作系统-网络的配置/","excerpt":"简介：本例中以CentOS 7举例说明如何设置Linux开机自动获取IP地址和设置固定IP地址。","text":"简介：本例中以CentOS 7举例说明如何设置Linux开机自动获取IP地址和设置固定IP地址。 自动获取动态IP地址1.输入“ip addr”并按回车键确定，发现无法获取IP(CentOS 7默认没有ifconfig命令)，记录下网卡名称（本例中为ens33）。 2.输入“cd /etc/sysconfig/network-scripts/”按回车键确定，继续输入“ls”按回车键查看文件。 3.输入“vi ifcfg-ens33”并按回车键确定（网卡名称可能不同）。亦可在第二步直接输入“cd /etc/sysconfig/network-scripts/ifcfg-ens33”直接编辑文件。 4.查看最后一项（蓝色框内），发现为“ONBOOT=no”。 5.按“i”键进入编辑状态，将最后一行“no”修改为“yes”，然后按“ESC”键退出编辑状态，并输入“:x”保存退出。 6.输入“service network restart”重启服务,亦可输入“systemctl restart netwrok”。 7.再次输入“ip addr”查看，现已可自动获取IP地址。 设置静态IP地址8.输入“cd /etc/sysconfig/network-scripts/”按回车键确定，继续输入“ls”按回车键查看文件，确定网卡名称。 9.输入“vi ifcfg-ens33”并按回车键确定（网卡名称可能不同）。如确知网卡名称可直接输入“cd /etc/sysconfig/network-scripts/ifcfg-ens33”编辑文件。 10.按“i”进入编辑状态，设置为“BOOTPROTO=’static’”（如设置为none则禁止DHCP，static则启用静态IP地址，设置为dhcp则为开启DHCP服务），并修改其他部分为您的设置， 本例中为192.168.1.200/24，GW:192.168.1.1，可根据您的需要配置IPV6部分。 注意：NM_CONTROLLED=no和ONBOOT=yes可根据您的需求进行设置。 11.确认无误后按“ESC”退出编辑状态，并输入“:x”保存退出，输入“service network restart”重启服务后输入“ifconfig”查看网络配置。 12.如需设置DNS(需在第9步设置NM_CONTROLLED=no)则输入“vi /etc/resolv.conf”并按回车键执行命令（如已在第9步配置DNS则可省略此步骤）。 13.在此文件里面输入DNS服务器地址（本例中以广东电信为例，亦可输入路由器DNS地址）并保存退出。","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://shang.at/categories/操作系统/"}],"tags":[{"name":"网络的配置","slug":"网络的配置","permalink":"https://shang.at/tags/网络的配置/"}]},{"title":"工具使用-UML","slug":"工具使用-UML","date":"2020-07-15T03:36:56.000Z","updated":"2020-12-11T08:09:12.465Z","comments":true,"path":"post/工具使用-UML/","link":"","permalink":"https://shang.at/post/工具使用-UML/","excerpt":"简介：统一建模语言（Unified Modeling Language，缩写UML），是非专利的第三代建模和规约语言。UML是一种开放的方法，用于说明、可视化、构建和编写一个正在开发的、面向对象的、软件密集系统的制品的开放方法。","text":"简介：统一建模语言（Unified Modeling Language，缩写UML），是非专利的第三代建模和规约语言。UML是一种开放的方法，用于说明、可视化、构建和编写一个正在开发的、面向对象的、软件密集系统的制品的开放方法。 推荐使用https://app.lucidchart.com/在线工具进行绘制 UML模型和图形 UML分为模型和图形两大类。区分UML模型和UML图是非常重要的，UML图（包括用例图、协作图、活动图、序列图、部署图、构件图、类图、状态图）是模型中信息的图表表达形式，但是UML模型独立于UML图存在。 在UML系统开发中有三个主要的模型： 功能模型：从用户的角度展示系统的功能，包括用例图。 对象模型：采用对象，属性，操作，关联等概念展示系统的结构和基础，包括类别图、对象图。 动态模型：展现系统的内部行为。包括序列图，活动图，状态图。 UML2.2中一共定义了14种图示。 结构性图形（Structure diagrams）强调的是系统式的建模： 静态图（static diagram)：包括类图、对象图、包图 实现图（implementation diagram）：包括组件图、部署图 剖面图 复合结构图 行为式图形（Behavior diagrams）强调系统模型中触发的事件 活动图 状态图 用例图 交互性图形（Interaction diagrams），属于行为图形的子集合，强调系统模型中的资料流程 通信图 交互概述图 时序图 时间图 2 UML类图作用UML展现了一系列最佳工程实践，这些最佳实践在对大规模，复杂系统进行建模方面，特别是软件架构层次方面已经被验证有效。 我们这次介绍的主要是类图，为了解析项目的系统结构和架构层次，可以简洁明了的帮助我们理解项目中类之间的关系。 类图的作用： （1）：在软件工程中，类图是一种静态的结构图，描述了系统的类的集合，类的属性和类之间的关系，可以简化了人们对系统的理解； （2）：类图是系统分析和设计阶段的重要产物，是系统编码和测试的重要模型。 3 类图格式在UML类图中，类使用包含类名、属性(field) 和方法(method) 且带有分割线的矩形来表示， 举个栗子。一个Animal类，它包含name,age,state,isPet这4个属性，以及name相关方法。 1234567891011121314151617181920212223class Animal: NSObject &#123; public var name: String? internal var isPet: Bool? fileprivate var state: String? private var age: Int? = 0 override init() &#123; self.name = \"no name\" self.age = 0 self.isPet = true self.state = \"dead\" &#125; public func getName() -&gt; String &#123; return self.name! &#125; internal func setName(name: String?) &#123; self.name = name &#125;&#125;复制代码 对应UML类图： 类名：粗体，如果是类是抽象类则类名显示为斜体！ 属性： 可见性 名称：类型[=默认值] 可见性一般为public、private和protected，在类图分别用+、-和#表示，在Swift中没有与protected完全对应的可见控制，因此选用的是internal对应为#；名称为属性的名称；类型为数据类型；默认值如变量 age默认值为0。 方法： 可见性 名称（参数列表 参数1，参数2） ：返回类型 可见性如上名称表达式的介绍，名称就是方法名，参数列表是可选的项，多参数的话参数直接用英文逗号隔开；返回值也是个可选项，返回值类型可以说基本的数据类型、用户自定义类型和void。如果是构造方法，则无返回类型！ 4类与类之间的关系表达类图中类与类之间的关系主要由：继承、实现、依赖、关联、聚合、组合这六大类型。表示方式如下图： (1)继承关系（Generalization/extends）继承关系也叫泛化关系，指的是一个类（称为子类、子接口）继承另外的一个类（称为父类、父接口）的功能，并可以增加它自己的新功能的能力，继承是类与类或者接口与接口之间最常见的关系。 继承用实线空心箭头表示，由子类指向父类。 下面写两个子类，Fish和Cat分别继承自Animal。 1234567891011121314class Fish: Animal &#123; public var fishType: String? func swim() &#123; &#125;&#125;class Cat: Animal &#123; public var hasFeet: Bool? func playToy(doll:Doll) &#123; doll.toyMoved() &#125;&#125;复制代码 (2)实现关系（implements）指的是一个class类实现interface接口（可以是多个）的功能；实现是类与接口之间最常见的关系；在Java中此类关系通过关键字implements明确标识，在iOS中我将其理解成代理的实现。 写一个洋娃娃类Doll，该类遵循了ToyAction协议，实现了玩具移动的方法。 1234567891011121314protocol ToyAction &#123; func toyMoved() -&gt; Void&#125;class Doll: NSObject,ToyAction &#123; public var body: Body? public var cloth: Cloth? func toyMoved() &#123; //洋娃娃玩具动作具体实现 &#125;&#125;复制代码 (3)依赖关系（Dependency）可以简单的理解，就是一个类A使用到了另一个类B，而这种使用关系是具有偶然性的、、临时性的、非常弱的，但是B类的变化会影响到A；比如某人要过河，需要借用一条船，此时人与船之间的关系就是依赖；表现在代码层面，为类B作为参数被类A在某个method方法中使用。 在我们的上述代码中Cat的playToy方法中参数引用了Doll，因此他们是依赖关系。 (4)关联关系（Association）他体现的是两个类、或者类与接口之间语义级别的一种强依赖关系，比如我和我的朋友；这种关系比依赖更强、不存在依赖关系的偶然性、关系也不是临时性的，一般是长期性的，而且双方的关系一般是平等的、关联可以是单向、双向的；表现在代码层面，为被关联类B以类属性的形式出现在关联类A中，也可能是关联类A引用了一个类型为被关联类B的全局变量； 写一个Person类，他拥有一个宠物猫，他们之间的关系是关联。 123456789class Head: NSObject &#123; &#125;class Person: NSObject &#123; public var pet: Cat? public var head: Head?&#125;复制代码 (5)聚合关系（Aggregation）聚合是关联关系的一种特例，他体现的是整体与部分、拥有的关系，即has-a的关系，此时整体与部分之间是可分离的，他们可以具有各自的生命周期，部分可以属于多个整体对象，也可以为多个整体对象共享；比如计算机与CPU、公司与员工的关系等；表现在代码层面，和关联关系是一致的，只能从语义级别来区分； 12345678class Cloth: NSObject &#123; &#125;class Body: NSObject &#123; &#125;复制代码 在上述代码中Doll由Body和Cloth组成，且即使失去Cloth，Doll也可以正常存在。 (6)组合关系（Composition）组合也是关联关系的一种特例，他体现的是一种contains-a的关系，这种关系比聚合更强，也称为强聚合；他同样体现整体与部分间的关系，但此时整体与部分是不可分的，整体的生命周期结束也就意味着部分的生命周期结束；比如你和你的大脑；表现在代码层面，和关联关系是一致的，只能从语义级别来区分； 上述代码中的Person拥有Head，并且这个整体和部分是不可分割的。 最后来看看这个例子中的整体关系： 其实理解了之后我们发现还是很简单的，学会了之后就可以投入实践中了，举一个简单第三方库的类图例子，下图是Masonry的类图整理，可以看到项目结构很清晰的展示了出来。 12345转自作者：zhengyi链接：https://juejin.im/post/5d318b485188255957377ac3来源：掘金著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","categories":[{"name":"工具使用","slug":"工具使用","permalink":"https://shang.at/categories/工具使用/"}],"tags":[{"name":"UML","slug":"UML","permalink":"https://shang.at/tags/UML/"}]},{"title":"Java学习-设计模式","slug":"Java学习-设计模式","date":"2020-07-14T09:26:40.000Z","updated":"2020-12-11T08:22:04.244Z","comments":true,"path":"post/Java学习-设计模式/","link":"","permalink":"https://shang.at/post/Java学习-设计模式/","excerpt":"简介：Java中23种设计模式，分为创建型、结构型和行为型","text":"简介：Java中23种设计模式，分为创建型、结构型和行为型 创建型 对象实例化的模式，创建型模式用于解耦对象的实例化过程。 Singleton单例模式 某个类只能有一个实例，提供一个全局的访问点。 实现1 饿汉式123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/** * 饿汉式 * 类加载到内存后，就实例化一个单例，JVM保证线程安全 * 简单实用，推荐使用！ * 唯一缺点：不管用到与否，类装载时就完成实例化 * Class.forName(\"\") * （话说你不用的，你装载它干啥） */public class Mgr01 &#123; private static final Mgr01 INSTANCE = new Mgr01(); private Mgr01() &#123;&#125;; public static Mgr01 getInstance() &#123; return INSTANCE; &#125; public void m() &#123; System.out.println(\"m\"); &#125; public static void main(String[] args) &#123; Mgr01 m1 = Mgr01.getInstance(); Mgr01 m2 = Mgr01.getInstance(); System.out.println(m1 == m2); &#125;&#125;// 或者使用静态代码块进行初始化，和Mgr01一样public class Mgr02 &#123; private static final Mgr02 INSTANCE; static &#123; INSTANCE = new Mgr02(); &#125; private Mgr02() &#123;&#125;; public static Mgr02 getInstance() &#123; return INSTANCE; &#125; public void m() &#123; System.out.println(\"m\"); &#125; public static void main(String[] args) &#123; Mgr02 m1 = Mgr02.getInstance(); Mgr02 m2 = Mgr02.getInstance(); System.out.println(m1 == m2); &#125;&#125; 加载jdbc驱动的时候，就是用的Class.forName(“com.mysql.jdbc.Driver”)，在com.mysql.jdbc.Driver中就是在静态代码块中对com.mysql.jdbc.Driver进行的注册 实现2 懒汉式123456789101112131415161718192021222324252627282930313233343536373839404142/** * lazy loading * 也称懒汉式，需要时才加载 * 需要注意的点： * 1、DCL * 2、实例需要加volatile修饰 */public class Mgr06 &#123; private static volatile Mgr06 INSTANCE; //JIT private Mgr06() &#123; &#125; public static Mgr06 getInstance() &#123; if (INSTANCE == null) &#123; //双重检查 synchronized (Mgr06.class) &#123; if(INSTANCE == null) &#123; try &#123; Thread.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; INSTANCE = new Mgr06(); &#125; &#125; &#125; return INSTANCE; &#125; public void m() &#123; System.out.println(\"m\"); &#125; public static void main(String[] args) &#123; for(int i=0; i&lt;100; i++) &#123; new Thread(()-&gt;&#123; System.out.println(Mgr06.getInstance().hashCode()); &#125;).start(); &#125; &#125;&#125; 对于懒汉式的单例模式的详细解读：1、懒汉式一：synchronized加在getInstance()上，每次调用getInstance都会进行一次加锁行为，会导致效率低下2、懒汉式二：DCL模式，Double Check Lock，双重检查锁定​ a. 第一步检查，如果发现INSTANCE不为null，直接返回，不会产生加锁的行为，提高效率​ b. 第二步加锁，同时只能由一个线程获得锁，也只有一个线程能最终创建实例对象​ c. 第三步检查，加完锁之后再次检查，防止有多个线程在第一步检查时同时竞争锁，当一个线程完成了实例的创建，那么其他的线程再次获得锁时，发现实例已经不为空，那么退出临界区且返回实例对象3、INSTANCE对象需要使用volatile关键字修饰​ 貌似以上的操作已经没有什么问题了。但是这里还是有一个隐藏的bug：也就是CPU的乱序执行，指令重拍。由于CPU的乱序执行会导致实例对象的半初始化状态：即对象实例虽已创建，在内存中有对应的空间，但是这时候是还没有初始化实例属性，实例属性都是初始值。在java中，基础类型的变量都有各自的初始值，但是引用类型的初始值都是null，这个时候如果其他的线程拿到了实例对象，就会导致NullPointerException。所以INSTANCE对象需要增加volatile关键字修饰，valatile关键字可以禁用CPU的乱序执行。具体详情需要结合编译成的jvm指令集查看 实现3 静态内部类123456789101112131415161718192021222324252627282930/** * 静态内部类方式 * JVM保证单例 * 加载外部类时不会加载内部类，这样可以实现懒加载 */public class Mgr07 &#123; private Mgr07() &#123; &#125; private static class Mgr07Holder &#123; private final static Mgr07 INSTANCE = new Mgr07(); &#125; public static Mgr07 getInstance() &#123; return Mgr07Holder.INSTANCE; &#125; public void m() &#123; System.out.println(\"m\"); &#125; public static void main(String[] args) &#123; for(int i=0; i&lt;100; i++) &#123; new Thread(()-&gt;&#123; System.out.println(Mgr07.getInstance().hashCode()); &#125;).start(); &#125; &#125;&#125; 实现4 枚举单例12345678910111213141516171819202122232425262728293031323334353637383940/** * 不仅可以解决线程同步，还可以防止反序列化。 * JVM 会阻止反射获取枚举类的私有构造方法，即可以 避免使用者使用反射的方式破坏单例的唯一性 */public enum Mgr08 &#123; INSTANCE; public void m() &#123;&#125; public static void main(String[] args) &#123; for(int i=0; i&lt;100; i++) &#123; new Thread(()-&gt;&#123; System.out.println(Mgr08.INSTANCE.hashCode()); &#125;).start(); &#125; &#125;&#125;/** * 内部隐藏的enum来生成单例对象 */public class SingletonObject7 &#123; private SingletonObject7()&#123;&#125; /** * 枚举类型是线程安全的，并且只会装载一次 */ private enum Singleton&#123; INSTANCE; private final SingletonObject7 instance; Singleton()&#123; instance = new SingletonObject7(); &#125; private SingletonObject7 getInstance()&#123; return instance; &#125; &#125; public static SingletonObject7 getInstance()&#123; return Singleton.INSTANCE.getInstance(); &#125;&#125; 但是不能设置属性呢？？？？？？ 如何防止用户破坏单例的唯一性？ 1、通过反射的方式破坏单例,反射是通过调用构造方法生成新的对象 2、如果单例类实现了序列化接口Serializable, 就可以通过反序列化破坏单例， 3、使用枚举单例 FactoryMethod工厂模式AbstractFactory抽象工厂模式Builder构建器Prototype原型结构性 把类或对象结合在一起形成一个更大的结构。 Facade门面 模式Decorator装饰器 模式Composite组合模式Flyweight享元Adapter适配器Bridge桥接Proxy静态代理与动态代理行为型 类和对象如何交互，及划分责任和算法。 Strategry策略模式ChainOfResponsibility责任链 模式Observer观察者 模式Mediator调停者 模式Iterator迭代器 模式Visitor访问者 模式Command命令 模式Memento备忘录 模式TemplateMethod模板方法State状态 模式Intepreter解释器 模式","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://shang.at/tags/设计模式/"}]},{"title":"容器-Docker","slug":"容器-Docker","date":"2020-07-13T23:36:10.000Z","updated":"2021-05-20T11:39:16.441Z","comments":true,"path":"post/容器-Docker/","link":"","permalink":"https://shang.at/post/容器-Docker/","excerpt":"简介：","text":"简介： 以查找并安装 Selenium-hub容器为例，记录docker的常用命令 首先设置国内docker hub镜像 ➜ ~ docker search hub 在docker hub中查找指定的镜像 ➜ ~ docker ps 查看本机运行的所有docker镜像进程 ➜ ~ docker ps -a 查看本机的所有docker镜像进程 ➜ ~ docker pull selenium/hub 下载指定的镜像 ➜ ~ docker search selenium ➜ ~ docker pull selenium/node-chrome ➜ ~ docker images 查看本地下载的所有镜像 ➜ ~ docker run -d -P —name selenium-hub selenium/hub 启动selenium/hub镜像，不指定端口映射，docker会自动将容器的端口映射到本机的一个空闲端口 ➜ ~ docker run -d -p 4444:4444 —name selenium-hub selenium/hub 启动selenium/hub镜像，指定端口映射 ➜ ~ docker stop f16ee70391f5 停止一个运行中的docker容器 ➜ ~ docker rm f16ee70391f5 删除一个停止的docker容器 ➜ ~ docker exec -it a98b84585114 /bin/bash 进入一个运行中容器bash命令行 ➜ ~ docker run -d —link selenium-hub:hub selenium/node-chrome 启动容器并链接到指定的容器 ➜ ~ docker run -d —link selenium-hub:hub -e NODE_MAX_INSTANCES=5 -e NODE_MAX_SESSION=5 selenium/node-chrome 启动容器并链接到指定的容器，指定环境变量 容器化技术介绍物理机时代 部署非常慢 成本很高 资源浪费 难于扩展与迁移 受制于硬件 虚拟机时代 多部署 资源池 资源隔离 很容易扩展 VM需要安装操作系统 容器化时代 初识DockerDocker是一个平台，其上承载了容器 开源的应用容器引擎，基于 Go 语言开发 容器是完全使用沙箱机制,创建和销毁容器开销极低 Docker就是容器化技术的代名词 Docker也具备一定虚拟化职能，利用Docker不仅可以创建容器，还能向本地的物理机申请CPU、内存等计算资源 标准化的应用打包提供了一种标准化的应用打包方式，一个镜像文件包含了一个应用运行所需要的所有信息，开箱即用，同时他还描述了运行这些应用需要哪些硬件上的要求，比如CPU、内存、硬盘 阿里云Docker镜像加速Docker的基本概念 Docker引擎包含三个部分： Server - docker daemon：容器的管理 Rest API - 基于HTTP协议 Client - docker cli server和client可以不在同一个节点上 - CS模式 容器与镜像 镜像：是一个文件，是只读的，不可运行，提供了运行程序完整的软硬件资源，是应用程序的集装箱 容器：是镜像的实例，由Docker负责创建，容器之间彼此隔离 Docker的执行流程 Client：客户端，用户提交命令的地方 Docker Host Docker daemon：Docker的守护进程，管理该节点上的镜像与容器 Images：镜像文件 Containers：运行的容器实例 Registry：注册中心，远程仓库，保存了各种各样的镜像文件 Docker常用命令 docker pull 镜像名&lt;:tags&gt; - 从远程仓库抽取镜像 docker images - 查看本地镜像 docker run 镜像名&lt;:tags&gt; - 创建容器，启动应用 docker ps - 查看正在运行中的镜像 docker rm &lt;-f&gt; 容器id - 删除容器 docker rmi &lt;-f&gt; 镜像名&lt;:tags&gt; - 删除镜像 Docker Hub: Docker宿主机与容器通信","categories":[{"name":"容器","slug":"容器","permalink":"https://shang.at/categories/容器/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://shang.at/tags/Docker/"}]},{"title":"","slug":"操作系统-CentOS7上的开发环境配置","date":"2020-07-13T14:02:29.000Z","updated":"2020-08-10T04:05:09.112Z","comments":true,"path":"post/操作系统-CentOS7上的开发环境配置/","link":"","permalink":"https://shang.at/post/操作系统-CentOS7上的开发环境配置/","excerpt":"简介：在Centos上进行开发的环境搭建","text":"简介：在Centos上进行开发的环境搭建 Linux Tools Quick Tutorial 安装必要的软件升级 yum源&amp;安装相关依赖包123sudo yum -y updatesudo yum -y upgradesudo yum install -y net-tools rsync mlocate wget vim ntpdate telnet gcc zlib-dev openssl-devel sqlite-devel bzip2-devel binutils qt make patch libgomp glibc-headers glibc-devel kernel-headers kernel-devel dkms strace 虚拟机软件Vagrant这里查看vagrant的详细使用 123wget https://releases.hashicorp.com/vagrant/2.2.9/vagrant_2.2.9_x86_64.rpmrpm -ivh vagrant_2.2.9_x86_64.rpm vagrant是基于VirtualBox的虚拟机管理软件 VirualBox1wget https://download.virtualbox.org/virtualbox/6.1.10/VirtualBox-6.1-6.1.10_138449_el7-1.x86_64.rpm rpm -ivh VirtualBox-6.1-6.1.10_138449_el7-1.x86_64.rpm 报错如下：libSDL-1.2.so.0()(64bit) 被 VirtualBox-6.1-6.1.10_138449_el7-1.x86_64 需要那么，需要安装SDL，yum install -y SDL 同时要保证安装的kernel-header和kernel的版本一致。安装完kernel之后，需要重启，可以使用uname -r 查看kernel版本 自定义VirtualBox的保存路径： 1VBoxManage setproperty machinefolder 目标路径 老的虚拟机位置不会变，新建的虚拟机会存在新的路径下 常见的命令： 创建虚拟机 创建并注册 1VBoxManage createvm --name learnAsm --register 删除虚拟机 （！！！会删除所有虚拟硬盘，谨慎操作！！！） 1VBoxManage unregistervm --delete learnAsm 注册虚拟机 假如你注销了，或者从别人那里复制来的虚拟机文件，可以重新注册它 1VBoxManage registervm &lt;your vms path&gt;/learnAsm.vbox 仅注销虚拟机 注销之后VirtualBox列表中显示了 1VBoxManage unregistervm learnAsm 列出已有的虚拟机 1VBoxManage list vms 查看虚拟机信息 123456VBoxManage -vVBoxManage list vms #列出虚拟机VBoxManage list runningvms #列出正在运行的虚拟机VBoxManage showvminfo learnAsm #显示虚拟机learnAsm的信息VBoxManage list hdds #列出硬盘VBoxManage list dvds #列出dvd Docker这里查看Docker的详细使用 123456789101112131415161718192021222324252627282930# 安装工具包yum install -y yum-utils device-mapper-persistent-data lvm2# 安装ali源yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo# 刷新安装源yum makecache fast# 安装 docker-ceyum -y install docker-ce# 启动dockersystemctl start docker# 停止dockersystemctl stop docker# 开机自启动systemctl enable docker# 配置aliyun的容器镜像加速器 # 登录aliyun.com 搜索容器镜像服务，进入管理后台，在左侧菜单栏下面可以找到 镜像加速器 选项sudo mkdir -p /etc/dockersudo tee /etc/docker/daemon.json &lt;&lt;-'EOF'&#123; \"registry-mirrors\": [\"https://oooxxxoox.mirror.aliyuncs.com\"]&#125;EOFsudo systemctl daemon-reloadsudo systemctl restart docker 实时监控网速1234567891011# 安装之前均需要安装epel源，yum -y install epel-release# 安装iftop工具yum install iftop -y# 查看开启的网卡ifconfig # 使用iftop监控，会进入一个类似top命令的界面，监控连接的地址以及网速iftop -i em4 systemctl命令详解123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107Systemctl是一个系统管理守护进程、工具和库的集合，用于取代System V、service和chkconfig命令，初始进程主要负责控制systemd系统和服务管理器。通过Systemctl –help可以看到该命令主要分为：查询或发送控制命令给systemd服务，管理单元服务的命令，服务文件的相关命令，任务、环境、快照相关命令，systemd服务的配置重载，系统开机关机相关的命令。 1. 列出所有可用单元 # systemctl list-unit-files2. 列出所有运行中单元 # systemctl list-units3. 列出所有失败单元 # systemctl –failed4. 检查某个单元（如 crond.service）是否启用 # systemctl is-enabled crond.service 5. 列出所有服务 # systemctl list-unit-files –type=service6. Linux中如何启动、重启、停止、重载服务以及检查服务（如 httpd.service）状态 # systemctl start httpd.service# systemctl restart httpd.service# systemctl stop httpd.service# systemctl reload httpd.service# systemctl status httpd.service注意：当我们使用systemctl的start，restart，stop和reload命令时，终端不会输出任何内容，只有status命令可以打印输出。7. 如何激活服务并在开机时启用或禁用服务（即系统启动时自动启动mysqld.service服务） # systemctl is-active mysqld.service# systemctl enable mysqld.service# systemctl disable mysqld.service8. 如何屏蔽（让它不能启动）或显示服务（如ntpdate.service） # systemctl mask ntpdate.serviceln -s ‘/dev/null”/etc/systemd/system/ntpdate.service’# systemctl unmask ntpdate.servicerm ‘/etc/systemd/system/ntpdate.service’9. 使用systemctl命令杀死服务 # systemctl killcrond 10. 列出所有系统挂载点 # systemctl list-unit-files –type=mount11. 挂载、卸载、重新挂载、重载系统挂载点并检查系统中挂载点状态 # systemctl start tmp.mount# systemctl stop tmp.mount# systemctl restart tmp.mount# systemctl reload tmp.mount# systemctl status tmp.mount12. 在启动时激活、启用或禁用挂载点（系统启动时自动挂载） # systemctl is-active tmp.mount# systemctl enable tmp.mount# systemctl disable tmp.mount13. 在Linux中屏蔽（让它不能启用）或可见挂载点 # systemctl mask tmp.mountln -s ‘/dev/null”/etc/systemd/system/tmp.mount’# systemctl unmask tmp.mountrm ‘/etc/systemd/system/tmp.mount’14. 列出所有可用系统套接口 # systemctl list-unit-files –type=socket15. 检查某个服务的所有配置细节 # systemctl show mysqld16. 获取某个服务（httpd）的依赖性列表 # systemctl list-dependencies httpd.service17. 启动救援模式 # systemctl rescue18. 进入紧急模式 # systemctl emergency19. 列出当前使用的运行等级 # systemctl get-default20. 启动运行等级5，即图形模式 # systemctl isolate runlevel5.target或# systemctl isolate graphical.target21. 启动运行等级3，即多用户模式（命令行） # systemctl isolate runlevel3.target或# systemctl isolate multiuser.target22. 设置多用户模式或图形模式为默认运行等级 # systemctl set-default runlevel3.target# systemctl set-default runlevel5.target23. 重启、停止、挂起、休眠系统或使系统进入混合睡眠 # systemctl reboot# systemctl halt# systemctl suspend# systemctl hibernate# systemctl hybrid-sleep对于不知运行等级为何物的人，说明如下。Runlevel 0 : 关闭系统Runlevel 1 : 救援，维护模式Runlevel 3 : 多用户，无图形系统Runlevel 4 : 多用户，无图形系统Runlevel 5 : 多用户，图形化系统Runlevel 6 : 关闭并重启机器 Centos重启与关机 Linux centos重启命令： 1、reboot 2、shutdown -r now 立刻重启(root用户使用) 3、shutdown -r 10 过10分钟自动重启(root用户使用) 4、shutdown -r 20:35 在时间为20:35时候重启(root用户使用) 如果是通过shutdown命令设置重启的话，可以用shutdown -c命令取消重启 Linux centos关机命令： 1、halt 立刻关机 2、poweroff 立刻关机 3、shutdown -h now 立刻关机(root用户使用) 4、shutdown -h 10 10分钟后自动关机 如果是通过shutdown命令设置关机的话，可以用shutdown -c命令取消重启","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://shang.at/categories/操作系统/"}],"tags":[{"name":"Centos开发环境","slug":"Centos开发环境","permalink":"https://shang.at/tags/Centos开发环境/"}]},{"title":"操作系统-CentOS7时区设置","slug":"操作系统-CentOS7时区设置","date":"2020-07-04T14:20:22.000Z","updated":"2020-07-26T03:29:55.319Z","comments":true,"path":"post/操作系统-CentOS7时区设置/","link":"","permalink":"https://shang.at/post/操作系统-CentOS7时区设置/","excerpt":"简介：","text":"简介： https://www.cnblogs.com/zhangeamon/p/5500744.html","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://shang.at/categories/操作系统/"}],"tags":[{"name":"centos7时区设置","slug":"centos7时区设置","permalink":"https://shang.at/tags/centos7时区设置/"}]},{"title":"Linux-ssh","slug":"Linux-ssh","date":"2020-07-04T14:13:25.000Z","updated":"2020-07-04T14:13:59.375Z","comments":true,"path":"post/Linux-ssh/","link":"","permalink":"https://shang.at/post/Linux-ssh/","excerpt":"简介：","text":"简介： http://feihu.me/blog/2014/env-problem-when-ssh-executing-command-on-remote/","categories":[{"name":"Linux","slug":"Linux","permalink":"https://shang.at/categories/Linux/"}],"tags":[{"name":"ssh","slug":"ssh","permalink":"https://shang.at/tags/ssh/"}]},{"title":"Python学习-deque","slug":"Python学习-deque","date":"2020-07-02T15:52:28.000Z","updated":"2020-07-02T15:52:52.037Z","comments":true,"path":"post/Python学习-deque/","link":"","permalink":"https://shang.at/post/Python学习-deque/","excerpt":"简介：","text":"简介： https://zhuanlan.zhihu.com/p/63502912","categories":[{"name":"Python","slug":"Python","permalink":"https://shang.at/categories/Python/"}],"tags":[{"name":"deque","slug":"deque","permalink":"https://shang.at/tags/deque/"}]},{"title":"数据结构与算法-左神学习笔记-高级1-单调栈和窗口及其更新结构","slug":"数据结构与算法-左神学习笔记-高级1-单调栈和窗口及其更新结构","date":"2020-07-02T11:07:20.000Z","updated":"2020-08-19T08:09:18.921Z","comments":true,"path":"post/数据结构与算法-左神学习笔记-高级1-单调栈和窗口及其更新结构/","link":"","permalink":"https://shang.at/post/数据结构与算法-左神学习笔记-高级1-单调栈和窗口及其更新结构/","excerpt":"简介：","text":"简介：","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"}],"tags":[{"name":"左神学习笔记","slug":"左神学习笔记","permalink":"https://shang.at/tags/左神学习笔记/"}]},{"title":"Python学习-标准库学习","slug":"Python学习-标准库学习","date":"2020-07-01T09:17:07.000Z","updated":"2020-07-05T01:38:22.197Z","comments":true,"path":"post/Python学习-标准库学习/","link":"","permalink":"https://shang.at/post/Python学习-标准库学习/","excerpt":"简介：","text":"简介： https://docs.python.org/zh-cn/3/library/index.html","categories":[{"name":"Python","slug":"Python","permalink":"https://shang.at/categories/Python/"}],"tags":[{"name":"Python标准库","slug":"Python标准库","permalink":"https://shang.at/tags/Python标准库/"}]},{"title":"Python学习-itertools","slug":"Python学习-itertools","date":"2020-07-01T09:16:51.000Z","updated":"2021-01-25T10:20:52.122Z","comments":true,"path":"post/Python学习-itertools/","link":"","permalink":"https://shang.at/post/Python学习-itertools/","excerpt":"简介：","text":"简介： https://docs.python.org/zh-cn/3/library/itertools.html 12345678910# itertools.productdef product(*args, repeat=1): # product('ABCD', 'xy') --&gt; Ax Ay Bx By Cx Cy Dx Dy # product(range(2), repeat=3) --&gt; 000 001 010 011 100 101 110 111 pools = [tuple(pool) for pool in args] * repeat result = [[]] for pool in pools: result = [x+[y] for x in result for y in pool] for prod in result: yield tuple(prod) 1234567891011121314151617181920212223242526272829// js 版本的笛卡尔积// [[1,2,3], 2]// args的定义：可以传入多个数组，最后一个参数表示重复次数// 比如想得到0-2的笛卡尔积：// product([1,2], 2) -- 返回11 12 21 22// product([1,2], 3) -- 返回111 211 121 221 112 212 122 222// product(['A', 'B', 'C', 'D'], ['x', 'y'], 1) -- 返回Ax Ay Bx By Cx Cy Dx Dyfunction product(...args) &#123; var repeat = args[args.length - 1] var pools = args.slice(0, args.length - 1) var pools1 = pools.concat() for (let i = 1; i &lt; repeat; i++) &#123; pools = pools.concat(pools1) &#125; console.log(pools) // pools: [[1,2,3],[1,2,3]] var result = [[]] pools.forEach(pool =&gt; &#123; // pool: [1,2,3] var temp = [] pool.forEach(y =&gt; &#123; result.forEach(x =&gt; &#123; temp.push(x.concat([y])) &#125;) &#125;) result = temp &#125;); return result&#125;","categories":[{"name":"Python","slug":"Python","permalink":"https://shang.at/categories/Python/"}],"tags":[{"name":"itertools","slug":"itertools","permalink":"https://shang.at/tags/itertools/"}]},{"title":"Spark应用-大数据集的处理","slug":"Spark应用-大数据集的处理","date":"2020-07-01T09:12:03.000Z","updated":"2021-05-19T02:15:09.669Z","comments":true,"path":"post/Spark应用-大数据集的处理/","link":"","permalink":"https://shang.at/post/Spark应用-大数据集的处理/","excerpt":"简介：在spark上处理超大数据集的时候，有时候需要与外界系统进行交互或者需要将大数据量导出，下面讲解几种较方便的方案","text":"简介：在spark上处理超大数据集的时候，有时候需要与外界系统进行交互或者需要将大数据量导出，下面讲解几种较方便的方案 rdd.mapPartitions()https://stackoverflow.com/questions/21185092/apache-spark-map-vs-mappartitions 业务场景：需要给大批量的数据进行解密操作，第一种解决方案是将数据load到driver端(4G内存)，然后进行解密，但是当数据量达到一定的量级，driver便会OOM了，所以有了第二种方案：使用rdd.mapPartitions()，在excutor端进行解密操作。基本原理是每个partition执行一次解密操作，每个partition的数据量基本不会导致 excutor OOM了。 遇到的问题： 1、对于rdd.mapPartitions() 接口内部的数据类型不熟悉 ​ rdd.mapPartitions()接受的函数对象func，在被调起的时候，会有一个默认的参数传递给它，是一个iterator对象，可以接成这一个partition的数据集合，每个元素都是一个Row类型的变量，可以使用asDict()函数将其转换成map对象来操作。 ​ 可以这样理解，实际上就是在远程的excutor上操作了一个集合，处理完后再将结果返回 2、因为调用解密接口的时候需要动态传递一个参数到解密服务，但是rdd.mapPartitions() 执行逻辑的时候实际上是在excutor端执行的，因此传递给rdd.mapPartitions() 的函数并不能接收参数 ​ rdd.mapPartitions()的参数是一个函数，在python中叫做高阶函数，如果熟悉java的话，那实际上类似于java8(及以后)中lambda表达式或者java中的函数回调。高阶函数可以接收函数对象作为参数，同时也可以返回一个函数对象。这里就利用高阶函数的这个特性，在func外层包装一个make_func()函数，用它来接收外部传来的参数，然后在func内部使用make_func接收的参数，这种特性叫做闭包，闭包还有一些其他的陷阱，请见这里 在scala中也可以实现该过程，在scala中有一种柯里化的方案。看这里 3、解密的字段仅仅是极个别的字段 ​ 解决方案：先将需要解密的字段加一个特殊的后缀(注意要避免其他的字段有相同的后缀)；然后在func中拿到有特定后缀的字段，将其取出，并且调用解密服务；最后将字段重新命名回来。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576import copyimport jsonfrom pyspark.rdd import RDDfrom pyspark.sql import DataFramefrom pyspark.sql.types import Rowfrom asiacredit.conf.request import reqConffrom asiacredit.util.requeset.http_request import RequestKP_CHANNEL = 'XXXXXX'def make_func(channel): def func(iterator): request_obj = Request(url=reqConf.decryptConf.url, ak=reqConf.decryptConf.ak, sk=reqConf.decryptConf.sk) request_body = json.loads('&#123;\"encryptDataList\":\"\", \"decryptionChannel\":\"%s\"&#125;' % channel) _iterator = [item.asDict() for item in iterator] if len(_iterator) &lt;= 0: return [Row(**item) for item in _iterator] encrypted_columns = [k for k, v in _iterator[0].items() if '_encrypted' in k] for col in encrypted_columns: arr = [item[col] for item in _iterator if item[col] if arr: copy_request_body = copy.copy(request_body) copy_request_body['encryptDataList'] = arr request_obj.body = json.dumps(copy_request_body) encrypted_res = request_obj.request() col_res = &#123;line['encryptStr']: line['decryptStr'] for line in encrypted_res['decryptDataList']&#125; for i in range(len(_iterator)): if _iterator[i][col]: _iterator[i][col] = col_res[_iterator[i][col]] yield [Row(**item) for item in _iterator] return funcclass DecryptData: def __init__(self, encrypt_data: DataFrame, columns: list, decryption_channel=KP_CHANNEL, partition_num=0): \"\"\" 将解密操作放到集群节点上进行 如果不指定partition_num，会自动按照默认的分区数进行解密 :param encrypt_data: 要解密的DataFrame :param columns: 被加密的字段列表 :param decryption_channel: 加密的channel :param partition_num: 为1时，和第一版的接口效果一致，但是这里占用的是executor的资源，driver不会因为数据量过大二OOM executor的内存一般会比driver大很多，所以OOM的概率会降低 或者 这里在外层可以动态计算数据量设置一个合适的partition num \"\"\" self.columns = columns self.columns1 = [f'&#123;col&#125;_encrypted' for col in columns] self.source_columns = [col_name for col_name, _ in encrypt_data.dtypes] for old, new in zip(self.columns, self.columns1): encrypt_data = encrypt_data.withColumnRenamed(old, new) self.enough_data = encrypt_data.count() &gt; 0 self.encrypt_data = encrypt_data self.partition_num = partition_num if decryption_channel is None: raise AttributeError('decryption_channel can not be none') self.decryption_channel = decryption_channel def decrypt(self): if self.enough_data: encrypt_data_rdd: RDD = self.encrypt_data.rdd \\ if self.partition_num == 0 else self.encrypt_data.repartition(self.partition_num).rdd print('partition nums:' + str(encrypt_data_rdd.getNumPartitions())) res_df = encrypt_data_rdd.mapPartitions(make_func(self.decryption_channel)).flatMap( lambda line: line).toDF() else: print(\"there's no data to decrypt\") res_df = self.encrypt_data for old, new in zip(self.columns1, self.columns): res_df = res_df.withColumnRenamed(old, new) return res_df.select(*self.source_columns).distinct() toLocalIetator()https://www.waitingforcode.com/apache-spark/collecting-part-data-driver-rdd-tolocaiIterator/read toLocalIetator()的工作原理类似于python中的itertools.chain(*iterables)，类似于下面的方式： 12345def chain(*iterables): # chain('ABC', 'DEF') --&gt; A B C D E F for it in iterables: for element in it: yield element 它是将所有的partition形成一个类似于Ietator的形式，使用的时候，spark会将数据数据一部分一部分的从excutor端收集到driver端(从sparkui上也可以看出来)，并不会一次性将所有partition的数据全量收集到driver端，导致driver端的OOM，这也是一种导出大量数据时候的选择，但是数据需要经历一个从excutor向driver传输的过程，这个过程是比较费时的。一个使用案例如下： 123456789101112131415161718192021222324252627282930313233import shutildef _mkdir(base_dir, filename, delete=False): path = os.path.join(base_dir, filename) if os.path.exists(path) and delete: shutil.rmtree(path, True) if not os.path.exists(path): os.makedirs(path) return pathimport csvdef to_csv(df:DataFrame, filename, threshold=1000000, base_dir='result/'): row_num = 0 index = 0 csvwrite = None csvfile = None first = True for val in df.toLocalIterator():# print(type(val.asDict()))# print(val.asDict())# print([v for k,v in val.asDict().items()]) if first or row_num&gt;=threshold: print(filename + ' write ' + str(index) + '...' + str(threshold*index)) file_path = _mkdir(base_dir, filename, first) csvfile = open(os.path.join(file_path, filename+'_%04d.csv' % index), 'w', buffering=4096) csvwrite = csv.writer(csvfile) index += 1 row_num = 0 first = False csvwrite.writerow(val) csvfile.flush() row_num += 1 重分区写入集群，然后下载这种方案是：直接在集群内部设置适当的分区数，然后将数据写入集群内，最后将生成的文件下载下来。这种方式在数据量十分大的情况下，repartition的过程也是很费时的。 so，注意尽量不要使用repartition(numPartition, *col)，而是选择使用coalesce(numPartition)。这二者有很大区别：repartition会导致shuffle，会导致集群内部大量的数据传输，非常耗费时间；如果仅仅是将大量的partition合并成较少量的partition，可以直接使用coalesce，coalesce的原理是将多个partition直接合并成一个partition，是一个窄依赖，不会发生suffle，如果传入的参数大于当前的分区数，那么分区数不会发生变化。","categories":[{"name":"Spark应用","slug":"Spark应用","permalink":"https://shang.at/categories/Spark应用/"}],"tags":[{"name":"Spark大数据集防止driver OOM","slug":"Spark大数据集防止driver-OOM","permalink":"https://shang.at/tags/Spark大数据集防止driver-OOM/"}]},{"title":"Python学习-类的特殊方法","slug":"Python学习-类的特殊方法","date":"2020-07-01T02:22:24.000Z","updated":"2020-07-01T06:03:54.974Z","comments":true,"path":"post/Python学习-类的特殊方法/","link":"","permalink":"https://shang.at/post/Python学习-类的特殊方法/","excerpt":"简介：通过定义具有特殊名称的方法，类可以实现某些通过特殊语法调用的操作（例如算术运算或下标和切片）。这是Python运算符重载的方法，它允许类针对语言运算符定义自己的行为。例如，如果一个类定义了一个名为__getitem__()的方法，并且x是该类的实例，则x[i]大致等效于type(x).__getitem__(x, i)。除非另有说明，否则当未定义适当的方法（通常为AttributeError或TypeError）时，尝试执行操作会引发异常。 将特殊方法设置为None表示相应的操作不可用。例如，如果一个类将__iter__()设置为None，则该类不可迭代，因此在其实例上调用iter()会引发TypeError（而不会回到__getitem__()）。 默认情况下，我们自定义的类，它的这些特殊方法都有默认的实现。","text":"简介：通过定义具有特殊名称的方法，类可以实现某些通过特殊语法调用的操作（例如算术运算或下标和切片）。这是Python运算符重载的方法，它允许类针对语言运算符定义自己的行为。例如，如果一个类定义了一个名为__getitem__()的方法，并且x是该类的实例，则x[i]大致等效于type(x).__getitem__(x, i)。除非另有说明，否则当未定义适当的方法（通常为AttributeError或TypeError）时，尝试执行操作会引发异常。 将特殊方法设置为None表示相应的操作不可用。例如，如果一个类将__iter__()设置为None，则该类不可迭代，因此在其实例上调用iter()会引发TypeError（而不会回到__getitem__()）。 默认情况下，我们自定义的类，它的这些特殊方法都有默认的实现。 参考官网 https://docs.python.org/zh-cn/3/reference/datamodel.html Basic customization__new____init____del____repr__Customizing attribute accessCustomizing module attribute accessImplementing DescriptorsCustomizing class creationMetaclassesCustomizing instance and subclass checksEmulating generic typesEmulating callable objectsEmulating container typesEmulating numeric typesWith Statement Context ManagersSpecial method lookup","categories":[{"name":"Python","slug":"Python","permalink":"https://shang.at/categories/Python/"}],"tags":[{"name":"Python类的特殊方法","slug":"Python类的特殊方法","permalink":"https://shang.at/tags/Python类的特殊方法/"}]},{"title":"Java学习-JMH","slug":"Java学习-JMH","date":"2020-06-30T15:13:41.000Z","updated":"2020-07-05T01:38:55.632Z","comments":true,"path":"post/Java学习-JMH/","link":"","permalink":"https://shang.at/post/Java学习-JMH/","excerpt":"简介：本节学习Java准测试工具套件-JMH","text":"简介：本节学习Java准测试工具套件-JMH 官网：http://openjdk.java.net/projects/code-tools/jmh/ 官方样例：http://hg.openjdk.java.net/code-tools/jmh/file/tip/jmh-samples/src/main/java/org/openjdk/jmh/samples/ 博客：https://www.xncoding.com/2018/01/07/java/jmh.html","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"JAVA-JMH","slug":"JAVA-JMH","permalink":"https://shang.at/tags/JAVA-JMH/"}]},{"title":"算法-动态规划","slug":"算法-动态规划","date":"2020-06-29T17:54:22.000Z","updated":"2020-07-13T23:48:37.398Z","comments":true,"path":"post/算法-动态规划/","link":"","permalink":"https://shang.at/post/算法-动态规划/","excerpt":"简介：动态规划是高级的算法思想，本节主要记录DP的分析思路","text":"简介：动态规划是高级的算法思想，本节主要记录DP的分析思路 动态规划 其本质是动态递推 避免人肉递归。可以尝试画出递归树 找到最近最简方法，将其拆解成可重复解决的问题 如何找到最近最简方法：数学归纳法思维 如何区分DP问题：DP一般会被用来求解最值问题 DP与递归和分治的联系 DP与 递归或者分治没有根本上的却别 共性：找到重复子问题 差异性：DP有最优子结构，中途可以淘汰次优解 实战例题509. 斐波那契数 傻递归 - 自顶向下 画出递归树 时间复杂度：每计算一个节点，需要计算其余的两个节点，以此类推下去，这是一个二叉树的结构，所以它的时间复杂度是O($2^n$)的 优化： 加入备忘录 在傻递归的基础上，加入一个缓存，以达到避免重复子问题的重复计算的问题 使用DP的思维解决 - 自底向上 DP三步曲 找到重复子问题 根据数学归纳法：要计算第n个斐波那契数，那么我们只需要计算第n-1和n-2个斐波那契数就可以了 状态定义 - 且找到base case 假设使用a[i]表示第i个斐波那契数，那么f[i] = f[i-1]+f[i-2] 且f[0]=0,f[1]=1 DP方程 f(n)=\\left\\{ \\begin{aligned} 0 &,& n=0 \\\\ 1 &,& n=1 \\\\ f(n-1)+f(n-2) &,& n>1 \\end{aligned} \\right. 70. 爬楼梯 考虑下变体： 不止可以上1阶或2阶：可以上1、2、3阶等 可以上1 2 3阶，且相邻的两个步伐不能相同，该如何设计 本体： f(n)=\\left\\{ \\begin{aligned} 1 &,& n=1 \\\\ 2 &,& n=2 \\\\ f(n-1)+f(n-2) &,& n>2 \\end{aligned} \\right.变体1： f(n)=\\left\\{ \\begin{aligned} 1 &,& n=1 \\\\ 2 &,& n=2 \\\\ 4 &,& n=3 \\\\ f(n-1)+f(n-2)+f(n-3) &,& n>3 \\end{aligned} \\right.变体2： 定义dp[0…2][i]， ​ dp[0][i]表示到达i最后一次走了1步； ​ dp[1][i]表示到达i最后一次走了2步； ​ dp[2][i]表示到达i最后一次走了3步 那么 ​ dp[0][i]=dp[1][i-1]+dp[2][i-1]；到当前台阶走了1步，那么前面只能再选最后一次走了2步和3步的 ​ dp[1][i]=dp[0][i-2]+dp[2][i-2]；到当前台阶走了2步，那么前面只能再选最后一次走了1步和3步 ​ dp[2][i]=dp[0][i-3]+dp[1][i-3] 到当前台阶走了3步，那么前面只能再选最后一次走了1步和2步的 结果为：dp[0][-1]+dp[1][-1]+dp[2][-1]，即到达n的所有步伐的总和 1234567891011121314151617181920212223def changeClimbStairs(self, n: int) -&gt; int: if n == 1: return 1 if n == 2: return 1 if n == 3: return 3 # dp[0][i] 最后一次走了1步到达 i # dp[1][i] 最后一次走了2步到达 i # dp[2][i] 最后一次走了3步到达 i dp = [ [1, 0, 1] + [0] * (n - 3), # 最后一次走1步 [0, 1, 1] + [0] * (n - 3), # 最后一次走2步 [0, 0, 1] + [0] * (n - 3) # 最后一次走3步 ] for i in range(3, n): # 到当前台阶走了1步，那么前面只能再选最后一次走了2步和3步的 dp[0][i] = dp[1][i - 1] + dp[2][i - 1] # 到当前台阶走了2步，那么前面只能再选最后一次走了1步和3步的 dp[1][i] = dp[0][i - 2] + dp[2][i - 2] # 到当前台阶走了3步，那么前面只能再选最后一次走了1步和2步的 dp[2][i] = dp[0][i - 3] + dp[1][i - 3] return dp[0][-1] + dp[1][-1] + dp[2][-1] 322. 零钱兑换 略过递归方案，直接上DP DP问题三步曲 找到重复子问题 根据数学归纳法：已知某金额所需的最少硬币数，可得其他金额的所需最少硬币数 状态定义 假设dp[i]表示，金额为i时需要的最少硬币数，那么因为有coins个硬币可选，所以dp[i]=min(dp[i-k] for k in coins) + 1，加1表示要选择一个面值为k的硬币 dp初始值为mount+1，因为金额为amout，所需硬币最多为amout个1元硬币，长度为amout+1 dp[0]=0，表示当金额为0时，需要的硬币数也为0 最终结果：dp[amout] DP方程 f(n) = \\left\\{ \\begin{aligned} 0 &,& n=0 \\\\ 1 &,& n=1 \\\\ min(f(n-k))+1 &,& k \\in coins \\\\ \\end{aligned} \\right.123456789101112class Solution: def coinChange(self, coins: List[int], amount: int) -&gt; int: dp = [amount + 1] * (amount + 1) dp[0] = 0 for i in range(1, amount + 1): for coin in coins: if coin &lt;= i: dp[i] = min(dp[i], dp[i - coin] + 1) return dp[amount] if dp[amount] &lt; amount + 1 else -1 62. 不同路径 直接DP三步曲 找到重复子问题 要求从位置(i,j)到END的不同路径数，由于从某个位置出发只能向右或向下走，如果我知道了从(i+1, j)和从(i,j+1)到END的路径数，那么我就能得到path_{i,j}，path_{i,j}=path_{i+1,j}+path_{i,j+1} 状态定义 定义dp[m][n]数组，表示棋盘，每个元素表示从该位置出发，到END的不同路径数 dp[][n]=1，右边界全为1，因为右边界的位置到END，路径数都是1 dp[m][]=1，下边界全为1，因为下边界的位置到END，路径数都是1 遍历方向：从END位置开始向上遍历 最终结果：dp[0][0] DP方程 f(x,y) = \\left\\{ \\begin{aligned} 1 &,& x=m\\\\ 1 &,& y=n\\\\ f(x+1, y)+f(x, y+1) &,& 0 \\leq x < m \\& 0 \\leq y < n \\end{aligned} \\right.123456789101112131415161718192021class Solution: def uniquePaths(self, m: int, n: int) -&gt; int: if n == 1: return 1 # 初始化dp数组，左边界和右边界都初始化为1 dp = [[1 if j == m - 1 or i == n - 1 else 0 for j in range(m)] for i in range(n)] for i in range(n - 2, -1, -1): for j in range(m - 2, -1, -1): dp[i][j] = dp[i + 1][j] + dp[i][j + 1] return dp[0][0] # 空间压缩：我们只需要一维的数组就可以存下路径的变化 def uniquePaths(self, m: int, n: int) -&gt; int: if n == 1: return 1 # 初始化dp数组为 1 dp = [1] * m for i in range(n - 2, -1, -1): for j in range(m - 2, -1, -1): dp[j] = dp[j] + dp[j + 1] return dp[0] 63. 不同路径 II 直接DP三步曲 找到重复子问题 要求从位置(i,j)到END的不同路径数，由于从某个位置出发只能向右或向下走，如果我知道了从(i+1, j)和从(i,j+1)到END的路径数，那么我就能得到path_{i,j}，path_{i,j}=path_{i+1,j}+path_{i,j+1} 状态定义 定义dp[m][n]数组，表示棋盘，每个元素表示从该位置出发，到END的不同路径数 dp[][n]=1，右边界最后一个障碍物之后全为1，最后一个障碍物及之前的位置全为0，因为右边界一旦出现了障碍物，那么在这之前的位置的路径就都被截断了 dp[m][]=1，下边界最后一个障碍物之后全为1，最后一个障碍物及之前的位置全为0，因为下边界一旦出现了障碍物，那么在这之前的位置的路径就都被截断了 需要注意的是：棋盘上为1的位置不能走，那个位置的路径数为0 遍历方向：从END位置开始遍历 最终结果：dp[0][0] DP方程 f(x,y) = \\left\\{ \\begin{aligned} 0 &,& x=m\\&x\\leq lastcol(lastrow表示下边界最后一个障碍物的位置)\\\\ 0 &,& y=n\\&y\\leq lastrow(lastrow表示右边界最后一个障碍物的位置)\\\\ 1 &,& x=m\\&x>lastcol(lastrow表示下边界最后一个障碍物的位置)\\\\ 1 &,& y=n\\&y>lastrow(lastrow表示右边界最后一个障碍物的位置)\\\\ 0 &,& obstacleGrid(x, y)=1\\\\ f(x-1, y)+f(x, y-1) &,& 0 \\leq x < m \\& 0 \\leq y < n \\end{aligned} \\right.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687class Solution: def uniquePathsWithObstacles(self, obstacleGrid: List[List[int]]) -&gt; int: m, n = len(obstacleGrid[0]), len(obstacleGrid) # if obstacleGrid[-1][-1] == 1 or obstacleGrid[0][0] == 1: return 0 # 起点和终点为1，直接返回0 # if n == 1: # 只有一行 # if sum(obstacleGrid[0]) &gt;= 1: # 行出现了1 # return 0 # else: # return 1 # if m == 1: # 只有一列 # if sum(list(chain(*obstacleGrid))) &gt;= 1: # 列出现了1 # return 0 # else: # return 1 # 如果 最后一行或最后一列出现了1，那么1出现的位置及之前的位置和1没有区别 last_row, last_col = -1, -1 # 标记最后一行(列)出现障碍的位置 for i in range(n - 1, -1, -1): if obstacleGrid[i][-1] == 1: last_row = i break for i in range(m - 1, -1, -1): if obstacleGrid[-1][i] == 1: last_col = i break # 根据棋盘的最后一行和最后一列初始化 dp数组 # 包含了 起点和终点为1 的情况 # 包含了 只有一行和只有一列的情况 dp = [[1 if i &gt; last_col and j == n - 1 or j &gt; last_row and i == m - 1 else 0 for i in range(m)] for j in range(n)] for i in range(n - 2, -1, -1): for j in range(m - 2, -1, -1): if obstacleGrid[i][j] == 1: dp[i][j] = 0 else: dp[i][j] = dp[i + 1][j] + dp[i][j + 1] return dp[0][0] def uniquePathsWithObstacles1(self, obstacleGrid: List[List[int]]) -&gt; int: if not obstacleGrid or not obstacleGrid[0]: return 0 m, n = len(obstacleGrid), len(obstacleGrid[0]) # 状态定义 - base case # dp[i][j] 表示从i,j到达finish位置的路径数 # 初始化为0 dp = [[0] * n for _ in range(m)] # 特殊考虑右边界和下边界:找到右边界和下边界最后一个障碍物的位置 # 从后往前找，不用从前往后 全部遍历 last_col, last_row = -1, -1 # 遍历状态 for i in range(m-1, -1, -1): for j in range(n-1, -1, -1): if obstacleGrid[i][j] == 1: # 遇到障碍物 dp[i][j] = 0 # 在遍历状态的过程中记录 右边界和下边界 最后一个障碍物的位置 last_col = max(last_col, j) if i==m-1 else last_col last_row = max(last_row, i) if j==n-1 else last_row else: # 正常的格子 if i&lt;m-1 and j&lt;n-1: # 不在右边界和下边界 dp[i][j] = dp[i+1][j] + dp[i][j+1] elif i==m-1: # 下边界:如果当前位置是在下边界的最后一个障碍物之后，那么赋值1，否则赋值0 dp[i][j] = 1 if j&gt;last_col else 0 elif j==n-1: # 右边界:如果当前位置是在右边界的最后一个障碍物之后，那么赋值1，否则赋值0 dp[i][j] = 1 if i&gt;last_row else 0 # 答案为dp[0][0] # print(last_col, last_row) return dp[0][0] # dp空间压缩：实际上我们在进行递推的时候，只需要一维数组就可以 # 为什么能做到这样的空间压缩？ # 当开始遍历上一层(i--)，dp[j]的值是保留了下一层的值，所以dp[j] += dp[j+1]是相当于之前的 dp[i][j] = dp[i+1][j] + dp[i][j+1] ~~~！！！！ # 同时也可以避免右边界和下边界的 特殊情况，因为一维的情况下 相当于是已经自动加了限制 def uniquePathsWithObstacles(self, obstacleGrid: List[List[int]]) -&gt; int: m, n = len(obstacleGrid[0]), len(obstacleGrid) dp = [0] * m dp[m - 1] = 1 for i in range(n - 1, -1, -1): for j in range(m - 1, -1, -1): if obstacleGrid[i][j] == 1: # 当前位置为障碍物 dp[j] = 0 elif j &lt; m - 1: # 因为要使用下一个坐标，这里要检测坐标的合法性，防止指针越界 dp[j] += dp[j + 1] return dp[0] 1143. 最长公共子序列 思维：DP最终会归结到一个状态数组中，所以拿到一个这样的题目后，就往一维状态数组上靠拢，一维搞不定，就尝试用二维数组，再不行就三维…然后依靠数学归纳法进行推导，看是否能根据已知内容的位置 推导出当前位置的值 直接上DP三步曲 找到重复子问题 abcde和abc的问题可以由下列的二维表格描述，如果要求某个位置的值，只需要知道它之前的某些位置即可 - a b c d e a LCS(a,a)=1 LCS(a,ab)=1 LCS(a,abc)=1 LCS(a,aabcd)=1 LCS(a,abcde)=1 c LCS(ac,a)=1 LCS(ac,ab)=1 LCS(ac,abc)=2 2 2 e 1 1 2 2 LCS(ace,abcde)=3 定义状态 dp[i][j]，i表示第一个字符串的位置编号，j表示第二个字符串的位置编号，整体表示两个子串的最长公共子序列的长度 dp长度初始化为text1.length+1*text2.length+1，因为至少要包含没有字符的情况 初始值：dp[0][0]=0 遍历方向：从前往后。因为base case在前部 最终结果：dp[text1.length][text2.length] DP方程 f(x, y) = \\left\\{ \\begin{aligned} 0 &,& x=0\\&y=0 \\\\ max(f(x-1, y), f(x, y-1)) &,& text1(x)!=text2(y) \\\\ f(x-1, y-1)+1 &,& text1(x)=text2(y) \\\\ \\end{aligned} \\right.1234567891011121314151617181920212223242526272829303132333435class Solution: def longestCommonSubsequence1(self, text1: str, text2: str) -&gt; int: m, n = len(text1), len(text2) dp = [[0 for _ in range(m + 1)] for _ in range(n + 1)] for i in range(1, n + 1): for j in range(1, m + 1): if text1[j - 1] == text2[i - 1]: # 当前位置的两个字符一样，取对角的值再加1(加上自身) dp[i][j] = dp[i - 1][j - 1] + 1 else: # 当前位置的两个字符不一样，取两个字符对应的最大值 dp[i][j] = max(dp[i - 1][j], dp[i][j - 1]) return dp[n][m] # dp空间压缩：发现求当前位置的值，只需要left和last_line的值 def longestCommonSubsequence(self, text1: str, text2: str) -&gt; int: m, n = len(text1), len(text2) # 当前位置，当前位置的左一位置 curr, left = 0, 0 # 上一行的值 last_line = [0] * (m + 1) for i in range(1, n + 1): for j in range(1, m + 1): if text1[j - 1] == text2[i - 1]: # 当前位置的两个字符一样，取对角的值再加1(加上自身) curr = last_line[j - 1] + 1 else: # 当前位置的两个字符不一样，取两个字符对应的最大值 curr = max(last_line[j], left) # 根据当前计算的结果 更新上一行，遍历到下一行的时候，正好可以用 if j == m: # 换行：可以更新当前位置的up位置元素，且把left重置为0 last_line[j - 1], last_line[j], left = left, curr, 0 else: # 不换行：不能更新up位置，因为在同一行中遍历的时候，要使用原始的值 last_line[j - 1], left = left, curr return curr 120. 三角形最小路径和 直接DP三步曲 找到重复子问题 要想求得到达当前位置的最小路径和，那么我只需要知道到达当前位置的up和up_left最小路径和即可 状态定义 dp[i][j]，i和j分别表示矩阵的坐标，整体表示到达该坐标的最小路径和 dp[0][0]为矩阵的第一个元素值 最终结果：min(dp[-1]) DP方程 f(x, y) = \\left\\{ \\begin{aligned} matrix(0, 0) &, & x=0\\&y=0 \\\\ min(f(x-1, y), f(x-1, y-1)) + matrix(x,y) &,& 0\\leq x \\& 0\\leq y \\end{aligned} \\right.123456789101112131415161718192021222324252627class Solution: def minimumTotal1(self, triangle: List[List[int]]) -&gt; int: m, n = len(triangle[-1]), len(triangle) dp = [[0 for _ in row] for row in triangle] dp[0][0] = triangle[0][0] for i in range(1, n): for j, val in enumerate(triangle[i]): if j == 0: dp[i][j] = dp[i - 1][j] + val elif j == len(triangle[i]) - 1: dp[i][j] = dp[i - 1][j - 1] + val else: dp[i][j] = min(dp[i - 1][j], dp[i - 1][j - 1]) + val return min(dp[-1]) # dp空间压缩 def minimumTotal(self, triangle: List[List[int]]) -&gt; int: m, n = len(triangle[-1]), len(triangle) dp = triangle for i in range(1, n): for j, val in enumerate(triangle[i]): if j == 0: dp[i][j] = dp[i - 1][j] + val elif j == len(triangle[i]) - 1: dp[i][j] = dp[i - 1][j - 1] + val else: dp[i][j] = min(dp[i - 1][j], dp[i - 1][j - 1]) + val return min(dp[-1]) 221. 最大正方形 DP三步曲 找到重复子问题 以某个位置(i,j)为右下角的正方形的边长，可以根据当前位置的up、left、left_up三个位置确定出来 状态定义 dp[i][j]表示以(i,j)为右下角的正方形的边长，那么如果(i,j)为0，dp[i][j]=0；否则dp[i][j]=min(up,left,up_left)+1 加1的目的是，至少包含它本身 DP方程 123456789101112131415class Solution: def maximalSquare(self, matrix: List[List[str]]) -&gt; int: if not matrix or not matrix[0]: return 0 m, n = len(matrix[0]), len(matrix) dp = [[0 for _ in range(m)] for _ in range(n)] max_edge = dp[0][0] = int(matrix[0][0]) for i in range(0, n): for j in range(0, m): if i == 0 or j == 0: dp[i][j] = 0 if matrix[i][j] == '0' else 1 else: dp[i][j] = 0 if matrix[i][j] == '0' else min(dp[i - 1][j], dp[i][j - 1], dp[i - 1][j - 1]) + 1 max_edge = max(dp[i][j], max_edge) return max_edge * max_edge 312. 戳气球 DP三步曲 找到重复子问题 当只有一个气球时，只需要戳一次即可 当有两个气球时，也只需要戳一次 当有三个气球时，那么肯定要先戳中间那一个 当有四个气球时，那么就要 状态定义 气球的个数为n 假设dp[i][j]表示序号在(i,j)之间的获得的硬币最大值，那么我们需要通过遍历(i,j)之间的所有序号，求出dp[i][k]和dp[k][j]，因为dp[i][j]=dp[i][k]+dp[k][j]+k， 最终结果：dp[0][n+1] DP方程 198. 打家劫舍 略过递归方案，直接上DP DP问题三步曲 找到重复子问题 根据数学归纳法：如果已知小偷在第i-1个房间对应的最高金额，那么小偷在第i个房间的对应的最高金额有以下两种情况 小偷偷了第i-1房间：那么小偷势必不能偷第i个房间，那么这个时候小偷偷的金额应该继承上一个值 小偷没有偷第i-1房间：那么小偷一定要偷第i个房间(因为是求的最大值)，这个时候小偷偷的金额应该是上一个值加上第i个房间的金额 状态定义 那么我们怎么表示某一个房间偷与不偷呢？此时我们可以考虑在一维的基础之上加一维附加这个状态，用dp[i][0]表示没有偷第i个房间，dp[i][1]表示偷了第i个房间，nums[i]表示第i间房的金额，那么 计算没有偷i房的时候在i的最大金额：dp[i][0]=max(dp[i-1][0], dp[i-1][1]) 计算偷i房的时候在i的最大金额：dp[i][1]=dp[i-1][0]+nums[i] dp的初始值为全为0，长度为n+1， 第0个元素表示房间数为0时，能偷到的金额为0， 第n个元素表示到达最后一个房间，能偷到的金额为多少 遍历从1开始 最终的结果：max(dp[n][0], dp[n][1]) DP方程定义 f(n,0) =\\left\\{ \\begin{aligned} 0 &,& n=0 \\\\ max(f(n-1, 0), f(n-1, 1)) &,& n>1 \\end{aligned} \\right. f(n,1) =\\left\\{ \\begin{aligned} 0 &,& n=0 \\\\ f(n-1, 0)+nums(n) &,& n>1 \\end{aligned} \\right. res = max(f(n, 0), f(n, 1))1234567891011class Solution: def rob(self, nums: List[int]) -&gt; int: n = len(nums) # 房间个数 dp = [[0 for _ in range(2)] for _ in range(n + 1)] for i in range(1, n + 1): dp[i][0] = max(dp[i - 1][0], dp[i - 1][1]) # 不偷第i间房 dp[i][1] = dp[i - 1][0] + nums[i - 1] # 偷第i间房 return max(dp[n][0], dp[n][1]) # 返回第i间房 偷与不偷的最大金额 DP问题三步曲-version2 第一个版本中，我们考虑的时候，并不知道小偷有没有偷某个房间，所以加了一个维度表示某个房间偷与不偷的 对应的最大金额 这个版本中，我们换个思路，可以肯定的是，小偷一定会偷其中一间房，也就是说，如果我们假定某间房必偷的话，那么我们最终的结果应该是所有结果的最大值 找到重复子问题 根据数学归纳法：由于小偷不能偷连续的两间房，那么我们如果要计算第i间房最大金额，该怎么计算呢？这个时候，前一间房要么偷了，要么没偷，如果前一间房没偷，那么看前两间房偷没偷，前两间房如果没偷，那就继续往前推 状态定义-DP数组含义及base case dp[i]表示当偷到第i间房时的最大金额，不管第i间房偷与不偷， dp[i]=max(dp[i-1], dp[i-2]+nums[i]) 表示：偷与不偷i-1的时候，在第i间房的最大金额 dp初始化全为0，长度为n+2 dp[0], dp[1]表示第1间房前1间、两间的金额都为0 遍历从2开始 最终结果：max(dp) DP方程 f(n) = \\left\\{ \\begin{aligned} 0 &,& n=-1 \\\\ 0 &,& n=0 \\\\ max(f(n-1), f(n-2)+nums(n)) &,& n>0 \\end{aligned} \\right.12345678910111213141516171819202122232425class Solution: def robnn(self, nums: List[int]) -&gt; int: n = len(nums) dp = [0] * (n + 2) for i in range(2, n + 2): dp[i] = max(dp[i - 1], dp[i - 2] + nums[i - 2]) return max(dp) # ====&gt; 优化，去掉最后的取最大值的函数，且降低空间复杂度 def rob(self, nums: List[int]) -&gt; int: n = len(nums) i_2, i_1, res = 0, 0, 0 # 前两间房的最大金额，前一间房的最大金额，当前房间的最大金额 for i in range(n): # dp[i] = max(dp[i - 1], dp[i - 2] + nums[i - 2]) # 更新偷到当前房间的最大金额 res = max(i_1, i_2 + nums[i]) # 上一个res变成前一间房 # 上一个i_1变成前两间房 i_1, i_2 = res, i_1 return res 44. 通配符匹配 DP三步曲 找到重复子问题 要想知道p[1…j]是否匹配s[1…i]，那么需要知道p[0…j-1]与s[0…i-1]是否匹配 状态定义-DP数组含义及base case dp[i][j]表示p[1…j]是否匹配s[1…i] dp[0][0]=True 如果p和s都是空字符串，那么两者是匹配的，直接返回True 如果s为空，p不为空且只包含*这一种字符，则两者匹配，直接返回True 如果p为空，s不为空，则两者不匹配，直接返回False 如果s为空，p不为空且不止包含*这一种字符，则两者不匹配，直接返回False 状态转移方程 dp[i][j] = \\left\\{ \\begin{aligned} dp[i-1][j-1] &,& s[i]=p[j] \\\\ dp[i-1][j] ∨ dp[i][j-1] &,& p[j]='*' \\\\ dp[i-1][j-1] &,& p[j]='?' \\end{aligned} \\right. 1234567891011121314151617181920class Solution: def isMatch(self, s: str, p: str) -&gt; bool: if not p and not s: return True if not p and s: return False if not s and ('*' not in set(p) or len(set(p))&gt;1): return False dp = [[False]*(len(p)+1) for _ in range(len(s)+1)] dp[0][0] = True for j in range(1, len(p)+1): if p[j-1]=='*': dp[0][j] = dp[0][j-1] else: break for i in range(1, len(s)+1): for j in range(1, len(p)+1): if s[i-1]==p[j-1] or p[j-1]=='?': dp[i][j] = dp[i-1][j-1] elif p[j-1]=='*': dp[i][j] = dp[i][j-1] | dp[i-1][j] print(dp) return dp[len(s)][len(p)]","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"}],"tags":[{"name":"动态规划","slug":"动态规划","permalink":"https://shang.at/tags/动态规划/"}]},{"title":"Python学习-python3.6-dict有序且效率更高","slug":"Python学习-python3-6-dict有序且效率更高","date":"2020-06-28T23:22:42.000Z","updated":"2020-07-02T15:49:24.809Z","comments":true,"path":"post/Python学习-python3-6-dict有序且效率更高/","link":"","permalink":"https://shang.at/post/Python学习-python3-6-dict有序且效率更高/","excerpt":"","text":"https://www.cnblogs.com/xieqiankun/p/python_dict.html","categories":[{"name":"Python","slug":"Python","permalink":"https://shang.at/categories/Python/"}],"tags":[{"name":"python的dict","slug":"python的dict","permalink":"https://shang.at/tags/python的dict/"}]},{"title":"Hadoop学习","slug":"Hadoop学习","date":"2020-06-26T16:34:48.000Z","updated":"2020-07-15T09:10:12.865Z","comments":true,"path":"post/Hadoop学习/","link":"","permalink":"https://shang.at/post/Hadoop学习/","excerpt":"简介：","text":"简介： 大数据 分布式数据存储 ​ 数据一致性 - CAP - poxes 主从 ​ 单点故障 - HA ​ 内存压力 - 分片管理 分布式计算 - 计算向数据移动 ​ mapreduce 数据以一条记录为单位及经过map方法映射成KV，相同的K为一组，这一组数据条用一次reduce方法，在方法内迭代计算一组数据。 迭代器模式，数据集一般是用迭代计算的方式 ​ 为什么要有split？split只是一个逻辑上的概念。 ​ 与数据物理存储上的block解耦(软件工程上：加一层解耦)，默认情况下，split等于block，但是也可以小于block，也可能大于block 集群内部的通信采用RPC的方式，同时只会发生client向server发送请求，server处理完之后返回client响应结果，sever不会主动向client发送消息","categories":[{"name":"Hadoop学习","slug":"Hadoop学习","permalink":"https://shang.at/categories/Hadoop学习/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://shang.at/tags/Hadoop/"}]},{"title":"Hadoop学习-Yarn-Scheduler","slug":"Hadoop学习-Yarn-Scheduler","date":"2020-06-25T06:24:45.000Z","updated":"2020-06-25T06:25:31.320Z","comments":true,"path":"post/Hadoop学习-Yarn-Scheduler/","link":"","permalink":"https://shang.at/post/Hadoop学习-Yarn-Scheduler/","excerpt":"","text":"","categories":[{"name":"Hadoop学习","slug":"Hadoop学习","permalink":"https://shang.at/categories/Hadoop学习/"}],"tags":[{"name":"Yarn-Scheduler","slug":"Yarn-Scheduler","permalink":"https://shang.at/tags/Yarn-Scheduler/"}]},{"title":"Hadoop学习-Yarn-Container","slug":"Hadoop学习-Yarn-Container","date":"2020-06-25T06:24:29.000Z","updated":"2020-06-25T06:25:23.223Z","comments":true,"path":"post/Hadoop学习-Yarn-Container/","link":"","permalink":"https://shang.at/post/Hadoop学习-Yarn-Container/","excerpt":"","text":"","categories":[{"name":"Hadoop学习","slug":"Hadoop学习","permalink":"https://shang.at/categories/Hadoop学习/"}],"tags":[{"name":"Yarn-Container","slug":"Yarn-Container","permalink":"https://shang.at/tags/Yarn-Container/"}]},{"title":"Hadoop学习-如何实现一个在Yarn上的Application","slug":"Hadoop学习-如何实现一个在Yarn上的Application","date":"2020-06-25T01:27:51.000Z","updated":"2020-06-25T01:28:09.068Z","comments":true,"path":"post/Hadoop学习-如何实现一个在Yarn上的Application/","link":"","permalink":"https://shang.at/post/Hadoop学习-如何实现一个在Yarn上的Application/","excerpt":"","text":"","categories":[{"name":"Hadoop学习","slug":"Hadoop学习","permalink":"https://shang.at/categories/Hadoop学习/"}],"tags":[{"name":"实现一个在Yarn上的Application","slug":"实现一个在Yarn上的Application","permalink":"https://shang.at/tags/实现一个在Yarn上的Application/"}]},{"title":"Hadoop学习-Mapreduce编程模式","slug":"Hadoop学习-Mapreduce编程模式","date":"2020-06-25T01:26:50.000Z","updated":"2020-12-18T10:02:58.291Z","comments":true,"path":"post/Hadoop学习-Mapreduce编程模式/","link":"","permalink":"https://shang.at/post/Hadoop学习-Mapreduce编程模式/","excerpt":"","text":"Hadoop-2.7.3","categories":[{"name":"Hadoop学习","slug":"Hadoop学习","permalink":"https://shang.at/categories/Hadoop学习/"}],"tags":[{"name":"Mapreduce编程模式","slug":"Mapreduce编程模式","permalink":"https://shang.at/tags/Mapreduce编程模式/"}]},{"title":"Hive学习-安装","slug":"Hive学习-安装","date":"2020-06-24T07:41:54.000Z","updated":"2020-07-04T14:15:46.359Z","comments":true,"path":"post/Hive学习-安装/","link":"","permalink":"https://shang.at/post/Hive学习-安装/","excerpt":"","text":"配置过程 mysql 安装 配置可以远程连接 jdbc driver hive使用的jdbc drive要与mysql的版本匹配 hive配置 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556&lt;!-- hive-site.xml --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://hostmachine:3306/metastore?createDatabaseIfNotExist=true&lt;/value&gt; &lt;description&gt;the URL of the MySQL database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;hive1234&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;datanucleus.autoCreateSchema&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;datanucleus.fixedDatastore&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;datanucleus.autoStartMechanism&lt;/name&gt; &lt;value&gt;SchemaTable&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置数据存放在hdfs上的路径 --&gt; &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive/warehouse&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置metastore service 的节点 --&gt; &lt;property&gt; &lt;name&gt;hive.metastore.uris&lt;/name&gt; &lt;value&gt;thrift://node2:9083&lt;/value&gt; &lt;description&gt;IP address (or fully-qualified domain name) and port of the metastore host&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.schema.verification&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 启动过程 首先启动hdfs：start-dfs.sh 初始化metestore：schematool --dbType mysql --initSchema 启动metastore service：hive --service metastore 启动hive：hive https://www.jianshu.com/p/6108e0aed204","categories":[{"name":"Hive学习","slug":"Hive学习","permalink":"https://shang.at/categories/Hive学习/"}],"tags":[{"name":"Hive安装","slug":"Hive安装","permalink":"https://shang.at/tags/Hive安装/"}]},{"title":"数据库-mysql-环境配置","slug":"数据库-mysql-环境配置","date":"2020-06-24T06:16:33.000Z","updated":"2020-12-07T02:14:25.434Z","comments":true,"path":"post/数据库-mysql-环境配置/","link":"","permalink":"https://shang.at/post/数据库-mysql-环境配置/","excerpt":"","text":"platform：MAC 安装1234567891011121314brew istall mysqlWe've installed your MySQL database without a root password. To secure it run: mysql_secure_installationMySQL is configured to only allow connections from localhost by defaultTo connect run: mysql -urootTo have launchd start mysql now and restart at login: brew services start mysqlOr, if you don't want/need a background service you can just run: mysql.server start 新安装的mysql，需要重置密码： The initial root account may or may not have a password. Choose whichever of the following procedures applies: If the root account exists with an initial random password that has been expired, connect to the server as root using that password, then choose a new password. This is the case if the data directory was initialized using mysqld —initialize, either manually or using an installer that does not give you the option of specifying a password during the install operation. Because the password exists, you must use it to connect to the server. But because the password is expired, you cannot use the account for any purpose other than to choose a new password, until you do choose one. If you do not know the initial random password, look in the server error log. Connect to the server as root using the password: 12shell&gt; mysql -u root -p Enter password: (enter the random root password here) Choose a new password to replace the random password: mysql&gt; ALTER USER ‘root’@’localhost’ IDENTIFIED BY ‘root-password’; If the root account exists but has no password, connect to the server as root using no password, then assign a password. This is the case if you initialized the data directory using mysqld —initialize-insecure. Connect to the server as root using no password: 1shell&gt; mysql -u root --skip-password Assign a password: 1mysql&gt; ALTER USER 'root'@'localhost' IDENTIFIED BY 'root-password'; After assigning the root account a password, you must supply that password whenever you connect to the server using the account. For example, to connect to the server using the mysql client, use this command: 12shell&gt; mysql -u root -p Enter password: (enter root password here) To shut down the server with mysqladmin, use this command: 12shell&gt; mysqladmin -u root -p shutdown Enter password: (enter root password here) 设置远程访问设置my.cnf使用brew 安装的mysql，my.cnf文件在/usr/local/etc/my.cnf，修改bind-address为0.0.0.0，然后重启brew services restart mysql 创建用户，赋予权限1234567891011121314# 登录mysqlmysql -u root -p 123456# 创建新用户create user 'hive' identified by 'hive1234';# 授权grant all privileges on *.* to 'hive'@'%' with grant option;# *.* 前边的*号指的是数据库，后面的*号指的是表，*.*的意思就是任意数据库下的任意表# 'root'@'%'，'root'用户名，'%'任意的主机名。# 这条配置信息就是说，允许任意节点以root身份登录，并且可以访问mysql里的任意库下的任意表# 刷新flush privileges; 至此，便可以在其他host上访问mysql服务了。 其他问题 jar记得更新 server 时区 如果不是新安装的mysql，那么可以按照以下的方式进行修改密码：cd /usr/local/mysql/bin sudo ./mysqld_safe —skip-grant-tables &amp; 然后开启另一个终端tab，执行mysql -uroot -p，要求输入密码的时候直接回车即可登录成功 use mysql;update user set authentication_string=’’ where user=’root’ flush privileges; 最后需要重新退出登录，然后正常重启mysql server(—skip-grant-tables模式无法修改密码)，重新登录 修改密码 use mysql; ALTER user ‘root’@’localhost’ IDENTIFIED BY ‘!Qwer1234’; flush privileges;","categories":[{"name":"数据库","slug":"数据库","permalink":"https://shang.at/categories/数据库/"}],"tags":[{"name":"mysql环境配置","slug":"mysql环境配置","permalink":"https://shang.at/tags/mysql环境配置/"}]},{"title":"Hadoop学习-Shell脚本学习","slug":"Hadoop学习-Shell脚本学习","date":"2020-06-24T05:28:09.000Z","updated":"2020-06-24T05:28:38.353Z","comments":true,"path":"post/Hadoop学习-Shell脚本学习/","link":"","permalink":"https://shang.at/post/Hadoop学习-Shell脚本学习/","excerpt":"","text":"","categories":[{"name":"Hadoop学习","slug":"Hadoop学习","permalink":"https://shang.at/categories/Hadoop学习/"}],"tags":[{"name":"Hadoop-Shell","slug":"Hadoop-Shell","permalink":"https://shang.at/tags/Hadoop-Shell/"}]},{"title":"工具使用-iterm2","slug":"工具使用-iterm2","date":"2020-06-23T10:30:55.000Z","updated":"2020-06-23T10:32:31.790Z","comments":true,"path":"post/工具使用-iterm2/","link":"","permalink":"https://shang.at/post/工具使用-iterm2/","excerpt":"","text":"标签12345新建标签：command + t关闭标签：command + w切换标签：command + 数字 command + 左右方向键切换全屏：command + enter查找：command + f 分屏12345垂直分屏：command + d水平分屏：command + shift + d切换屏幕：command + option + 方向键 command + [ 或 command + ]查看历史命令：command + ;查看剪贴板历史：command + shift + h 其他1234567891011121314151617181920212223清除当前行：ctrl + u到行首：ctrl + a到行尾：ctrl + e前进后退：ctrl + f/b (相当于左右方向键)上一条命令：ctrl + p搜索命令历史：ctrl + r删除当前光标的字符：ctrl + d删除光标之前的字符：ctrl + h删除光标之前的单词：ctrl + w删除到文本末尾：ctrl + k交换光标处文本：ctrl + t清屏1：command + r清屏2：ctrl + l自带有哪些很实用的功能/快捷键⌘ + 数字在各 tab 标签直接来回切换选择即复制 + 鼠标中键粘贴，这个很实用⌘ + f 所查找的内容会被自动复制⌘ + d 横着分屏 / ⌘ + shift + d 竖着分屏⌘ + r = clear，而且只是换到新一屏，不会想 clear 一样创建一个空屏ctrl + u 清空当前行，无论光标在什么位置输入开头命令后 按 ⌘ + ; 会自动列出输入过的命令⌘ + shift + h 会列出剪切板历史可以在 Preferences &gt; keys 设置全局快捷键调出 iterm，这个也可以用过 Alfred 实现 常用的一些快捷键1234567891011121314⌘ + 1 / 2 左右 tab 之间来回切换，这个在 前面 已经介绍过了⌘← / ⌘→ 到一行命令最左边/最右边 ，这个功能同 C+a / C+e⌥← / ⌥→ 按单词前移/后移，相当与 C+f / C+b，其实这个功能在Iterm中已经预定义好了，⌥f / ⌥b，看个人习惯了好像就这几个设置方法如下当然除了这些可以自定义的也不能忘了 linux 下那些好用的组合C+a / C+e 这个几乎在哪都可以使用C+p / !! 上一条命令C+k 从光标处删至命令行尾 (本来 C+u 是删至命令行首，但iterm中是删掉整行)C+w A+d 从光标处删至字首/尾C+h C+d 删掉光标前后的自负C+y 粘贴至光标后C+r 搜索命令历史，这个较常用 选中即复制iterm2 有 2 种好用的选中即复制模式。 一种是用鼠标，在 iterm2 中，选中某个路径或者某个词汇，那么，iterm2 就自动复制了。 另一种是无鼠标模式，command+f,弹出 iterm2 的查找模式，输入要查找并复制的内容的前几个字母，确认找到的是自己的内容之后，输入 tab，查找窗口将自动变化内容，并将其复制。如果输入的是 shift+tab，则自动将查找内容的左边选中并复制。 自动完成输入打头几个字母，然后输入 command+; iterm2 将自动列出之前输入过的类似命令。 剪切历史输入 command+shift+h，iterm2 将自动列出剪切板的历史记录。如果需要将剪切板的历史记录保存到磁盘，在 Preferences &gt; General &gt; Save copy/paste history to disk 中设置。","categories":[{"name":"工具使用","slug":"工具使用","permalink":"https://shang.at/categories/工具使用/"}],"tags":[{"name":"iterm2","slug":"iterm2","permalink":"https://shang.at/tags/iterm2/"}]},{"title":"Hadoop学习-源码编译","slug":"Hadoop学习-源码编译","date":"2020-06-23T10:18:16.000Z","updated":"2020-06-23T10:19:33.849Z","comments":true,"path":"post/Hadoop学习-源码编译/","link":"","permalink":"https://shang.at/post/Hadoop学习-源码编译/","excerpt":"","text":"","categories":[{"name":"Hadoop学习","slug":"Hadoop学习","permalink":"https://shang.at/categories/Hadoop学习/"}],"tags":[{"name":"Hadoop源码编译","slug":"Hadoop源码编译","permalink":"https://shang.at/tags/Hadoop源码编译/"}]},{"title":"Hadoop学习-集群搭建","slug":"Hadoop学习-集群搭建","date":"2020-06-23T10:17:38.000Z","updated":"2020-06-26T03:45:57.288Z","comments":true,"path":"post/Hadoop学习-集群搭建/","link":"","permalink":"https://shang.at/post/Hadoop学习-集群搭建/","excerpt":"","text":"配置12345678# hadoop.sh -&gt; /etc/profile.d/export HADOOP_PREFIX=/root/hadoopexport HADOOP_YARN_HOME=$&#123;HADOOP_PREFIX&#125;export HADOOP_CONF_DIR=$&#123;HADOOP_PREFIX&#125;/etc/hadoopexport YARN_LOG_DIR=$&#123;HADOOP_YARN_HOME&#125;/logsexport YARN_IDENT_STRING=rootexport HADOOP_MAPRED_IDENT_STRING=rootexport PATH=$&#123;HADOOP_PREFIX&#125;/bin:$&#123;HADOOP_PREFIX&#125;/sbin:$&#123;PATH&#125; 12345678910111213&lt;!-- core-site.xml -&gt; path_to_hadoop/etc/hadoop/ --&gt;&lt;!-- https://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-common/core-default.xml --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://node4:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:///root/data/hadoop/tmp&lt;/value&gt; &lt;description&gt;A base for other temporary directories.&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt; 12345678910111213141516171819202122232425262728293031323334353637383940&lt;!-- hdfs-site.xml -&gt; path_to_hadoop/etc/hadoop/ --&gt;&lt;!-- https://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:///root/data/hadoop/namenode&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:///root/data/hadoop/datanode&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;!-- config secondary namenode --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;node3:50090&lt;/value&gt; &lt;/property&gt; &lt;!-- enable webhdfs --&gt; &lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- disable permissions; only for development, of course --&gt; &lt;property&gt; &lt;name&gt;dfs.permissions.enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.handler.count&lt;/name&gt; &lt;value&gt;5&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.handler.count&lt;/name&gt; &lt;value&gt;5&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 12345678910111213141516171819202122&lt;!-- yarn-site.xml -&gt; path_to_hadoop/etc/hadoop/ --&gt;&lt;!-- https://hadoop.apache.org/docs/r2.7.3/hadoop-yarn/hadoop-yarn-common/yarn-default.xml --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!-- enable log aggregation, this is false by default --&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;node4&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.timeline-service.hostname&lt;/name&gt; &lt;value&gt;node4&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 123456789101112&lt;!-- mapred-site.xml -&gt; path_to_hadoop/etc/hadoop/ --&gt;&lt;!-- https://hadoop.apache.org/docs/r2.7.3/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;node3:19888&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 启动流程Step1. 第一次启动HDFS，需要格式化一下hdfs 1$HADOOP_PREFIX/bin/hdfs namenode -format &lt;cluster-name&gt; Step2. 启动 1234start-dfs.shstart-yarn.shyarn-daemon.sh start proxyservermr-jobhistory-daemon.sh start historyserver 停止流程1234stop-dfs.shstop-yarn.shyarn-daemon.sh stop proxyservermr-jobhistory-daemon.sh stop historyserver 访问Once the Hadoop cluster is up and running check the web-ui of the components as described below: Daemon Web Interface Notes NameNode http://nn_host:port/ Default HTTP port is 50070. SecondaryNameNode http://nn_host:port/ Default HTTP port is 50090. ResourceManager http://rm_host:port/ Default HTTP port is 8088. YarnWebProxy http://proxy_host:port/ no Default HTTP port. 需要自定义 MapReduce JobHistory Server http://jhs_host:port/ Default HTTP port is 19888.","categories":[{"name":"Hadoop学习","slug":"Hadoop学习","permalink":"https://shang.at/categories/Hadoop学习/"}],"tags":[{"name":"Hadoop集群搭建","slug":"Hadoop集群搭建","permalink":"https://shang.at/tags/Hadoop集群搭建/"}]},{"title":"Hadoop学习-配置详解","slug":"Hadoop学习-配置详解","date":"2020-06-23T09:12:52.000Z","updated":"2020-06-26T02:48:44.351Z","comments":true,"path":"post/Hadoop学习-配置详解/","link":"","permalink":"https://shang.at/post/Hadoop学习-配置详解/","excerpt":"","text":"版本：2.7.3 core-default.xml parameter default value notes fs.defaultFS file:/// 定义namenode的URI，改成hdfs://host:port/ hadoop.tmp.dir /tmp/hadoop-${user.name} 定义其他临时目录的根目录 io.file.buffer.size 4096 读写文件操作时的缓存字节数，必须是硬件上的内存页大小的整数倍 hdfs-default.xmlnamenode parameter default value note dfs.namenode.http(s)-address 0.0.0.0:50070(50470) 配置dfs的web ui界面，不建议修改。可以通过http://namenode_hostname:50070访问 dfs.namenode.name.dir file://${hadoop.tmp.dir}/dfs/name 配置DFS namenode的fsimage文件存放在本次文件系统的路径。如果配置了使用逗号分隔的多个路径，那么namemode会在每个目录下面都冗余的存放一份。 dfs.namenode.edits.dir dfs.namenode.name.dir 配置DFS namenode的edits文件存放在本次文件系统的路径。如果配置了使用逗号分隔的多个路径，那么namemode会在每个目录下面都冗余的存放一份。 dfs.namenode.fs-limits.min-block-size 1048576 1m 最小块大小（以字节为单位），由Namenode在创建时强制执行。这样可以防止意外创建具有很小块大小（因此有很多块）的文件，这会降低性能。减少数据块的数量， dfs.namenode.handler.count 10 namenode端服务的线程数，测试时可以配置的小一些，减少内存占用 datanode parameter default value notes dfs.datanode.data.dir file://${hadoop.tmp.dir}/dfs/data 文件块的在local filesystem中的存放路径。如果提供的是逗号分隔的目录列表，那么数据将会存储在所有的目录中，(通常目录列表是在不同的设备上)。目录应该被相应的存储类型所标记(HDFS上有四种存储设备：SSD、DISK、ARCHIVE、RAM_DISK)，如果没有指定，默认是DISK。如果目录不存在，那么会自动创建(需要获取目录权限) dfs.datanode.handler.count 10 namenode端服务的线程数，测试时可以配置的小一些，减少内存占用 secondary namenode parameter default value notes dfs.namenode.secondary.http(s)-address 0.0.0.0:50090(50091) 配置secondary namenode的http server和端口 dfs.namenode.checkpoint.dir file://${hadoop.tmp.dir}/dfs/namesecondary dfs.namenode.checkpoint.edits.dir ${dfs.namenode.checkpoint.dir} dfs.namenode.checkpoint.period 3600 dfs.namenode.checkpoint.txns 1000000 dfs.namenode.checkpoint.check.period 60 dfs.namenode.checkpoint.max-retries 3 dfs.namenode.num.checkpoints.retained 2 dfs parameter default value notes dfs.permissions.enabled true 配置是否启用权限检查，默认是启用的。测试时可以设置为false。当开启状态时，dfs不会检测文件的权限检测。HDFS PermissionHDFS默认启动namenode的user为superuser，这个 dfs.blocksize 134217728 128m 单位字节，新文件的block 大小 dfs.hosts / dfs.hosts.exclude List of permitted/excluded DataNodes.If necessary, use these files to control the list of allowable datanodes. dfs.replication 3 块副本数 dfs.webhdfs.enabled true 启动namenode和datanode上的WebHHDFS(REST API) mapred-default.xmlMapReduce Applications Parameter Value Notes mapreduce.framework.name yarn Execution framework set to Hadoop YARN. mapreduce.map.memory.mb 1536 Larger resource limit for maps. mapreduce.map.java.opts -Xmx1024M Larger heap-size for child jvms of maps. mapreduce.reduce.memory.mb 3072 Larger resource limit for reduces. mapreduce.reduce.java.opts -Xmx2560M Larger heap-size for child jvms of reduces. mapreduce.task.io.sort.mb 100 Higher memory-limit while sorting data for efficiency. mapreduce.task.io.sort.factor 10 More streams merged at once while sorting files. mapreduce.reduce.shuffle.parallelcopies 5 Higher number of parallel copies run by reduces to fetch outputs from very large number of maps. MapReduce JobHistory Server Parameter Value Notes mapreduce.jobhistory.address 0.0.0.0:10020 MapReduce JobHistory Server IPC host:port mapreduce.jobhistory.webapp.address 0.0.0.0:19888 MapReduce JobHistory Server Web UI host:port yarn.app.mapreduce.am.staging-dir /tmp/hadoop-yarn/staging The staging dir used while submitting jobs. mapreduce.jobhistory.intermediate-done-dir ${yarn.app.mapreduce.am.staging-dir}/history/done_intermediate Directory where history files are written by MapReduce jobs. mapreduce.jobhistory.done-dir ${yarn.app.mapreduce.am.staging-dir}/history/done Directory where history files are managed by the MR JobHistory Server. yarn-default.xmlResourceManager and NodeManager Parameter default Value Notes yarn.acl.enable false 是否开启ACLs yarn.admin.acl * ACL to set admins on the cluster. ACLs are of for comma-separated-usersspacecomma-separated-groups. Defaults to special value of * which means anyone. Special value of just space means no one has access. yarn.log-aggregation-enable false 是否启动日志聚合。日志聚合会收集每个container的日志并且在应用完成后将他们移动到HDFS中。具体目录由下面两个选项配置yarn.nodemanager.remote-app-log-dir和yarn.nodemanager.remote-app-log-dir-suffix。用户可以通过Application Timeline Server访问这些日志文件 ResourceManager Parameter default Value Notes yarn.resourcemanager.address ${yarn.resourcemanager.hostname}:8032 配置RM的URIfor clients to submit jobs. yarn.resourcemanager.scheduler.address ${yarn.resourcemanager.hostname}:8030 资源调度器URIfor ApplicationMasters to talk to Scheduler to obtain resources. yarn.resourcemanager.resource-tracker.address ${yarn.resourcemanager.hostname}:8031 for NodeManagers. yarn.resourcemanager.admin.address ${yarn.resourcemanager.hostname}:8033 for administrative commands. yarn.resourcemanager.webapp.address ${yarn.resourcemanager.hostname}:8088 RM web application yarn.resourcemanager.hostname 0.0.0.0 ResourceManager host.应该改成特定的hostname yarn.web-proxy.address 默认没有配置，会作为RM的一部分运行 The address for the web proxy as HOST:PORT, if this is not given then the proxy will run as part of the RM yarn.resourcemanager.scheduler.class org.apache.hadoop.yarn.server.resourcemanager.&lt;br /&gt;scheduler.capacity.CapacityScheduler 指定RM使用的调度器：CapacityScheduler (recommended), FairScheduler (also recommended), or FifoScheduler yarn.scheduler.minimum-allocation-mb 1024 In MBs，Minimum limit of memory to allocate to each container request at the ResourceManager. yarn.scheduler.maximum-allocation-mb 8192 In MBs，Maximum limit of memory to allocate to each container request at the Resource Manager. yarn.resourcemanager.nodes.include-path / yarn.resourcemanager.nodes.exclude-path List of permitted/excluded NodeManagers.If necessary, use these files to control the list of allowable NodeManagers. NodeManager Parameter default Value Notes yarn.nodemanager.resource.memory-mb 8192 Resource i.e. available physical memory, in MB, for given NodeManager.Defines total available resources on the NodeManager to be made available to running containers yarn.nodemanager.vmem-pmem-ratio 2.1Maximum ratio by which virtual memory usage of tasks may exceed physical memory The virtual memory usage of each task may exceed its physical memory limit by this ratio. The total amount of virtual memory used by tasks on the NodeManager may exceed its physical memory usage by this ratio. yarn.nodemanager.local-dirs ${hadoop.tmp.dir}/nm-local-dirComma-separated list of paths on the local filesystem where intermediate data is written. Multiple paths help spread disk i/o. yarn.nodemanager.log-dirs ${yarn.log.dir}/userlogsComma-separated list of paths on the local filesystem where logs are written. Multiple paths help spread disk i/o. yarn.nodemanager.log.retain-seconds 10800 Default time (in seconds) to retain log files on the NodeManager. Only applicable if log-aggregation is disabled. yarn.nodemanager.remote-app-log-dir /tmp/logs HDFS directory where the application logs are moved on application completion. Need to set appropriate permissions. Only applicable if log-aggregation is enabled. yarn.nodemanager.remote-app-log-dir-suffix logs Suffix appended to the remote log dir. Logs will be aggregated to ${yarn.nodemanager.remote-app-log-dir}/${user}/${thisParam} Only applicable if log-aggregation is enabled. yarn.nodemanager.aux-services mapreduce_shuffle Shuffle service that needs to be set for Map Reduce applications. History Server (Needs to be moved elsewhere) Parameter default Value Notes yarn.log-aggregation.retain-seconds -1 How long to keep aggregation logs before deleting them. -1 disables. Be careful, set this too small and you will spam the name node. yarn.log-aggregation.retain-check-interval-seconds -1 Time between checks for aggregated log retention. If set to 0 or a negative value then the value is computed as one-tenth of the aggregated log retention time. Be careful, set this too small and you will spam the name node.","categories":[{"name":"Hadoop学习","slug":"Hadoop学习","permalink":"https://shang.at/categories/Hadoop学习/"}],"tags":[{"name":"Hadoop配置详解","slug":"Hadoop配置详解","permalink":"https://shang.at/tags/Hadoop配置详解/"}]},{"title":"Shell编程-常用命令","slug":"Shell编程-常用命令","date":"2020-06-23T08:53:17.000Z","updated":"2021-02-10T02:38:20.804Z","comments":true,"path":"post/Shell编程-常用命令/","link":"","permalink":"https://shang.at/post/Shell编程-常用命令/","excerpt":"","text":"文本编辑sedawk磁盘系统df查看磁盘使用量：df -hT du查看各文件夹大小：du -h --max-depth=1 /path Linux上的定时器cron命令xargsexecfind grep 从文件内容查找匹配指定字符串的行：$ grep “被查找的字符串” 文件名例子：在当前目录里第一级文件夹中寻找包含指定字符串的.in文件grep “thermcontact” /.in 从文件内容查找与正则表达式匹配的行：$ grep –e “正则表达式” 文件名 查找时不区分大小写：$ grep –i “被查找的字符串” 文件名 查找匹配的行数：$ grep -c “被查找的字符串” 文件名 从文件内容查找不匹配指定字符串的行：$ grep –v “被查找的字符串” 文件名 从根目录开始查找所有扩展名为.log的文本文件，并找出包含”ERROR”的行find / -type f -name “.log” | xargs grep “ERROR”例子：从当前目录开始查找所有扩展名为.in的文本文件，并找出包含”thermcontact”的行find . -name “.in” | xargs grep “thermcontact” 系统管理tophttps://blog.csdn.net/hsd2012/article/details/51387332 strace strace 命令是一种强大的工具, 能够显示任何由用户空间程式发出的系统调用. strace 显示这些调用的参数并返回符号形式的值. strace 从内核接收信息, 而且无需以任何特别的方式来构建内核. strace 的每一行输出包括系统调用名称, 然后是参数和返回值. 远程控制sshscp12345# 复制某文件(夹) 到远程地址scp [-r] xxxxx user@host:path# 如果要求是同一个目录可以直接用如下写法scp [-r] xxxxx user@host:`pwd` rsync软件管理rpmRPM 全名是『 RedHat Package Manager 安装12345678910111213141516171819rpm -i *.rpm # 安装rpm包rpm -ivh *.rmp选项与参数：-i ：install 的意思-v ：察看更细部的安装资讯画面-h ：以安装资讯列显示安装进度范例一：安装 rp-pppoe-3.5-32.1.i386.rpmrpm -ivh rp-pppoe-3.5-32.1.i386.rpmPreparing... ####################################### [100%] 1:rp-pppoe ####################################### [100%] 范例二、一口气安装两个以上的软件时：rpm -ivh a.i386.rpm b.i386.rpm *.rpm# 后面直接接上许多的软件文件！范例三、直接由网络上面的某个文件安装，以网址来安装：rpm -ivh http://website.name/path/pkgname.rpm rpm 安装时常用的选项与参数说明 可下达的选项 代表意义 —nodeps 使用时机：当发生软件属性相依问题而无法安装，但你执意安装时 危险性： 软件会有相依性的原因是因为彼此会使用到对方的机制或功能，如果强制安装而不考虑软件的属性相依， 则可能会造成该软件的无法正常使用！ —replacefiles 使用时机： 如果在安装的过程当中出现了『某个文件已经被安装在你的系统上面』的资讯，又或许出现版本不合的信息 (confilcting files) 时，可以使用这个参数来直接覆盖文件。 危险性： 覆盖的动作是无法复原的！所以，你必须要很清楚的知道被覆盖的文件是真的可以被覆盖喔！否则会欲哭无泪！ —replacepkgs 使用时机： 重新安装某个已经安装过的软件！如果你要安装一堆 RPM 软件文件时，可以使用 rpm -ivh *.rpm ，但若某些软件已经安装过了， 此时系统会出现『某软件已安装』的资讯，导致无法继续安装。此时可使用这个选项来重复安装喔！ —force 使用时机：这个参数其实就是 —replacefiles 与 —replacepkgs 的综合体！ —test 使用时机： 想要测试一下该软件是否可以被安装到使用者的 Linux 环境当中，可找出是否有属性相依的问题。范例为： rpm -ivh pkgname.i386.rpm —test —justdb 使用时机： 由於 RPM 数据库破损或者是某些缘故产生错误时，可使用这个选项来升级软件在数据库内的相关资讯。 —nosignature 使用时机： 想要略过数码签章的检查时，可以使用这个选项。 —prefix 新路径 使用时机： 要将软件安装到其他非正规目录时。举例来说，你想要将某软件安装到 /usr/local 而非正规的 /bin, /etc 等目录， 就可以使用『 —prefix /usr/local 』来处理了。 —noscripts 使用时机：不想让该软件在安装过程中自行运行某些系统命令。 说明： RPM 的优点除了可以将文件放置到定位之外，还可以自动运行一些前置作业的命令，例如数据库的初始化。 如果你不想要让 RPM 帮你自动运行这一类型的命令，就加上他吧！ 升级12rpm -Uvh *.rpm rpm -Fvh *.rpm 选项 含义 -Uvh 后面接的软件即使没有安装过，则系统将予以直接安装； 若后面接的软件有安装过旧版，则系统自动升级至新版； -Fvh 如果后面接的软件并未安装到你的 Linux 系统上，则该软件不会被安装；亦即只有已安装至你 Linux 系统内的软件会被『升级』！ 查询123456789101112131415161718rpm -qa &lt;==已安装软件rpm -q[licdR] 已安装的软件名称 &lt;==已安装软件rpm -qf 存在於系统上面的某个档名 &lt;==已安装软件rpm -qp[licdR] 未安装的某个文件名称 &lt;==查阅RPM文件选项与参数：查询已安装软件的资讯：-q ：仅查询，后面接的软件名称是否有安装；-qa ：列出所有的，已经安装在本机 Linux 系统上面的所有软件名称；-qi ：列出该软件的详细资讯 (information)，包含开发商、版本与说明等；-ql ：列出该软件所有的文件与目录所在完整档名 (list)；-qc ：列出该软件的所有配置档 (找出在 /etc/ 底下的档名而已)-qd ：列出该软件的所有说明档 (找出与 man 有关的文件而已)-qR ：列出与该软件有关的相依软件所含的文件 (Required 的意思)-qf ：由后面接的文件名称，找出该文件属於哪一个已安装的软件；查询某个 RPM 文件内含有的资讯：-qp[icdlR]：注意 -qp 后面接的所有参数以上面的说明一致。但用途仅在於找出 某个 RPM 文件内的资讯，而非已安装的软件资讯！注意！ 练习 12345678910111213141516例题：我想要知道我的系统当中，以 c 开头的软件有几个，如何实做？我的 WWW 服务器为 Apache ，我知道他使用的 RPM 软件档名为 httpd 。现在，我想要知道这个软件的所有配置档放置在何处，可以怎么作？承上题，如果查出来的配置文件已经被我改过，但是我忘记了曾经修改过哪些地方，所以想要直接重新安装一次该软件，该如何作？如果我误砍了某个重要文件，例如 /etc/crontab，偏偏不晓得他属於哪一个软件，该怎么办？答：rpm -qa | grep ^c | wc -lrpm -qc httpd假设该软件在网络上的网址为：http://web.site.name/path/httpd-x.x.xx.i386.rpm则我可以这样做：rpm -ivh http://web.site.name/path/httpd-x.x.xx.i386.rpm --replacepkgs虽然已经没有这个文件了，不过没有关系，因为 RPM 有记录在 /var/lib/rpm 当中的数据库啊！所以直接下达：rpm -qf /etc/crontab就可以知道是那个软件罗！重新安装一次该软件即可！ 卸载1234rpm -e *** # 可能会因为依赖的关系，导致卸载失败rpm -e *** --nodeps # 强制卸载rpm --rebuilddb # 当rpm数据库/var/lib/rpm内的文件破损，可以使用这个命令重建数据库(很慢) yum","categories":[{"name":"Shell编程","slug":"Shell编程","permalink":"https://shang.at/categories/Shell编程/"}],"tags":[{"name":"常见命令","slug":"常见命令","permalink":"https://shang.at/tags/常见命令/"}]},{"title":"操作系统-centos7修改hostname","slug":"操作系统-centos7修改hostname","date":"2020-06-23T04:31:11.000Z","updated":"2020-07-26T03:29:49.522Z","comments":true,"path":"post/操作系统-centos7修改hostname/","link":"","permalink":"https://shang.at/post/操作系统-centos7修改hostname/","excerpt":"","text":"在CentOS7中，有三种定义的主机名:静态的（static）、瞬态的（transient）、灵活的（pretty）。“静态”主机名也称为内核主机名，是系统在启动时从/etc/hostname自动初始化的主机名。“瞬态”主机名是在系统运行时临时分配的主机名，例如，通过DHCP或mDNS服务器分配。静态主机名和瞬态主机名都遵从作为互联网域名同样的字符限制规则。而另一方面，“灵活”主机名则允许使用自由形式（包括特殊/空白字符）的主机名，以展示给终端用户。 方法一1234567891011121314151617181920212223242526[root@Geeklp201 ~]# hostnamectl #查看一下当前主机名的情况 Static hostname: Geeklp201 Icon name: computer-vm Chassis: vm Machine ID: 77efa27de81d470883b5bb0ed04f468c Boot ID: fa62bd1c0f5e4e53a0691fb97971594f Virtualization: vmware Operating System: CentOS Linux 7 (Core) CPE OS Name: cpe:/o:centos:centos:7 Kernel: Linux 3.10.0-693.el7.x86_64 Architecture: x86-64[root@Geeklp201 ~]# hostnamectl set-hostname geeklp --static[root@Geeklp201 ~]# hostnamectl status Static hostname: geeklp Pretty hostname: Geeklp201 Icon name: computer-vm Chassis: vm Machine ID: 77efa27de81d470883b5bb0ed04f468c Boot ID: fa62bd1c0f5e4e53a0691fb97971594f Virtualization: vmware Operating System: CentOS Linux 7 (Core) CPE OS Name: cpe:/o:centos:centos:7 Kernel: Linux 3.10.0-693.el7.x86_64 Architecture: x86-64重启VM 方法二通过修改文件/etc/hostname来实现主机名的修改。把该文件内容替换成自己想要的主机名重启即可。","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://shang.at/categories/操作系统/"}],"tags":[{"name":"centos7修改hostname","slug":"centos7修改hostname","permalink":"https://shang.at/tags/centos7修改hostname/"}]},{"title":"工具使用-vim","slug":"工具使用-vim","date":"2020-06-23T04:22:22.000Z","updated":"2020-08-11T01:07:38.733Z","comments":true,"path":"post/工具使用-vim/","link":"","permalink":"https://shang.at/post/工具使用-vim/","excerpt":"","text":"vim快捷键 命令模式：esc 编辑模式： 在当前字符前开始编辑：命令模式下按i 在行首处开始编辑：I 在当前字符后开始编辑：命令模式下按a 在行尾处开始编辑：A 另起一行：命令模式下按o 在当前行的之上另起一行：命令模式下按O 删除当前字符后的当前行的所有内容：D 恢复修改：u 跳到最后一行：G 复制 单行复制 在命令模式下，将光标移动到将要复制的行处，按“yy”进行复制，将光标移动到将要粘贴的行处，按“p”进行粘贴； 多行复制 在命令模式下，将光标移动到将要复制的首行处，按“nyy”复制n行；其中n为1、2、3…… 2、粘贴 在命令模式下，将光标移动到将要粘贴的行处，按“p”进行粘贴 查询 输入： /abc 查询 abc 开头的单词 之后，所以以abc开头的单词都会标记高亮，输入 n 会查找下一个结果 ?pattern 向上搜索#继续搜索上一个 执行命令在命令模式下，输入:和!后面跟着要执行的命令，比如：:! ls .，表示列出当前目录的所有文件 查看 暂时显示/取消行号： 使用Vim打开文件后，在Normal模式下输入 :set number（或 :set nu）显示行号 :set nonumber （或 :set nonu）取消行号 永久显示行号 查找Vim设定文件 sudo find / -name vimrc 修改Vim设定文件 /etc/vimrc ,末尾添加 set number （或 set nu） 保存即可。","categories":[{"name":"工具使用","slug":"工具使用","permalink":"https://shang.at/categories/工具使用/"}],"tags":[{"name":"vim","slug":"vim","permalink":"https://shang.at/tags/vim/"}]},{"title":"大数据-常见端口","slug":"大数据-常见端口","date":"2020-06-21T01:17:50.000Z","updated":"2020-06-23T07:50:07.013Z","comments":true,"path":"post/大数据-常见端口/","link":"","permalink":"https://shang.at/post/大数据-常见端口/","excerpt":"","text":"mysql：3306 redis： zookeeper：2181 kafka：9092 eagle：8048 hdfs：50070 yarn：8088 spark：4041 flink：8081","categories":[{"name":"大数据生态","slug":"大数据生态","permalink":"https://shang.at/categories/大数据生态/"}],"tags":[{"name":"大数据端口","slug":"大数据端口","permalink":"https://shang.at/tags/大数据端口/"}]},{"title":"大数据-环境配置","slug":"大数据-环境配置","date":"2020-06-20T10:01:00.000Z","updated":"2020-12-11T08:02:30.653Z","comments":true,"path":"post/大数据-环境配置/","link":"","permalink":"https://shang.at/post/大数据-环境配置/","excerpt":"","text":"基础linux的文件和目录的权限规则使用ls -l命令可以查看当前目录的文件列表以及权限信息，显示如下 drwxr-xr-x linux安装rpm包免密登录123456789# 生成密钥 公钥sudo ssh-keygen -t rsa -f ~/.ssh/id_rsa# 将公钥拷贝到目标机器ssh-copy-id user@tartget_hostame# 这样之后就可以在节点之间 免密登录了ssh user@tartget_hostamessh tartget_hostame # 如果当前登录的用户和target的用户名一致，则不需要加user@ 注意，如果执行ssh-copy-id 输入密码后仍报错(Permission denied (publickey,gssapi-keyex,gssapi-with-mic))，可以尝试按照如下方案解决： 进入target机器，进入cd /etc/ssh/sshd_config，然后修改为PasswordAuthentication yes，最后重启sshd服务service sshd restart即可 同步时钟ntpdate cn.pool.ntp.org 或 ntpdate ntp[1-7].aliyun.com clock -w 开源大数据平台Cloudera Ambari+HDP CM+CDH 搭建CM+CDH大数据平台 节点 配置 mysql cloudera-scm-server cloudera-scm-agent node100 2G 2cores ✅ master 16G 8cores ✅ ✅ node111 8G 6cores ✅ node112 8G 6cores ✅ node113 8G 6cores ✅ 1、准备一个数据库节点 使用vagrant创建一个虚拟机，vagrant配置文件如下： 1234567891011121314151617# -*- mode: ruby -*-# vi: set ft=ruby :Vagrant.configure(\"2\") do |config| config.vm.box = \"centos/7\" config.vm.hostname = \"node100\" config.vm.provider :virtualbox do |v| v.name = \"node100\" v.memory = 2024 v.cpus = 2 end config.vm.network :private_network, ip: \"10.211.55.109\" config.vm.provision :shell, path: \"mysql.sh\"end 对应的mysql.sh如下： 12345678910#!/bin/bashsudo yum -y updatesudo yum -y upgradesudo yum install -y net-tools rsync mlocate wget vim ntpdate telnet gcc zlib-dev openssl-devel sqlite-devel bzip2-devel binutils qt make patch libgomp glibc-headers glibc-devel kernel-headers kernel-devel dkms straceif [ -z $(rpm -qa | grep 'mysql-community-server') ]; then wget https://dev.mysql.com/get/mysql57-community-release-el7-9.noarch.rpm rpm -ivh mysql57-community-release-el7-9.noarch.rpm yum install -y mysql-community-serverfi 2、设置mysql密码和配置mysql可以远程连接 启动mysql：service mysqld start or systemctl start mysqld.service 设置开机自启动：systemctl enable mysqld.service 检查状态：service mysqld status or systemctl stop mysqld.service 查看初始密码：grep &#39;temporary password&#39; /var/log/mysqld.log 重置root密码： 12345678# 登录mysqlmysql -uroot -p# 修改密码ALTER USER 'root'@'localhost' IDENTIFIED BY 'MyNewPass4!';# 刷新flush privileges; 设置远程连接： 修改/etc/my.cnf中的bind-address=0.0.0.0 update user set host = &#39;%&#39; where user = &#39;root&#39;; 创建一个用户 1234567891011# 创建新用户create user 'cdh' identified by '!Cdh1234';# 授权grant all privileges on *.* to 'cdh'@'%' with grant option;# *.* 前边的*号指的是数据库，后面的*号指的是表，*.*的意思就是任意数据库下的任意表# 'root'@'%'，'root'用户名，'%'任意的主机名。# 这条配置信息就是说，允许任意节点以root身份登录，并且可以访问mysql里的任意库下的任意表# 刷新flush privileges; 3、安装cm+cdh 使用Vagrant创建四个虚拟机： 12345678910111213141516171819202122232425262728293031323334353637# -*- mode: ruby -*-# vi: set ft=ruby :Vagrant.configure(\"2\") do |config| config.vm.box = \"centos/7\" # master 节点 config.vm.define :master do |master| master.vm.hostname = \"master\" master.vm.provider :virtualbox do |v| v.name = \"master\" v.memory = 16384 # 16G v.cpus = 8 end master.vm.network :public_network, :bridge=&gt;'em1', ip: \"192.168.0.110\" master.vm.provision 'bootstrap_master', type: :shell, path: \"bootstrap_master.sh\" end # node 节点 (1..3).each do |i| config.vm.define \"node11#&#123;i&#125;\" do |node| node.vm.hostname = \"node11#&#123;i&#125;\" node.vm.provider :virtualbox do |v| v.name = \"node11#&#123;i&#125;\" v.memory = 8096 # 8G v.cpus = 6 end node.vm.network :public_network, :bridge=&gt;'em1', ip: \"192.168.0.11#&#123;i&#125;\" end end config.vm.provision 'java', type: :shell, path: \"java.sh\" config.vm.provision 'sshd', type: :shell, path: \"sshd.sh\" config.vm.provision 'bootstrap', type: :shell, path: \"bootstrap.sh\"end boostrap.sh 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253#!/bin/bashsudo yum -y updatesudo yum -y upgradesudo yum install -y net-tools rsync mlocate wget vim ntpdate telnet gcc zlib-dev openssl-devel \\ sqlite-devel bzip2-devel binutils make patch libgomp glibc-headers glibc-devel kernel-headers kernel-devel dkms strace \\ bind-utils psmisc cyrus-sasl-plain cyrus-sasl-gssapi redhat-lsb httpd mod_ssl python-psycopg2 MySQL-python postgresql-libs# sshkeyif [ ! -f \"~/.ssh/id_rsa\" ]; then sudo ssh-keygen -t rsa -f ~/.ssh/id_rsafi# set hostsecho '127.0.0.1' &gt; /etc/hostsecho '192.168.0.109 node100' &gt;&gt; /etc/hostsecho '192.168.0.110 master' &gt;&gt; /etc/hostsecho '192.168.0.111 node111' &gt;&gt; /etc/hostsecho '192.168.0.112 node112' &gt;&gt; /etc/hostsecho '192.168.0.113 node113' &gt;&gt; /etc/hosts# set system timezonetimedatectl set-timezone Asia/Shanghai# 时钟同步ntpdate ntp1.aliyun.comclock -w# set mysql drivermkdir -p /usr/share/java/cp /vagrant/components/mysql-connector-java-5.1.49.jar /usr/share/java/mysql-connector-java.jar# cdhif [ ! -f \"/opt/cloudera/parcel-repo/CDH-6.2.0-1.cdh6.2.0.p0.967373-el7.parcel\" ]; then mkdir -p /opt/cloudera/parcel-repo cp /vagrant/components/CDH-6.2.0-1.cdh6.2.0.p0.967373-el7.parcel /opt/cloudera/parcel-repo/CDH-6.2.0-1.cdh6.2.0.p0.967373-el7.parcel cp /vagrant/components/manifest.json /opt/cloudera/parcel-repo/manifest.json cd /opt/cloudera/parcel-repo &amp;&amp; sha1sum CDH-6.2.0-1.cdh6.2.0.p0.967373-el7.parcel | awk '&#123; print $1 &#125;' &gt; CDH-6.2.0-1.cdh6.2.0.p0.967373-el7.parcel.shafi# cm:daemonsif [ -z $(rpm -qa | grep 'cloudera-manager-daemons') ]; then rpm -i /vagrant/components/cloudera-manager-daemons-6.2.0-968826.el7.x86_64.rpmfi# cm:agentif [ -z $(rpm -qa | grep 'cloudera-manager-agent') ]; then rpm -i /vagrant/components/cloudera-manager-agent-6.2.0-968826.el7.x86_64.rpmfi# 修改cm agent的配置文件，server_host=master，否则会找不到 serversed -i 's/server_host=localhost/server_host=master/g' /etc/cloudera-scm-agent/config.ini Bootstrap_master.sh 1234567891011121314151617181920212223242526272829303132333435363738#!/bin/bash# 安装mysql client# if [ -z $(rpm -qa | grep 'mysql-community-client') ]; then# wget https://repo.mysql.com//mysql80-community-release-el7-3.noarch.rpm# rpm -ivh mysql80-community-release-el7-3.noarch.rpm# yum install -y mysql-community-client# fiif [ -z $(rpm -qa | grep 'mysql-community-client') ]; then wget https://dev.mysql.com/get/mysql57-community-release-el7-9.noarch.rpm rpm -ivh mysql57-community-release-el7-9.noarch.rpm yum install -y mysql-community-client mysql-connector-javafi# cm:daemonsif [ -z $(rpm -qa | grep 'cloudera-manager-daemons') ]; then rpm -i /vagrant/components/cloudera-manager-daemons-6.2.0-968826.el7.x86_64.rpmfi# cm:agentif [ -z $(rpm -qa | grep 'cloudera-manager-agent') ]; then rpm -i /vagrant/components/cloudera-manager-agent-6.2.0-968826.el7.x86_64.rpmfi# cm:serverif [ -z $(rpm -qa | grep 'cloudera-manager-server') ]; then rpm -i /vagrant/components/cloudera-manager-server-6.2.0-968826.el7.x86_64.rpmfi# cdhif [ ! -f \"/opt/cloudera/parcel-repo/CDH-6.2.0-1.cdh6.2.0.p0.967373-el7.parcel\" ]; then cp /vagrant/components/CDH-6.2.0-1.cdh6.2.0.p0.967373-el7.parcel /opt/cloudera/parcel-repo/CDH-6.2.0-1.cdh6.2.0.p0.967373-el7.parcel cp /vagrant/components/manifest.json /opt/cloudera/parcel-repo/manifest.json cd /opt/cloudera/parcel-repo &amp;&amp; sha1sum CDH-6.2.0-1.cdh6.2.0.p0.967373-el7.parcel | awk '&#123; print $1 &#125;' &gt; CDH-6.2.0-1.cdh6.2.0.p0.967373-el7.parcel.shafimysql -hnode100 -uroot -p!Aa147258 &lt; /vagrant/cdh.sql Sshd.sh 1234567# 设置ssh 连接# open PasswordAuthenticationsed -i 's/#PasswordAuthentication yes/PasswordAuthentication yes/g' /etc/ssh/sshd_configsed -i 's/PasswordAuthentication no/#PasswordAuthentication yes/g' /etc/ssh/sshd_config# restart sshdservice sshd restart Java.sh 123456if [ -z $(rpm -qa | grep 'jdk1.8-1.8.0') ]; then rpm -qa | grep jdk | xargs rpm -e rpm -i /vagrant/components/jdk-8u191-linux-x64.rpm cp /vagrant/conf/java.sh /etc/profile.d/fi Cdh.sql 初始化相关数据库 1234567891011121314151617181920212223242526272829303132333435CREATE DATABASE IF NOT EXISTS scm DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;create user IF NOT EXISTS 'scm' identified by '!Cdh1234';GRANT ALL privileges ON scm.* TO 'scm'@'%' with grant option;CREATE DATABASE IF NOT EXISTS amon DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;create user IF NOT EXISTS 'amon' identified by '!Cdh1234';GRANT ALL privileges ON amon.* TO 'amon'@'%' with grant option;CREATE DATABASE IF NOT EXISTS rman DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;create user IF NOT EXISTS 'rman' identified by '!Cdh1234';GRANT ALL privileges ON rman.* TO 'rman'@'%' with grant option;CREATE DATABASE IF NOT EXISTS hue DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;create user IF NOT EXISTS 'hue' identified by '!Cdh1234';GRANT ALL privileges ON hue.* TO 'hue'@'%' with grant option;CREATE DATABASE IF NOT EXISTS metastore DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;create user IF NOT EXISTS 'hive' identified by '!Cdh1234';GRANT ALL privileges ON metastore.* TO 'hive'@'%' with grant option;CREATE DATABASE IF NOT EXISTS sentry DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;create user IF NOT EXISTS 'sentry' identified by '!Cdh1234';GRANT ALL privileges ON sentry.* TO 'sentry'@'%' with grant option;CREATE DATABASE IF NOT EXISTS nav DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;create user IF NOT EXISTS 'nav' identified by '!Cdh1234';GRANT ALL privileges ON nav.* TO 'nav'@'%' with grant option;CREATE DATABASE IF NOT EXISTS navms DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;create user IF NOT EXISTS 'navms' identified by '!Cdh1234';GRANT ALL privileges ON navms.* TO 'navms'@'%' with grant option;CREATE DATABASE IF NOT EXISTS oozie DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;create user IF NOT EXISTS 'oozie' identified by '!Cdh1234';GRANT ALL privileges ON oozie.* TO 'oozie'@'%' with grant option; 下载cm，建议使用迅雷下载 —— 我是将下载的文件放到了components文件夹下，vagrant启动的时候直接安装 1234wget https://archive.cloudera.com/cm6/6.2.0/redhat7/yum/RPMS/x86_64/cloudera-manager-daemons-6.2.0-968826.el7.x86_64.rpmwget https://archive.cloudera.com/cm6/6.2.0/redhat7/yum/RPMS/x86_64/cloudera-manager-agent-6.2.0-968826.el7.x86_64.rpmwget https://archive.cloudera.com/cm6/6.2.0/redhat7/yum/RPMS/x86_64/cloudera-manager-server-6.2.0-968826.el7.x86_64.rpmwget https://archive.cloudera.com/cm6/6.2.0/redhat7/yum/RPMS/x86_64/cloudera-manager-server-db-2-6.2.0-968826.el7.x86_64.rpm 设置 Cloudera Manager 数据库：/opt/cloudera/cm/schema/scm_prepare_database.sh --host node100 mysql scm scm 需要使用--host指定mysql主机位置，否则默认是localhost，在scm_prepare_database.sh中有配置 下载cdh，建议使用迅雷下载 —— 我是将下载的文件放到了components文件夹下，vagrant启动的时候将他们复制到/opt/cloudera/parcel-repo，CM会自动到该目录下找到对应的文件 12345cd /opt/cloudera/parcel-repowget https://archive.cloudera.com/cdh6/6.2.0/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373-el7.parcelwget https://archive.cloudera.com/cdh6/6.2.0/parcels/manifest.jsonsha1sum CDH-6.2.0-1.cdh6.2.0.p0.967373-el7.parcel | awk '&#123; print $1 &#125;' &gt; CDH-6.2.0-1.cdh6.2.0.p0.967373-el7.parcel.sha 启动Cloudera Manager Server：systemctl start cloudera-scm-server 查看启动日志：tail -f /var/log/cloudera-scm-server/cloudera-scm-server.log 4、CM设置过程： 登录：用户名密码都是admin 欢迎页 下一步，勾选协议 选择套餐 安装 设置集群名称 选择集群节点 设置仓库 jdk设置，不要勾选 认证设置 安装Agent 在这一步，卡了很久，agent总是安装失败：遇到了两个问题： ​ a. agent找不到server，agent的配置错误，需要修改agent的配置文件：sed -i &#39;s/server_host=localhost/server_host=master/g&#39; /etc/cloudera-scm-agent/config.ini ​ b. 改完之后，发现还是找不到，总是去localhost找，经过查询，发现需要执行以下命令：mv /usr/bin/host /usr/bin/host.back，估计是CM做了缓存，需要清掉错误的缓存 安装Parcels， 初始化完成 集群配置：我选择了自定义 分配角色 设置数据库 Review Changes 遇到的问题： 1. Command failed to run because service HDFS has an invalid configuration. Review and correct its configuration. First error: HDFS service not configured for High Availability must have a SecondaryNameNode，解决方案：进入下面的界面，点击add role instances，把snn加上 2. Namenode format失败：点击这里手动format CDH 系统优化之关闭透明大页面及设置swappiness","categories":[{"name":"大数据生态","slug":"大数据生态","permalink":"https://shang.at/categories/大数据生态/"}],"tags":[{"name":"大数据环境","slug":"大数据环境","permalink":"https://shang.at/tags/大数据环境/"}]},{"title":"虚拟机-Vagrant使用","slug":"虚拟机-Vagrant使用","date":"2020-06-20T03:48:16.000Z","updated":"2020-08-10T02:35:41.459Z","comments":true,"path":"post/虚拟机-Vagrant使用/","link":"","permalink":"https://shang.at/post/虚拟机-Vagrant使用/","excerpt":"","text":"什么是Vagrantvagrant是一个基于VirtualBox, VMware, AWS等平台的一个构造和管理VM的工具，它提供了一个简单的工作流程，让VM的创建和管理全都自动化。vagrant的配置是基于ruby的 Vagrant是基于Box的，Box是针对Vagrant运行环境的封装 如何使用VagrantStep 1：安装 官网：https://www.vagrantup.com 虚拟机位置：/root/.vagrant.d/ Step 2：初始化环境 123mkdir vm-workspacecd vm-workspacevagrant init 123# box下载很慢的时候，可以先下载box，然后通过命令安装boxwget https://vagrantcloud.com/centos/boxes/7/versions/2004.01/providers/virtualbox.boxvagrant box add --name centos/7 virtualbox.box Step 3：配置Vagrantfile 经过vagrant init之后，会在vm-workspace下生成一个Vagrantfile，内容如下(删除了注释) 12345# -*- mode: ruby -*-# vi: set ft=ruby :Vagrant.configure(\"2\") do |config| config.vm.box = \"centos/7\"end 配置文件简介： 2 是指当前配置的Vagrant config的版本，目前Vagrant只支持两个版本 1和2，这里我不用改，用2就可以 config 就是配置对象，我们可以对他进行配置 do … end 是ruby的语法，就是一个代码块 下面是创建了 123456789101112131415161718192021222324252627282930313233343536373839404142# -*- mode: ruby -*-# vi: set ft=ruby :# 2 指定了配置的版本Vagrant.configure(\"2\") do |config| # 指定 box为 centos/7 config.vm.box = \"centos/7\" # 使用define定义vm的配置节点：一个配置节点就是一个虚拟机。 # 这里表示：在config中配置一个master的vm，该vm的配置对象命名为master，下面可以对该配置进行配置 config.vm.define :master do |master| # 配置master的hostname为master master.vm.hostname = \"master\" # 定义 虚机容器提供者配置，这里使用virtualbox。打开virtualbox后，可以在里面看到对应的vm实例 master.vm.provider :virtualbox do |v| v.name = \"master\" # vm的名称 v.memory = 1024 # vm的内存 v.cpus = 1 # vm可以使用的CPU个数 end # 配置master vm使用host-only网络模式，ip为10.211.55.100 master.vm.network :private_network, ip: \"10.211.55.100\" # 配置vagrant 启动vm的时候，需要执行的命令或脚本 # master.vm.provision :shell, path: \"bootstrap_master.sh\" end # 再次循环给vm创建3个配置节点，即再创建3个虚拟机 (1..3).each do |i| config.vm.define \"node#&#123;i&#125;\" do |node| node.vm.hostname = \"node#&#123;i&#125;\" node.vm.provider :virtualbox do |v| v.name = \"node#&#123;i&#125;\" v.memory = 1024 v.cpus = 1 end node.vm.network :private_network, ip: \"10.211.55.10#&#123;i&#125;\" # node.vm.provision :shell, path: \"bootstrap_master.sh\" end end # config.vm.synced_folder \"\" \"\" # vagrant默认会把当前工作目录挂载在vm的/vagrant目录下 config.vm.provision :shell, path: \"bootstrap.sh\" config.vm.provision :shell, path: \"sshd.sh\"end 123456789101112131415161718# bootstrap.sh:虚拟机初始化的过程，并且配置java、sshkey、hostssudo yum -y updatesudo yum -y upgradesudo yum groupinstall -y developmentsudo yum install -y java-1.8.0-openjdk net-tools rsync mlocate wget vim \\ gcc zlib-dev openssl-devel sqlite-devel bzip2-devel python-devel# set Javaecho 'export JAVA_HOME=/usr/lib/jvm/jre' &gt;&gt; /etc/profile.d/java.shecho 'export PATH=/usr/lib/jvm/jre/bin:$PATH' &gt;&gt; /etc/profile.d/java.sh# sshkeysudo ssh-keygen -t rsa -f ~/.ssh/id_rsa# set hostsecho '10.211.55.100 node0' &gt;&gt; /etc/hostsecho '10.211.55.101 node1' &gt;&gt; /etc/hostsecho '10.211.55.102 node2' &gt;&gt; /etc/hostsecho '10.211.55.102 node3' &gt;&gt; /etc/hosts 12345678# sshd.sh: centos/7下的ssh 默认没有开启PasswordAuthentication，所以单独使用这个脚本开启一下# 免去后面配置免密登录的时候，再去修改# open PasswordAuthenticationsed -i 's/#PasswordAuthentication yes/PasswordAuthentication yes/g' /etc/ssh/sshd_configsed -i 's/PasswordAuthentication no/#PasswordAuthentication yes/g' /etc/ssh/sshd_config# restart sshdservice sshd restart 解释： Vagrant的网络连接方式有三种： NAT : 缺省创建，用于让vm可以通过host转发访问局域网甚至互联网。 host-only : 只有主机可以访问vm，其他机器无法访问它。 bridge : 此模式下vm就像局域网中的一台独立的机器，可以被其他机器访问。 ​ 设置bridge模式的时候，需要手动指定 bridge，如：master.vm.network :public_network, :bridge=&gt;&#39;em1&#39;, ip: &quot;192.168.0.110&quot;，也可设置静态IP 根据vagrantfile的层次，分为： configure级：它定义在 Vagrant.configure(“2”) 的下一层次，形如： config.vm.provision … vm级：它定义在 config.vm.define :master do |master| 的下一层次，master.vm.provision … 执行的顺序是先执行configure级任务，再执行vm级任务，即便configure级任务在vm定义的下面才定义 Step 4：启动Vagrant 1vagrant up Step 5：连接vm 1vagrant ssh node0 Step 6：切换到root用户 12supassword默认是vagrant 至此，就可以使用vm了。 有一个非常有用的命令：vagrant rsync-auto。因为我们可能会经常性的修改共享文件夹，这个命令可以触发共享文件夹的同步，而不需要执行vagrant reload来同步共享文件夹，该命令会重启VM Vagrant常用命令vagrant help vagrant help 1vagrant help vagrant [command] -h 1vagrant box -h vagrant box 添加box 1vagrant box add --name centos-7.4-base centos-7.4-base.box 查看box 1vagrant box list 移除box 1vagrant box remove centos-7.4-base privision重新执行provision 1vagrant provision [vm-name] [--provision-with x,y,z] 执行指定的provision，使用—provision-with 参数，同时在配置Vagrant的时候，需要如下配置： 12# type关键字 必须显式写出master.vm.provision 'bootstrap_master', type: :shell, path: \"bootstrap_master.sh\" 那么只执行 bootstrap_master的命令为：vagrant provision master --provision-with bootstrap_master vagrant vm 初始化 如果自己编写Vagrantfile，不需要这一步 这一步会把config.vm.box写入 1vagrant init centos-7.4-base 启动虚拟机 1vagrant up 查看状态 1vagrant status ssh 1vagrant ssh 暂停虚拟机 1vagrant suspend 恢复虚拟机 1vagrant resume 关闭虚拟机 1vagrant halt 销毁虚拟机 1vagrant destroy 更新Vagrantfile，刷新容器 1vagrant reload vagrant snapshot使用虚拟机快照命令需要先启动虚拟机 保存虚拟机快照 1vagrant snapshot save snap1 list虚拟机快照 1vagrant snapshot list 恢复虚拟机快照 1vagrant snapshot restore snap1 删除虚拟机快照 1vagrant snapshot delete snap1","categories":[{"name":"虚拟机","slug":"虚拟机","permalink":"https://shang.at/categories/虚拟机/"}],"tags":[{"name":"Vagrant","slug":"Vagrant","permalink":"https://shang.at/tags/Vagrant/"}]},{"title":"大数据-消息队列-数据采集-Kafka","slug":"大数据-消息队列-数据采集-Kafka","date":"2020-06-20T03:14:01.000Z","updated":"2020-12-11T08:03:37.624Z","comments":true,"path":"post/大数据-消息队列-数据采集-Kafka/","link":"","permalink":"https://shang.at/post/大数据-消息队列-数据采集-Kafka/","excerpt":"","text":"Kafka的基本概念和架构Apache Kafka是Apache软件基金会的开源的流处理平台，该平台提供了消息的订阅与发布的消息队列，一般用作系统间解耦、异步通信、削峰填谷等作用 系统间解耦 将分支业务与主业务的耦合性解除：用户注册时只需要等待调用注册服务的时间，后续我们通知用户注册成功的时间是和主业务流程解耦的，发送短信的时间不在用户等待的范围内；同时短信服务发生了异常也不会影响到用户的注册服务 异步通信 上面的例子同样也解释了异步通信 削峰填谷 在一些数据产生比较大的场景，MQ可以在流计算前和入库前起到一个缓冲的作用 Message Queue消息队列是一种在分布式和大数据开发中不可或缺的中间件。在分布式开发或者大数据开发中通常使用消息队列进行缓冲、系统解耦和削峰填谷等业务场景，常见的消息队列工作模式大致会分为两大类： 至多一次：消息生产者将数据写入消息系统，然后由消费者负责去拉取消息服务器中的消息，一旦消息被确认消费后，由消息服务器主动删除队列中的数据，这种消费方式一般只允许被一个消费者消费，并且队列中的数据不允许被重复消费 没有限制[Kafka]：同上诉不同，生产者发布完数据之后，该消息可以被多个消费者同时消费，并且同一个消费者可以多次消费消息服务器中的同一个记录，主要是因为消息服务器一般可以长时间存储海量消息 Kafkakafka集群以Topic的形式负责分类集群中的Record，每一个Record属于一个Topic。每个Topic底层都会对应一组分区的日志用于持久化Topic中的Record。同时在kafka集群中，Topic的每一个日志分区都一定会有1个Broker担当该分区的Leader，其他Broker担当该分区的follower，Leader负责分区数据的读写操作。follower负责同步该分区的数据。这样如果分区的Leader宕机，该分区的其他follower会选取出新的leader继续负责该分区数据的读写。其中集群中的Leader的监控和Topic的部分元数据是存储在Zookeeper中 kafka逻辑上概念kafka中所有的消息时通过Topic为单位进行管理，每个kafka中的topic通常会有多个订阅者，负责订阅发送到该Topic中的数据。kafka负责管理集群中每个Topic的一组日志分区数据。 生产者将数据发布到相应的Topic。负责选择将哪个记录发送到Topic的哪个partition。例如，可以round-robin方式完成此操作，然而这种选择仅仅是为了数据负载。也可以根据某些语义分区功能(例如基于记录中的key)进行此操作 每组日志分区是一个有序的不可变的日志序列(分区有序，全局无序，但是可以通过组织key来保证消费的数据是有序的)，分区中的每一个Record都被分配了唯一的序列编号，称为offset，kafka集群会持久化所有发布到Topic中的Record信息，该Topic的持久化时间是通过配置文件执行的，默认是168小时 log.retention.hours=168 7*24 一周的缓存时间 kafka底层会定期的检查日志文件，然后将过期的数据从log中移除，由于kafka使用硬盘存储(零拷贝)日志文件，因此使用kafka长时间缓存一些日志文件是不存在问题的 在消费者消费Topic中数据的是时候，每个消费者会维护本次消费对应分区的偏移量，消费者会在消费完一个批次的数据之后，会将本次消费的偏移量提交给kafka集群，因此对于每个消费者而言，可以随意的控制消费者的偏移量。因此在kafka中消费者可以从一个Topic分区中的任意位置读取队列数据，由于每个消费者独立控制了自己的消费偏移量，因此多个消费者之间彼此相互独立。 kafka中对Topic实现日志分区有以下的目的： 首先，它们允许日志扩展到超出单个服务器所能容纳的大小。每个单独的分区都必须适合托管它的服务器，但是一个Topic可能存在很多分区，因此它可以处理任意数量的数据 —- 如果数据后期发生增长，直接增加分区数就可以吗? 其次，每个服务器充当其他某些分区的Leader，也可能充当其他分区的Follower，因此集群中负载得到了很好的平衡 逻辑概念 Producer：生产消息的客户端 Consumer：消费消息的客户端 Consumer Group：同一个ConsumerGroup中的Consumer往往是一个服务的多个实例，用来提高消费的效率，也就是说同一个CG中的多个C不能重复消费消息；不同CG往往代表了多种不同的服务，他们处理不同的业务，所以，不同的CG中的C对于消息的处理是相互独立的，如CG2中的C2可以重复的消费在CG1中的C1已经消费过的消息 消费者使用Consumer Group名称标记自己，并且发布到Topic的每条记录都会传递到每个订阅Consumer Group中的一个消费者实例。 如果所有Consumer实例都具有相同的Consumer Group，那么Topic中的记录会在该ConsumerGroup中的Consumer实例进行均分消费； 如果所有Consumer实例具有不同的ConsumerGroup，则每条记录将广播到所有Consumer Group进程。 更常见的是，我们发现Topic具有少量的Consumer Group，每个Consumer Group可以理解为一个“逻辑的订阅者”。每个Consumer Group均由许多Consumer实例组成，以实现可伸缩性和容错能力。这无非就是发布-订阅模型，其中订阅者是消费者的集群而不是单个进程。这种消费方式Kafka会将Topic按照分区的方式均分给一个Consumer Group下的实例，如果ConsumerGroup下有新的成员介入，则新介入的Consumer实例会去接管ConsumerGroup内其他消费者负责的某些分区，同样如果一下ConsumerGroup下的有其他Consumer实例宕机，则由该ConsumerGroup中其他Consumer实例接管。 消费者组内的消费者数量一般不能大于Topic中的分区数，如果大于了，那么多出来的消费者会分不到分区而消费不到消息 Tocpic：一组Record可以作为一个Topic在集群中被管理 Record：Producer生产的每一条消息就是一个Record，每一个Record只能属于一个Topic 由于Kafka的Topic的分区策略，因此Kafka仅提供分区中记录的有序性，也就意味着相同Topic的不同分区记录之间无顺序。因为针对于绝大多数的大数据应用和使用场景， 使用分区内部有序或者使用key进行分区策略已经足够满足绝大多数应用场景。但是，如果您需要记录全局有序，则可以通过只有一个分区Topic来实现，尽管这将意味着每个ConsumerGroup只有一个Consumer进程 Partition：每个Topic会有num.partitions(默认)个分区，每个Topic在创建的时候，也可以被指定分区的个数。 Kafka中对Topic实现日志分区的有以下目的： 支持集群存储的横向扩容。如果单一服务器的资源不够用，那么增加集群节点即可 每个服务器充当其某些分区的Leader，也可能充当其他分区的Follwer，因此群集中的负载得到了很好的平衡。 同一个Topic 多个分区可以提高Consumer消费的并行度 在kafka中同一个partition的record是严格有序的，但是不同partition的record并不是严格有序的。也就是说，kafka只能保证partition内部record的有序消费 Duplicate(副本)：每个分区会有--replication-factor个副本，是在Topic被创建的时候指定的 offset (非常重要的一个概念) — 待补充 在kafka中对于消息的生产和消费都是通过offset控制的。同一个partition的消息record的offset是递增的，消费者消费的时候，也是消费的指定的offset之后的消息。 消费者会定期的上传自己消费的offset给kafka server Segments — 待补充 架构上的概念 Broker：Kafka集群内的主机节点被称为broker，broker可能是某个partition的Leader，也可能是另外一个partition的follower Leader：kafka采用主从的架构，每个partition都有自己的leader。每个partition的leader负责消息的读写 Follower：partition的从节点，主要用来同步该分区的数据，当leader宕机之后，可以用来竞选leader ISR： kafka高性能之道-顺序写&amp;MMAPkafka的特性之一就是高吞吐量，但是kafka的消息是保存或缓存在磁盘上的，一般认为在磁盘上读写数据是会降低性能的，但是kafka即使是普通的服务器，也可以轻松支持百万/s级的写入请求，超出了大部分的消息中间件，这种特性也使得kafka在祺智处理等海量数据场景广泛应用，kafka会把收到的消息都顺序写入磁盘中，防止丢失数据。为了优化写入速度kafka采用了两个技术：顺序写入和MMFile 因为硬盘是机械机构，每次读写都会寻址-&gt;写入，其中寻址是一个机械操作，它是最耗时的，所以硬盘最讨厌随机IO，最喜欢顺序IO。为了提高读写硬盘的速度，kafka就是使用的顺序IO，这样省去了大量的内存开销以及节省了IO寻址的时间。但是单纯的使用顺序写入，kafka的写入性能也不可能和内存进行对比，因此kafka的数据并不是实时的写入硬盘的 kafka充分利用了现代操作系统分页存储的特性来利用内存提高IO效率。Memory Mapped Files也称为内存映射文件，在64位的操作系统中一般可以表示20G的数据文件，它的工作原理是直接利用操作系统的Page实现文件到物理内存的直接映射。完成MMP映射后，用户堆内存的所有操作会被操作系统自动刷新到磁盘上，极大地降低了IO使用率 kafka高性能之道-消费消息-零拷贝kafka服务器在响应客户端读取的时候，底层使用ZeroCopy技术，直接将磁盘无序拷贝到用户控件，而是直接将数据通过内核空间传递输出，数据并没有抵达用户空间 传统IO操作步骤 用户进程调用read等系统调用向操作系统发送IO请求，请求读取数据到自己的内存缓冲区中，自己进入阻塞状态 操作系统收到请求后，进一步将IO请求发送磁盘 磁盘驱动器收到内核IO请求，把数据从磁盘读取到驱动器的缓冲区，此时不占用CPU。当驱动器缓冲区被读满后，向内核发起中断信号告知自己缓冲区已满 内核收到中断，使用CPU时间将磁盘驱动器缓存中的数据拷贝到内核缓冲区。 如果内核缓冲区的数据少于用户申请的读取的数据，重复步骤3和4，直到内核缓冲区的数据足够多为止 将数据从内核缓冲区拷贝到用户缓冲区，同时从系统调用中返回，IO任务结束 DMA读取 - Direct Memory Access 直接存储器访问 用户进程调用read等系统调用向操作系统发送IO请求，请求读取数据到自己的内存缓冲区中，自己进入阻塞状态 操作系统收到请求后，进一步将IO请求发送给DMA，然后让CPU干别的活 DMA进一步将IO请求发送给磁盘 磁盘驱动器收到DMA的IO请求，把数据从磁盘读取到驱动器的缓存中，当驱动器的缓冲区被读满后，想DMA发起中断信号告知自己的缓冲区已满 DMA收到磁盘驱动器的新号，将磁盘驱动器的缓存中的数据拷贝到内核缓冲区。此时不占用CPU，这个时候如果内核缓冲区的数据少于用户申请的读取的数据，内核就会一直重复步骤3和4，知道内核缓冲区的数据足够多位置 当DMA读取了足够多的数据，就会发送中断信号给CPU CPU收到DMA的中断信号，知道数据已经准备好，于是将数据从内核空间拷贝到用户空间，系统调用返回 DMA与传统IO中断模式相比：DMA模式下，DMA就是CPU的一个代理，他负责了一部分的拷贝工作，从而减轻了CPU的负担，DMA的优点就是：中断少，CPU负担低 网络IO的两种方案对比： 一般方案 文件在磁盘中被拷贝到内核缓冲区 数据再从内核缓冲区拷贝到用户空间 数据再从用户空间拷贝到socket的相关缓冲区 数据从socket相关缓冲区拷贝到相关协议引擎发送出去 Zero拷贝 文件在磁盘中数据被拷贝到内核缓冲区 数据从内核缓冲区直接拷贝到socket相关缓冲区 数据从socket缓冲区拷贝到相关协议引擎发送出去 kafka的基本使用命令 启动kafka 1./bin/kafka-server-start.sh -daemon config/server.properties 关闭kafka 1./bin/kafka-server-stop.sh 创建topic 123456kafka-topics.sh \\--bootstrap-server node1:9092,node2:9092,node3:9092 \\--create \\--topic topic01 \\--partitions 3 \\--replication-factor 3 查看topic列表 123kafka-topics.sh \\--bootstrap-server node1:9092,node2:9092,node3:9092 \\--list 查看topic详情 1234kafka-topics.sh \\--bootstrap-server node1:9092,node2:9092,node3:9092 \\--describe \\--topic topic01 修改topic 123456789101112kafka-topics.sh \\--bootstrap-server node1:9092,node2:9092,node3:9092 \\--create \\--topic topic03 \\--partitions 1 \\--replication-factor 1kafka-topics.sh \\--bootstrap-server node1:9092,node2:9092,node3:9092 \\--alter \\--topic topic03 \\--partitions 2 删除topic 1234kafka-topics.sh \\--bootstrap-server node1:9092,node2:9092,node3:9092 \\--delete \\--topic topic03 消费者订阅topic 1234567kafka-console-consumer.sh \\--bootstrap-server node1:9092,node2:9092,node3:9092 \\--topic topic01 \\--group g1 \\--property print.key=true \\--property print.value=true \\--property key.separator=, 消费者组 12345678kafka-consumer-groups.sh \\--bootstrap-server node1:9092,node2:9092,node3:9092 \\--listkafka-consumer-groups.sh \\--bootstrap-server node1:9092,node2:9092,node3:9092 \\--describe \\--group g1 生产者生产消息 123kafka-console-producer.sh \\--broker-list node1:9092,node2:9092,node3:9092 \\--topic topic01 API基本API高级API架构进阶高性能分析之零拷贝&amp;源码分析数据同步机制kafka与其他软件的集成与Flume的集成与SpringBoot的集成在大数据流计算中的应用ZookeeperKafka中的leader监控和topic的元数据，都是存在zk中","categories":[{"name":"大数据生态","slug":"大数据生态","permalink":"https://shang.at/categories/大数据生态/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://shang.at/tags/Kafka/"}]},{"title":"大数据-知识点","slug":"大数据-知识点","date":"2020-06-20T02:46:39.000Z","updated":"2020-06-22T10:30:39.392Z","comments":true,"path":"post/大数据-知识点/","link":"","permalink":"https://shang.at/post/大数据-知识点/","excerpt":"","text":"生态管理员-Zookeeper 节点通信-RPC Hadoop 数据存储-HDFS 资源管理-YARN 任务调度-AppMaster：所有第三方的应用可以实现AppMaster，即可将任务跑在YARN上 数据采集-Flume 消息队列-数据采集-Kafka 数仓-Hive 数据存储-HBase 流批一体-Spark 流批一体-Flink 机器学习-Spark mlib 机器学习-FlinkML","categories":[{"name":"大数据生态","slug":"大数据生态","permalink":"https://shang.at/categories/大数据生态/"}],"tags":[{"name":"大数据生态","slug":"大数据生态","permalink":"https://shang.at/tags/大数据生态/"}]},{"title":"java学习-底层知识总结","slug":"java学习-底层知识总结","date":"2020-06-13T00:20:45.000Z","updated":"2020-06-13T02:18:33.907Z","comments":true,"path":"post/java学习-底层知识总结/","link":"","permalink":"https://shang.at/post/java学习-底层知识总结/","excerpt":"","text":"JVMJVM：Java Virtual Machine JMM：Java Memory Model 1：JVM基础知识 什么是JVM 常见的JVM 2：ClassFileFormat3：类编译-加载-初始化hashcode锁的信息（2位 四种组合）GC信息（年龄）如果是数组，数组的长度 4：JMMnew Cat()pointer -&gt; Cat.class寻找方法的信息 5：对象1：句柄池 （指针池）间接指针，节省内存2：直接指针，访问速度快 6：GC基础知识栈上分配TLAB（Thread Local Allocation Buffer）OldEden老不死 - &gt; Old 7：GC常用垃圾回收器new Object()markword 8个字节类型指针 8个字节实例变量 0补齐 016字节（压缩 非压缩）Object o8个字节JVM参数指定压缩或非压缩","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"java底层","slug":"java底层","permalink":"https://shang.at/tags/java底层/"}]},{"title":"java学习-基础1-class文件格式","slug":"java学习-基础1-class文件格式","date":"2020-06-13T00:12:38.000Z","updated":"2020-12-11T08:20:27.279Z","comments":true,"path":"post/java学习-基础1-class文件格式/","link":"","permalink":"https://shang.at/post/java学习-基础1-class文件格式/","excerpt":"","text":"什么是JVM？ jvm与java没有关系，任何能翻译成class文件的语言，都能运行在jvm上 JDK JRE JVM Class File Format样例代码：非常简单，只有一个类型的声明 1234package com.mashibing.jvm.c1_bytecode;public class T0100_ByteCode01 &#123;&#125; 编译完之后，在用IDEA打开：会发现，被自动加了一个无参的构造方法 1234567891011//// Source code recreated from a .class file by IntelliJ IDEA// (powered by Fernflower decompiler)//package com.mashibing.jvm.c1_bytecode;public class T0100_ByteCode01 &#123; public T0100_ByteCode01() &#123; &#125;&#125; 然后我们使用一种能查看二进制文件的软件打开该class文件，(我这里用的是sublime)。会得到如下的16进制显示： 123456789101112131415161718192021cafe babe 0000 0034 0010 0a00 0300 0d07000e 0700 0f01 0006 3c69 6e69 743e 01000328 2956 0100 0443 6f64 6501 000f 4c696e65 4e75 6d62 6572 5461 626c 6501 00124c6f 6361 6c56 6172 6961 626c 6554 61626c65 0100 0474 6869 7301 0030 4c63 6f6d2f6d 6173 6869 6269 6e67 2f6a 766d 2f63315f 6279 7465 636f 6465 2f54 3031 30305f42 7974 6543 6f64 6530 313b 0100 0a536f75 7263 6546 696c 6501 0015 5430 3130305f 4279 7465 436f 6465 3031 2e6a 6176610c 0004 0005 0100 2e63 6f6d 2f6d 61736869 6269 6e67 2f6a 766d 2f63 315f 62797465 636f 6465 2f54 3031 3030 5f42 79746543 6f64 6530 3101 0010 6a61 7661 2f6c616e 672f 4f62 6a65 6374 0021 0002 00030000 0000 0001 0001 0004 0005 0001 00060000 002f 0001 0001 0000 0005 2ab7 0001b100 0000 0200 0700 0000 0600 0100 00000300 0800 0000 0c00 0100 0000 0500 09000a00 0000 0100 0b00 0000 0200 0c 结构如下：class文件就是按照如下的顺序紧凑的拼在一起的 Magic Number 4字节 魔数 可以认为是文件类型：以cafe babe开头的文件则为class类型的文件 class文件的前4个字节：cafe babe Minor Version 2字节 小版本：0000 小数点后的数值 Major Version 2字节 大版本：0034 小数点前的数值 class文件的版本号： JDK1.7默认为51.0、JDK1.8默认为52.0 constant_pool_count 2字节 常量池大小 constant_pool：长度为constan_pool_count-1的表 根据常量池大小 不是固定的 常量池表 常量池序号从#1开始 常量池的每种元素占用的字节数可能是不一样的，但是每个元素的第一个字节表示的是当前元素的类型(tag)，然后再根据当前元素的类型可以得到该元素占用的字节数 一个元素读完之后立即跟着下一个元素，中间没有空闲 常量池中每种元素对应的含义 1 CONSTANT_Utf8_info tag: 1 Length：utf8字符串占用的字节数 bytes：长度为length的字符串 3 7 CONSTANT_Class_info tag: 7 Index：2字节 类的全限定名的 常量池索引 全限定名例子如下：com/mashibing/jvm/c1_bytecode/T0100_ByteCode01 8 CONSTANT_String_info tag：8 index：2字节 指向字符串字面量的 常量池索引 10 CONSTANT_Methodref_info Tag：10 index：2字节 指向声明方法的类或者接口描述符CONSTANT_Class_info的常量池索引 index：2字节 指向字段描述符CONSTANT_NameAndType的常量池索引 12 CONSTANT_NameAndType_info tag：12 Index：2字节 指向该字段或方法名称的 常量池索引 index：2字节 指向该字段或方法描述符的 常量池索引 access_flags：访问标记 2字节 ACC_PUBLIC 0x0001 是否为public类型 ACC_FINAL 0x0010 是否为final ACC_SUPER 0x0020 该标志必须为真，JDK1.0.2之后编译出来的内容必须为真，指明invokespecial指令使用新语义 ACC_INTERFACE 0x0200 是否为接口 ACC_ABSTRACT 0x0400 是否为抽象类 ACC_SYNTHETIC 0x1000编译器自动生成，非用户代码 ACC_ANNOTATION 0x2000 是否为注解 ACC_ENUM 0x4000 是否为枚举类型 this_class：当前类的名称 2字节 index：2字节 指向类型描述符CONSTANT_Class_info的常量池索引 super_class：父类的名称 2字节 index：2字节 指向类型描述符CONSTANT_Class_info的常量池索引 interfaces_count：实现的接口数量 2字节 interfaces 实现的具体接口列表 fields_count：字段数量 2字节 fields：字段列表 access_flags 2字节 ACC_PUBLIC 0x0001 ACC_PRIVATE 0x0002 ACC_PROTECTED 0x0004 ACC_STATIC 0x0008 ACC_FINAL 0x0010 ACC_VOLATILE 0x0040 ACC_TRANSIENT 0x0080 ACC_SYNTHETIC 0x1000 ACC_ENUM 0x4000 name_index 2字节 decriptor_index 2字节 B byte C char D double F float I int J long S short Z boolean V void L Object ： Lcom/mashibing/jvm/Test [ 数组 一维 [B [Ljava/lang/String 二维 [[C [[Ljava/lang/String attribubtes_count 2字节 attributes methods_count：方法数量 2字节 methods：方法列表 access_flag 2字节 name_index 2字节 descriptor_index 2字节 attribute_count 2字节 attributes attributes_count - u2 附加属性数量 attributes：附加属性列表 既有预定义的属性，也可以自定义，java虚拟机自动忽略不认识的属性 code - 方法表 - 该方法编译的字节码指令 ConstantValue - 字段表 - final关键字定义的常量值 Deprecated - 类、方法表、字段表 Exceptions - 方法表 EnclosingMethod - 类文件 - 局部类或匿名类的外部封装方法 InnerClass - 类文件 - 内部类列表 LineNumberTable - Code属性 - java源码的行号与字节码指令的对应关系 LocalVariableTable - Code属性 - 方法局部变量表 SourceFile - 类文件 - 源文件名","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"class文件格式","slug":"class文件格式","permalink":"https://shang.at/tags/class文件格式/"}]},{"title":"Python学习-metaclass实现单例","slug":"Python学习-metaclass实现单例","date":"2020-06-12T01:35:16.000Z","updated":"2020-11-26T02:27:38.986Z","comments":true,"path":"post/Python学习-metaclass实现单例/","link":"","permalink":"https://shang.at/post/Python学习-metaclass实现单例/","excerpt":"","text":"单例模式单例模式（Singleton Pattern）是一种常用的软件设计模式，该模式的主要目的是确保某一个类只有一个实例存在。当你希望在整个系统中，某个类只能出现一个实例时，单例对象就能派上用场。 比如，某个服务器程序的配置信息存放在一个文件中，客户端通过一个 AppConfig 的类来读取配置文件的信息。如果在程序运行期间，有很多地方都需要使用配置文件的内容，也就是说，很多地方都需要创建 AppConfig 对象的实例，这就导致系统中存在多个 AppConfig 的实例对象，而这样会严重浪费内存资源，尤其是在配置文件内容很多的情况下。事实上，类似 AppConfig 这样的类，我们希望在程序运行期间只存在一个实例对象。 在 Python 中，我们可以用多种方法来实现单例模式 实现单例模式的几种方式1.使用模块其实，Python 的模块就是天然的单例模式，因为模块在第一次导入时，会生成 .pyc 文件，当第二次导入时，就会直接加载 .pyc 文件，而不会再次执行模块代码。因此，我们只需把相关的函数和数据定义在一个模块中，就可以获得一个单例对象了。如果我们真的想要一个单例类，可以考虑这样做： mysingleton.py 1234class Singleton(object): def foo(self): passsingleton = Singleton() 将上面的代码保存在文件 mysingleton.py 中，要使用时，直接在其他文件中导入此文件中的对象，这个对象即是单例模式的对象 1from a import singleton 2.使用装饰器123456789101112131415161718192021def Singleton(cls): _instance = &#123;&#125; def _singleton(*args, **kargs): if cls not in _instance: _instance[cls] = cls(*args, **kargs) return _instance[cls] return _singleton@Singletonclass A(object): a = 1 def __init__(self, x=0): self.x = xa1 = A(2)a2 = A(3) 3.使用类12345678910class Singleton(object): def __init__(self): pass @classmethod def instance(cls, *args, **kwargs): if not hasattr(Singleton, \"_instance\"): Singleton._instance = Singleton(*args, **kwargs) return Singleton._instance 一般情况，大家以为这样就完成了单例模式，但是这样当使用多线程时会存在问题 1234567891011121314151617181920class Singleton(object): def __init__(self): pass @classmethod def instance(cls, *args, **kwargs): if not hasattr(Singleton, \"_instance\"): Singleton._instance = Singleton(*args, **kwargs) return Singleton._instanceimport threadingdef task(arg): obj = Singleton.instance() print(obj)for i in range(10): t = threading.Thread(target=task,args=[i,]) t.start() 程序执行后，打印结果如下： 12345678910&lt;__main__.Singleton object at 0x02C933D0&gt;&lt;__main__.Singleton object at 0x02C933D0&gt;&lt;__main__.Singleton object at 0x02C933D0&gt;&lt;__main__.Singleton object at 0x02C933D0&gt;&lt;__main__.Singleton object at 0x02C933D0&gt;&lt;__main__.Singleton object at 0x02C933D0&gt;&lt;__main__.Singleton object at 0x02C933D0&gt;&lt;__main__.Singleton object at 0x02C933D0&gt;&lt;__main__.Singleton object at 0x02C933D0&gt;&lt;__main__.Singleton object at 0x02C933D0&gt; 看起来也没有问题，那是因为执行速度过快，如果在init方法中有一些IO操作，就会发现问题了，下面我们通过time.sleep模拟 我们在上面init方法中加入以下代码： 123def __init__(self): import time time.sleep(1) 重新执行程序后，结果如下 12345678910&lt;__main__.Singleton object at 0x034A3410&gt;&lt;__main__.Singleton object at 0x034BB990&gt;&lt;__main__.Singleton object at 0x034BB910&gt;&lt;__main__.Singleton object at 0x034ADED0&gt;&lt;__main__.Singleton object at 0x034E6BD0&gt;&lt;__main__.Singleton object at 0x034E6C10&gt;&lt;__main__.Singleton object at 0x034E6B90&gt;&lt;__main__.Singleton object at 0x034BBA30&gt;&lt;__main__.Singleton object at 0x034F6B90&gt;&lt;__main__.Singleton object at 0x034E6A90&gt; 问题出现了！按照以上方式创建的单例，无法支持多线程 解决办法：加锁！未加锁部分并发执行,加锁部分串行执行,速度降低,但是保证了数据安全 12345678910111213141516171819202122232425import timeimport threadingclass Singleton(object): _instance_lock = threading.Lock() def __init__(self): time.sleep(1) @classmethod def instance(cls, *args, **kwargs): with Singleton._instance_lock: if not hasattr(Singleton, \"_instance\"): Singleton._instance = Singleton(*args, **kwargs) return Singleton._instancedef task(arg): obj = Singleton.instance() print(obj)for i in range(10): t = threading.Thread(target=task,args=[i,]) t.start()time.sleep(20)obj = Singleton.instance()print(obj) 打印结果如下： 12345678910&lt;__main__.Singleton object at 0x02D6B110&gt;&lt;__main__.Singleton object at 0x02D6B110&gt;&lt;__main__.Singleton object at 0x02D6B110&gt;&lt;__main__.Singleton object at 0x02D6B110&gt;&lt;__main__.Singleton object at 0x02D6B110&gt;&lt;__main__.Singleton object at 0x02D6B110&gt;&lt;__main__.Singleton object at 0x02D6B110&gt;&lt;__main__.Singleton object at 0x02D6B110&gt;&lt;__main__.Singleton object at 0x02D6B110&gt;&lt;__main__.Singleton object at 0x02D6B110&gt; 这样就差不多了，但是还是有一点小问题，就是当程序执行时，执行了time.sleep(20)后，下面实例化对象时，此时已经是单例模式了，但我们还是加了锁，这样不太好，再进行一些优化，把intance方法，改成下面的这样就行： 1234567@classmethoddef instance(cls, *args, **kwargs): if not hasattr(Singleton, \"_instance\"): with Singleton._instance_lock: if not hasattr(Singleton, \"_instance\"): Singleton._instance = Singleton(*args, **kwargs) return Singleton._instance 这样，一个可以支持多线程的单例模式就完成了。 这也就是大名鼎鼎的DCL 1234567891011121314151617181920212223242526import timeimport threadingclass Singleton(object): _instance_lock = threading.Lock() def __init__(self): time.sleep(1) @classmethod def instance(cls, *args, **kwargs): if not hasattr(Singleton, \"_instance\"): with Singleton._instance_lock: if not hasattr(Singleton, \"_instance\"): Singleton._instance = Singleton(*args, **kwargs) return Singleton._instancedef task(arg): obj = Singleton.instance() print(obj)for i in range(10): t = threading.Thread(target=task,args=[i,]) t.start()time.sleep(20)obj = Singleton.instance()print(obj) 这种方式实现的单例模式，使用时会有限制，以后实例化必须通过 obj = Singleton.instance() 如果用 obj=Singleton() ,这种方式得到的不是单例 4.基于new方法实现（推荐使用，方便）通过上面例子，我们可以知道，当我们实现单例时，为了保证线程安全需要在内部加入锁 我们知道，当我们实例化一个对象时，是先执行了类的new方法（我们没写时，默认调用object.new），实例化对象；然后再执行类的init方法，对这个对象进行初始化，所有我们可以基于这个，实现单例模式 1234567891011121314151617181920212223242526import threadingclass Singleton(object): _instance_lock = threading.Lock() def __init__(self): pass def __new__(cls, *args, **kwargs): if not hasattr(Singleton, \"_instance\"): with Singleton._instance_lock: if not hasattr(Singleton, \"_instance\"): Singleton._instance = object.__new__(cls) return Singleton._instanceobj1 = Singleton()obj2 = Singleton()print(obj1,obj2)def task(arg): obj = Singleton() print(obj)for i in range(10): t = threading.Thread(target=task,args=[i,]) t.start() 打印结果如下： 1234567891011&lt;__main__.Singleton object at 0x038B33D0&gt; &lt;__main__.Singleton object at 0x038B33D0&gt;&lt;__main__.Singleton object at 0x038B33D0&gt;&lt;__main__.Singleton object at 0x038B33D0&gt;&lt;__main__.Singleton object at 0x038B33D0&gt;&lt;__main__.Singleton object at 0x038B33D0&gt;&lt;__main__.Singleton object at 0x038B33D0&gt;&lt;__main__.Singleton object at 0x038B33D0&gt;&lt;__main__.Singleton object at 0x038B33D0&gt;&lt;__main__.Singleton object at 0x038B33D0&gt;&lt;__main__.Singleton object at 0x038B33D0&gt;&lt;__main__.Singleton object at 0x038B33D0&gt; 采用这种方式的单例模式，以后实例化对象时，和平时实例化对象的方法一样 obj = Singleton() 5.基于metaclass方式实现相关知识 1234\"\"\"1.类由type创建，创建类时，type的__init__方法自动执行，类() 执行type的 __call__方法(类的__new__方法,类的__init__方法)2.对象由类创建，创建对象时，类的__init__方法自动执行，对象()执行类的 __call__ 方法\"\"\" 例子： 1234567891011class Foo: def __init__(self): pass def __call__(self, *args, **kwargs): passobj = Foo()# 执行type的 __call__ 方法，调用 Foo类（是type的对象）的 __new__方法，用于创建对象，然后调用 Foo类（是type的对象）的 __init__方法，用于对对象初始化。obj() # 执行Foo的 __call__ 方法 元类的使用1234567891011121314151617class SingletonType(type): def __init__(self,*args,**kwargs): super(SingletonType,self).__init__(*args,**kwargs) def __call__(cls, *args, **kwargs): # 这里的cls，即Foo类 print('cls',cls) obj = cls.__new__(cls,*args, **kwargs) cls.__init__(obj,*args, **kwargs) # Foo.__init__(obj) return objclass Foo(metaclass=SingletonType): # 指定创建Foo的type为SingletonType def __init__(self，name): self.name = name def __new__(cls, *args, **kwargs): return object.__new__(cls)obj = Foo('xx') 实现单例模式12345678910111213141516171819import threadingclass SingletonType(type): _instance_lock = threading.Lock() def __call__(cls, *args, **kwargs): if not hasattr(cls, \"_instance\"): with SingletonType._instance_lock: if not hasattr(cls, \"_instance\"): cls._instance = super(SingletonType,cls).__call__(*args, **kwargs) return cls._instanceclass Foo(metaclass=SingletonType): def __init__(self,name): self.name = nameobj1 = Foo('name')obj2 = Foo('name')print(obj1,obj2) 对于Lock对象而言，如果一个线程连续两次release，使得线程死锁。所以Lock不常用，一般采用Rlock进行线程锁的设定。 廖雪峰-mateclass","categories":[],"tags":[]},{"title":"java学习-运行时线程","slug":"java学习-运行时线程","date":"2020-06-11T03:40:56.000Z","updated":"2020-06-11T03:52:13.279Z","comments":true,"path":"post/java学习-运行时线程/","link":"","permalink":"https://shang.at/post/java学习-运行时线程/","excerpt":"","text":"一个Java进程启动之后，至少会创建以下几个线程： main Finalizer Reference Handler Signal Dispatcher Others: 用户自己创建的线程","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"运行时线程","slug":"运行时线程","permalink":"https://shang.at/tags/运行时线程/"}]},{"title":"数据结构与算法学习笔记-JOIN的算法实现","slug":"数据结构与算法学习笔记-JOIN的算法实现","date":"2020-06-10T06:40:09.000Z","updated":"2020-07-13T13:33:48.925Z","comments":true,"path":"post/数据结构与算法学习笔记-JOIN的算法实现/","link":"","permalink":"https://shang.at/post/数据结构与算法学习笔记-JOIN的算法实现/","excerpt":"https://www.infoq.cn/article/6XGx92FyQ45cMXpj2mgZ","text":"https://www.infoq.cn/article/6XGx92FyQ45cMXpj2mgZ JOIN(INNER JOIN) 内连接：左右两个集合的交集，只保留左右两个集合都有的元素 内连接的时候会先排序(O($NlogN$))，然后关联两个有序集合(O($N$)) 12 LEFT JOIN 左外连接：以左集合为主，右集合不存在的元素补null RIGHT JOIN 右外连接：以右集合为主，左集合不存在的元素补null FULL JOIN 全连接：左右两个集合的全集，其中 左集合有但是右集合不存在的元素，右集合对应的字段补null，反之亦然 left_semi 过滤出左集合中和右集合共有的部分，只返回左集合中的rows left_anti 过滤出左集合中 右集合没有的部分，只返回左集合中的rows","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"}],"tags":[{"name":"JOIN的算法实现","slug":"JOIN的算法实现","permalink":"https://shang.at/tags/JOIN的算法实现/"}]},{"title":"java学习-JDK环境切换","slug":"java学习-JDK环境切换","date":"2020-06-10T05:40:50.000Z","updated":"2020-06-10T05:47:55.073Z","comments":true,"path":"post/java学习-JDK环境切换/","link":"","permalink":"https://shang.at/post/java学习-JDK环境切换/","excerpt":"","text":"近期，JDK版本更新十分频繁，如果要想快速切换JDK版本，可以通过linux的alias命令来简单实现： 12345678910111213141516# mac环境# ~/.bash_profileexport JAVA_8_HOME=$(/usr/libexec/java_home -v 1.8)export JAVA_9_HOME=$(/usr/libexec/java_home -v 9)export JAVA_10_HOME=$(/usr/libexec/java_home -v 10)export JAVA_11_HOME=$(/usr/libexec/java_home -v 11)export JAVA_HOME=$JAVA_8_HOMEexport PATH=$JAVA_HOME/bin:$PATH# ~/.zshrc 注意要放到~/.zshrc文件的最下面# multi jdk configalias jdk8=\"export PATH=$JAVA_8_HOME/bin:$PATH\"alias jdk9=\"export PATH=$JAVA_9_HOME/bin:$PATH\"alias jdk10=\"export PATH=$JAVA_10_HOME/bin:$PATH\"alias jdk11=\"export PATH=$JAVA_11_HOME/bin:$PATH\" 默认环境为 1234➜ ~ java -versionjava version \"1.8.0_144\"Java(TM) SE Runtime Environment (build 1.8.0_144-b01)Java HotSpot(TM) 64-Bit Server VM (build 25.144-b01, mixed mode) 切换jdk9之后 12345➜ ~ jdk9➜ ~ java -versionjava version \"9.0.4\"Java(TM) SE Runtime Environment (build 9.0.4+11)Java HotSpot(TM) 64-Bit Server VM (build 9.0.4+11, mixed mode) 这样就可以在当前的terminal session中使用jdk9的新特性了，比如jshell","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"JDK环境切换","slug":"JDK环境切换","permalink":"https://shang.at/tags/JDK环境切换/"}]},{"title":"java学习-JVM虚拟机栈","slug":"java学习-JVM虚拟机栈","date":"2020-06-09T07:39:37.000Z","updated":"2020-06-09T07:40:05.128Z","comments":true,"path":"post/java学习-JVM虚拟机栈/","link":"","permalink":"https://shang.at/post/java学习-JVM虚拟机栈/","excerpt":"","text":"","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"JVM虚拟机栈","slug":"JVM虚拟机栈","permalink":"https://shang.at/tags/JVM虚拟机栈/"}]},{"title":"java学习-JVM疑问","slug":"java学习-JVM疑问","date":"2020-06-05T09:01:58.000Z","updated":"2020-06-09T07:36:50.916Z","comments":true,"path":"post/java学习-JVM疑问/","link":"","permalink":"https://shang.at/post/java学习-JVM疑问/","excerpt":"","text":"在学习JVM的内存模型的时候，我有这样一些疑惑： 1、我们通常只是定义了堆大小(-Xms初始，-Xmx最大)，虚拟机栈大小(-Xss)。但是我发现这并不能计算出一个java进程占用的全部内存大小。以下是我自己理解的(JDK1.8)：java进程占用的内存​ =JVM管理的内存+非JVM管理的内存​ =线程独立的内存+线程共享的内存​ =n*(虚拟机栈内存+程序计数器内存+本地方法栈内存)+堆内存(heap)+非堆内存(non-heap)+元空间(metaspace)+堆外内存(off-heap:direct memory)其中：​ JVM管理的内存：n*(虚拟机栈内存+程序计数器内存+本地方法栈内存)+堆内存(heap)+非堆内存(non-heap)​ 非JVM管理的内存：元空间(metaspace)+堆外内存(off-heap:direct memory)​ 线程独立的内存：n*(虚拟机栈内存+程序计数器内存+本地方法栈内存)，n是线程数​ 线程共享的内存：堆内存(heap)+非堆内存(non-heap)+元空间(metaspace)+对外内存(off-heap:direct memory) 2、在JDK1.7及以前，有个永久代(PermGen)，也就是文中说的方法区。这块区域也被称为非堆内存​ 那么在JDK1.8及以后，永久代变成了元空间，到了JVM管理之外了，那么JDK1.8及以后的版本中还有非堆内存(non-heap)的说法吗？如果有的话，是指什么呢？ 3、关于线程独立的这块内存\\{n*(虚拟机栈内存+程序计数器内存+本地方法栈内存)，n是线程数}，它是完全独立于其他的内存的吗？​ 还是会分享堆内存，受到堆内存大小的限制​ 还是说Thread对象是建立在堆内存，然后每个Thread对应的虚拟机栈都是独立的吗？ 换句话说，随着Thread的增加(堆内存充足：还能给新的对象分配内存)，java进程占用的内存会越来越大——-我觉得这肯定不对，但是我却无法解释 4、我做了一些测试(JDK1.8)：​ 4.1、指定很小的堆内存，改变虚拟机栈大小​ 4.1.1、-Xms2m -Xmx2m -Xss16m 启动java进程，直到递归调用1,016,085深度，会报StackOverflowError​ 4.1.2、-Xms2m -Xmx2m -Xss8m 启动java进程，直到递归调用318,031深度，会报StackOverflowError​ 4.2、指定很小的堆内存，如-Xms2m -Xmx2m，最终会报OutOfMemoryError 我谈一下我的理解，首先，新创建的线程对象肯定是放在堆中的；每个线程独立的虚拟机栈，存放了很多的栈帧，每个栈帧实际上存放了局部变量表(和其他三部分)，每个栈帧对应了一个函数调用，在这个线程中执行的每个函数中的变量可能会存放在堆里面，也有可能会直接在栈上分配内存，因为这里有一个逃逸的概念(仅函数内部使用的局部变量直接在栈上分配内存，不会占用堆内存)。 所以在我的理解里面，如果虚拟机栈是一个完全独立于堆的内存，那么虚拟机栈就不会受到堆内存大小的限制(比如我上面做的实验：当堆内存远小于虚拟机栈大小，最终报的异常仍然是StackOverflowError，而不是OutOfMemoryError) 所以我才会想到，如果虚拟机栈是一个完全独立于堆的内存，无限的创建线程，每个线程的虚拟机栈如果都无限接近于-Xss分配的最大限度，那么最终会耗尽系统的所有 内存吧 我又做了一个实验，就是无限的创建线程，然后在调用start()的时候，报了OutOfMemoryError:unable to create new native thread的异常，看来操作系统在这里是对线程数是有限制的。","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"JVM疑问","slug":"JVM疑问","permalink":"https://shang.at/tags/JVM疑问/"}]},{"title":"java学习-基本类型和包装类型","slug":"java学习-基本类型和包装类型","date":"2020-06-04T03:37:10.000Z","updated":"2020-12-11T08:19:27.726Z","comments":true,"path":"post/java学习-基本类型和包装类型/","link":"","permalink":"https://shang.at/post/java学习-基本类型和包装类型/","excerpt":"","text":"Java 的每个基本类型都对应了一个包装类型，比如说 int 的包装类型为 Integer，double 的包装类型为 Double。基本类型和包装类型的区别主要有以下 4 点。 01、包装类型可以为 null，而基本类型不可以别小看这一点区别，它使得包装类型可以应用于 POJO 中，而基本类型则不行。 POJO 是什么呢？这里稍微说明一下。 POJO 的英文全称是 Plain Ordinary Java Object，翻译一下就是，简单无规则的 Java 对象，只有属性字段以及 setter 和 getter 方法，示例如下。 1234567891011121314151617181920class Writer &#123;private Integer age;private String name;public Integer getAge() &#123;return age;&#125;public void setAge(Integer age) &#123;this.age = age;&#125;public String getName() &#123;return name;&#125;public void setName(String name) &#123;this.name = name;&#125;&#125; 和 POJO 类似的，还有数据传输对象 DTO（Data Transfer Object，泛指用于展示层与服务层之间的数据传输对象）、视图对象 VO（View Object，把某个页面的数据封装起来）、持久化对象 PO（Persistant Object，可以看成是与数据库中的表映射的 Java 对象）。 那为什么 POJO 的属性必须要用包装类型呢？ 《阿里巴巴 Java 开发手册》上有详细的说明，我们来大声朗读一下（预备，起）。 数据库的查询结果可能是 null，如果使用基本类型的话，因为要自动拆箱（将包装类型转为基本类型，比如说把 Integer 对象转换成 int 值），就会抛出 NullPointerException 的异常。 02、包装类型可用于泛型，而基本类型不可以泛型不能使用基本类型，因为使用基本类型时会编译出错。 12List&lt;int&gt; list = new ArrayList&lt;&gt;(); // 提示 Syntax error, insert &quot;Dimensions&quot; to complete ReferenceTypeList&lt;Integer&gt; list = new ArrayList&lt;&gt;(); 为什么呢？因为泛型在编译时会进行类型擦除，最后只保留原始类型，而原始类型只能是 Object 类及其子类——基本类型是个特例。 03、基本类型比包装类型更高效基本类型在栈中直接存储的具体数值，而包装类型则存储的是堆中的引用。 很显然，相比较于基本类型而言，包装类型需要占用更多的内存空间。假如没有基本类型的话，对于数值这类经常使用到的数据来说，每次都要通过 new 一个包装类型就显得非常笨重。 03、两个包装类型的值可以相同，但却不相等两个包装类型的值可以相同，但却不相等——这句话怎么理解呢？来看一段代码就明明白白了。 12345Integer chenmo = new Integer(10);Integer wanger = new Integer(10);System.out.println(chenmo == wanger); // falseSystem.out.println(chenmo.equals(wanger )); // true 两个包装类型在使用“==”进行判断的时候，判断的是其指向的地址是否相等。chenmo 和 wanger 两个变量使用了 new 关键字，导致它们在“==”的时候输出了 false。 而 chenmo.equals(wanger) 的输出结果为 true，是因为 equals 方法内部比较的是两个 int 值是否相等。源码如下。 1234567891011private final int value;public int intValue() &#123;return value;&#125;public boolean equals(Object obj) &#123;if (obj instanceof Integer) &#123;return value == ((Integer)obj).intValue();&#125;return false;&#125; 瞧，虽然 chenmo 和 wanger 的值都是 10，但他们并不相等。换句话说就是：将“==”操作符应用于包装类型比较的时候，其结果很可能会和预期的不符。 04、自动装箱和自动拆箱既然有了基本类型和包装类型，肯定有些时候要在它们之间进行转换。把基本类型转换成包装类型的过程叫做装箱（boxing）。反之，把包装类型转换成基本类型的过程叫做拆箱（unboxing）。 在 Java SE5 之前，开发人员要手动进行装拆箱，比如说： 12Integer chenmo = new Integer(10); // 手动装箱int wanger = chenmo.intValue(); // 手动拆箱 Java SE5 为了减少开发人员的工作，提供了自动装箱与自动拆箱的功能。 12Integer chenmo = 10; // 自动装箱int wanger = chenmo; // 自动拆箱 上面这段代码使用 JAD 反编译后的结果如下所示： 12Integer chenmo = Integer.valueOf(10);int wanger = chenmo.intValue(); 也就是说，自动装箱是通过 Integer.valueOf() 完成的；自动拆箱是通过 Integer.intValue() 完成的。理解了原理之后，我们再来看一道老马当年给我出的面试题。 1234567891011121314// 1）基本类型和包装类型int a = 100;Integer b = 100;System.out.println(a == b);// 2）两个包装类型Integer c = 100;Integer d = 100;System.out.println(c == d);// 3）c = 200;d = 200;System.out.println(c == d); 答案是什么呢？有举手要回答的吗？答对的奖励一朵小红花哦。 第一段代码，基本类型和包装类型进行 == 比较，这时候 b 会自动拆箱，直接和 a 比较值，所以结果为 true。 第二段代码，两个包装类型都被赋值为了 100，这时候会进行自动装箱，那 == 的结果会是什么呢？ 我们之前的结论是：将“==”操作符应用于包装类型比较的时候，其结果很可能会和预期的不符。那结果是 false？但这次的结果却是 true，是不是感觉很意外？ 第三段代码，两个包装类型重新被赋值为了 200，这时候仍然会进行自动装箱，那 == 的结果会是什么呢？ 吃了第二段代码的亏后，是不是有点怀疑人生了，这次结果是 true 还是 false 呢？扔个硬币吧，哈哈。我先告诉你结果吧，false。 为什么？为什么？为什么呢？ 事情到了这一步，必须使出杀手锏了——分析源码吧。 之前我们已经知道了，自动装箱是通过 Integer.valueOf() 完成的，那我们就来看看这个方法的源码吧。 12345public static Integer valueOf(int i) &#123;if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high)return IntegerCache.cache[i + (-IntegerCache.low)];return new Integer(i);&#125; 难不成是 IntegerCache 在作怪？你猜对了！ 12345678910111213141516171819202122private static class IntegerCache &#123;static final int low = -128;static final int high;static final Integer cache[];static &#123;// high value may be configured by propertyint h = 127;int i = parseInt(integerCacheHighPropValue);i = Math.max(i, 127);h = Math.min(i, Integer.MAX_VALUE - (-low) -1);high = h;cache = new Integer[(high - low) + 1];int j = low;for(int k = 0; k &lt; cache.length; k++)cache[k] = new Integer(j++);// range [-128, 127] must be interned (JLS7 5.1.7)assert IntegerCache.high &gt;= 127;&#125;&#125; 大致瞟一下这段代码你就全明白了。-128 到 127 之间的数会从 IntegerCache 中取，然后比较，所以第二段代码（100 在这个范围之内）的结果是 true，而第三段代码（200 不在这个范围之内，所以 new 出来了两个 Integer 对象）的结果是 false。 看完上面的分析之后，我希望大家记住一点：当需要进行自动装箱时，如果数字在 -128 至 127 之间时，会直接使用缓存中的对象，而不是重新创建一个对象。 自动装拆箱是一个很好的功能，大大节省了我们开发人员的精力，但也会引发一些麻烦，比如下面这段代码，性能就很差。 1234567long t1 = System.currentTimeMillis();Long sum = 0L;for (int i = 0; i &lt; Integer.MAX_VALUE;i++) &#123; sum += i;&#125;long t2 = System.currentTimeMillis();System.out.println(t2-t1); sum 由于被声明成了包装类型 Long 而不是基本类型 long，所以 sum += i 进行了大量的拆装箱操作（sum 先拆箱和 i 相加，然后再装箱赋值给 sum），导致这段代码运行完花费的时间足足有 2986 毫秒；如果把 sum 换成基本类型 long，时间就仅有 554 毫秒，完全不一个等量级啊。","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"基本类型和包装类型","slug":"基本类型和包装类型","permalink":"https://shang.at/tags/基本类型和包装类型/"}]},{"title":"java学习-JVM-heap-non-heap-off-heap","slug":"java学习-JVM-heap-non-heap-off-heap","date":"2020-06-03T22:50:26.000Z","updated":"2020-06-03T22:50:26.253Z","comments":true,"path":"post/java学习-JVM-heap-non-heap-off-heap/","link":"","permalink":"https://shang.at/post/java学习-JVM-heap-non-heap-off-heap/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"java网络编程-零拷贝","slug":"java网络编程-零拷贝","date":"2020-06-03T16:32:39.000Z","updated":"2020-12-11T08:18:16.713Z","comments":true,"path":"post/java网络编程-零拷贝/","link":"","permalink":"https://shang.at/post/java网络编程-零拷贝/","excerpt":"","text":"什么是零拷贝刚才讲阻塞 IO 的时候我讲到，系统内核处理 IO 操作分为两个阶段——等待数据和拷贝数据。等待数据，就是系统内核在等待网卡接收到数据后，把数据写到内核中；而拷贝数据，就是系统内核在获取到数据后，将数据拷贝到用户进程的空间中。以下是具体流程： 应用进程的每一次写操作，都会把数据写到用户空间的缓冲区中，再由 CPU 将数据拷贝到系统内核的缓冲区中，之后再由 DMA 将这份数据拷贝到网卡中，最后由网卡发送出去。这里我们可以看到，一次写操作数据要拷贝两次才能通过网卡发送出去，而用户进程的读操作则是将整个流程反过来，数据同样会拷贝两次才能让应用程序读取到数据。 应用进程的一次完整的读写操作，都需要在用户空间与内核空间中来回拷贝，并且每一次拷贝，都需要 CPU 进行一次上下文切换（由用户进程切换到系统内核，或由系统内核切换到用户进程），这样是不是很浪费 CPU 和性能呢？那有没有什么方式，可以减少进程间的数据拷贝，提高数据传输的效率呢？ 这时我们就需要零拷贝（Zero-copy）技术。 所谓的零拷贝，就是取消用户空间与内核空间之间的数据拷贝操作，应用进程每一次的读写操作，都可以通过一种方式，让应用进程向用户空间写入或者读取数据，就如同直接向内核空间写入或者读取数据一样，再通过 DMA 将内核中的数据拷贝到网卡，或将网卡中的数据 copy 到内核。 那怎么做到零拷贝？你想一下是不是用户空间与内核空间都将数据写到一个地方，就不需要拷贝了？此时你有没有想到虚拟内存？ 零拷贝有两种解决方式，分别是 mmap+write 方式和 sendfile 方式，mmap+write 方式的核心原理就是通过虚拟内存来解决的。 java中的零拷贝Netty中的零拷贝 具体去读Netty的源码再详细补充这里 了解完零拷贝，我们再看看 Netty 中的零拷贝。 我刚才讲到，RPC 框架在网络通信框架的选型上，我们最优的选择是基于 Reactor 模式实现的框架，如 Java 语言，首选的便是 Netty 框架。那么 Netty 框架是否也有零拷贝机制呢？Netty 框架中的零拷贝和我之前讲的零拷贝又有什么不同呢？ 刚才我讲的零拷贝是操作系统层面上的零拷贝，主要目标是避免用户空间与内核空间之间的数据拷贝操作，可以提升 CPU 的利用率。 而 Netty 的零拷贝则不大一样，他完全站在了用户空间上，也就是 JVM 上，它的零拷贝主要是偏向于数据操作的优化上。 那么 Netty 这么做的意义是什么呢？ 回想下[第 02 讲]，在这一讲中我讲解了 RPC 框架如何去设计协议，其中我讲到：在传输过程中，RPC 并不会把请求参数的所有二进制数据整体一下子发送到对端机器上，中间可能会拆分成好几个数据包，也可能会合并其他请求的数据包，所以消息都需要有边界。那么一端的机器收到消息之后，就需要对数据包进行处理，根据边界对数据包进行分割和合并，最终获得一条完整的消息。 那收到消息后，对数据包的分割和合并，是在用户空间完成，还是在内核空间完成的呢？ 当然是在用户空间，因为对数据包的处理工作都是由应用程序来处理的，那么这里有没有可能存在数据的拷贝操作？可能会存在，当然不是在用户空间与内核空间之间的拷贝，是用户空间内部内存中的拷贝处理操作。Netty 的零拷贝就是为了解决这个问题，在用户空间对数据操作进行优化。 那么 Netty 是怎么对数据操作进行优化的呢？ Netty 提供了 CompositeByteBuf 类，它可以将多个 ByteBuf 合并为一个逻辑上的 ByteBuf，避免了各个 ByteBuf 之间的拷贝。 ByteBuf 支持 slice 操作，因此可以将 ByteBuf 分解为多个共享同一个存储区域的 ByteBuf，避免了内存的拷贝。 通过 wrap 操作，我们可以将 byte[] 数组、ByteBuf、ByteBuffer 等包装成一个 Netty ByteBuf 对象, 进而避免拷贝操作。 Netty 框架中很多内部的 ChannelHandler 实现类，都是通过 CompositeByteBuf、slice、wrap 操作来处理 TCP 传输中的拆包与粘包问题的。 那么 Netty 有没有解决用户空间与内核空间之间的数据拷贝问题的方法呢？ Netty 的 ByteBuffer 可以采用 Direct Buffers，使用堆外直接内存进行 Socket 的读写操作，最终的效果与我刚才讲解的虚拟内存所实现的效果是一样的。(mmap方式) Netty 还提供 FileRegion 中包装 NIO 的 FileChannel.transferTo() 方法实现了零拷贝，这与 Linux 中的 sendfile 方式在原理上也是一样的。 总结零拷贝带来的好处就是避免没必要的 CPU 拷贝，让 CPU 解脱出来去做其他的事，同时也减少了 CPU 在用户空间与内核空间之间的上下文切换，从而提升了网络通信效率与应用程序的整体性能。 而 Netty 的零拷贝与操作系统的零拷贝是有些区别的，Netty 的零拷贝偏向于用户空间中对数据操作的优化，这对处理 TCP 传输中的拆包粘包问题有着重要的意义，对应用程序处理请求数据与返回数据也有重要的意义。 在 RPC 框架的开发与使用过程中，我们要深入了解网络通信相关的原理知识，尽量做到零拷贝，如使用 Netty 框架；我们要合理使用 ByteBuf 子类，做到完全零拷贝，提升 RPC 框架的整体性能。 其他关于零拷贝技术的文章： Java中的零拷贝","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"网络编程-零拷贝","slug":"网络编程-零拷贝","permalink":"https://shang.at/tags/网络编程-零拷贝/"}]},{"title":"java学习-JVM参数","slug":"java学习-JVM参数","date":"2020-06-03T10:36:02.000Z","updated":"2020-06-04T03:30:03.517Z","comments":true,"path":"post/java学习-JVM参数/","link":"","permalink":"https://shang.at/post/java学习-JVM参数/","excerpt":"","text":"查看JVM参数启动应用的时候分别加以下的参数可以打印相关的参数： java -XX:+PrintFlagsInitial 打印所有的JVM初始参数，但是会立刻终止应用 java -XX:+PrintFlagsFinal 打印所有的设置后的JVM参数，不会终止应用 jinfo [options] pid ​ jinfo -flags pid 查看指定pid的jvm的所有设置参数 -XX:+PrintVMOptions 程序运行时，打印虚拟机接受到的命令行显式参数。不会终止应用 -XX:+PrintCommandLineFlags 打印传递给虚拟机的显式和隐式参数。不会终止应用 常见的JVM参数 InitialHeapSize(单位是字节)：初始堆大小，默认是物理内存的1/64，最小为2m(设置了1m，发现PrintFlagsFinal打印出来的是2m)，可以使用-Xms指定，如-Xms64m，只能指定m、g这样的单位 MaxHeapSize：最大堆大小，默认是物理内存的1/4，最小为2m，可以使用-Xmx指定，如-Xmx64m MaxNewSize： NewSize： OldSize： MetaspaceSize： MaxMetaspaceSize： ThreadStackSize(单位是kb)：虚拟机栈大小，默认是1024k，可以使用-Xss指定，如-Xss256k MaxTenuringThreshold：对象晋升到老年代的年龄阈值，默认是15，可以使用-XX:MaxTenuringThreshold指定，如-XX:MaxTenuringThreshold=20 参数调优 默认空余堆内存小于40%时，JVM就会增大堆直到-Xmx的最大限制；空余堆内存大于70%时，JVM会减少堆直到 -Xms的最小限制。因此服务器一般设置-Xms、-Xmx相等以避免在每次GC 后调整堆的大小。","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"JVM参数","slug":"JVM参数","permalink":"https://shang.at/tags/JVM参数/"}]},{"title":"Java学习-常见异常","slug":"Java学习-常见异常","date":"2020-06-03T09:31:00.000Z","updated":"2020-06-12T02:06:08.511Z","comments":true,"path":"post/Java学习-常见异常/","link":"","permalink":"https://shang.at/post/Java学习-常见异常/","excerpt":"","text":"StackOverflowError和OutOfMemoryError的区别(JDK1.8)StackOverflowError：Thrown when a stack overflow occurs because an application recurses too deeply. OutOfMemoryError：Thrown when the Java Virtual Machine cannot allocate an object because it is out of memory, and no more memory could be made available by the garbage collector. 从对这两个Error的注释来看， 由于应用程序递归过深而在堆栈溢出时会抛出StackOverflowError；同一个函数递归调用时，内存不足 当Java虚拟机由于内存不足而无法分配对象，并且垃圾回收器无法再提供更多内存时，抛出OutOfMemoryError。创建新的对象(包括虚拟机栈：函数调用(也包括递归调用)时创建虚拟机栈)时，内存不足 在java应用启动的时候，可以通过-Xss来设置虚拟机栈大小，虚拟机栈默认大小为1024k 在java里，函数的递归调用受到以下几方面的影响： 虚拟机栈大小：设置的虚拟机栈 局部变量表大小 https://blog.csdn.net/chengyun19830206/article/details/78452321 https://dzone.com/articles/outofmemoryerror-unable-create https://blog.csdn.net/kylinsoong/article/details/16879653 https://www.jianshu.com/p/c09727dc8f92","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"常见异常","slug":"常见异常","permalink":"https://shang.at/tags/常见异常/"}]},{"title":"JAVA并发编程-12-并发框架(Disruptor)","slug":"JAVA并发编程-12-并发框架-Disruptor","date":"2020-06-03T02:54:59.000Z","updated":"2020-06-03T03:00:26.307Z","comments":true,"path":"post/JAVA并发编程-12-并发框架-Disruptor/","link":"","permalink":"https://shang.at/post/JAVA并发编程-12-并发框架-Disruptor/","excerpt":"","text":"","categories":[{"name":"JAVA并发编程","slug":"JAVA并发编程","permalink":"https://shang.at/categories/JAVA并发编程/"}],"tags":[{"name":"Disruptor","slug":"Disruptor","permalink":"https://shang.at/tags/Disruptor/"}]},{"title":"JAVA并发编程-11-响应式编程(RxJava)","slug":"JAVA并发编程-11-响应式编程-RxJava","date":"2020-06-03T02:53:44.000Z","updated":"2020-06-03T08:24:11.250Z","comments":true,"path":"post/JAVA并发编程-11-响应式编程-RxJava/","link":"","permalink":"https://shang.at/post/JAVA并发编程-11-响应式编程-RxJava/","excerpt":"","text":"https://juejin.im/post/5ed62cabf265da7709526718","categories":[{"name":"JAVA并发编程","slug":"JAVA并发编程","permalink":"https://shang.at/categories/JAVA并发编程/"}],"tags":[{"name":"RxJava","slug":"RxJava","permalink":"https://shang.at/tags/RxJava/"}]},{"title":"JAVA并发编程-10-协程","slug":"JAVA并发编程-10-协程","date":"2020-06-03T02:53:19.000Z","updated":"2020-06-03T08:24:17.642Z","comments":true,"path":"post/JAVA并发编程-10-协程/","link":"","permalink":"https://shang.at/post/JAVA并发编程-10-协程/","excerpt":"","text":"https://juejin.im/post/5ed62cabf265da7709526718","categories":[{"name":"JAVA并发编程","slug":"JAVA并发编程","permalink":"https://shang.at/categories/JAVA并发编程/"}],"tags":[{"name":"协程","slug":"协程","permalink":"https://shang.at/tags/协程/"}]},{"title":"JAVA并发编程-9-线程池","slug":"JAVA并发编程-9-线程池","date":"2020-06-03T02:52:35.000Z","updated":"2020-06-03T02:59:46.242Z","comments":true,"path":"post/JAVA并发编程-9-线程池/","link":"","permalink":"https://shang.at/post/JAVA并发编程-9-线程池/","excerpt":"","text":"","categories":[{"name":"JAVA并发编程","slug":"JAVA并发编程","permalink":"https://shang.at/categories/JAVA并发编程/"}],"tags":[{"name":"线程池","slug":"线程池","permalink":"https://shang.at/tags/线程池/"}]},{"title":"JAVA并发编程-8-阻塞队列","slug":"JAVA并发编程-8-阻塞队列","date":"2020-06-03T02:52:16.000Z","updated":"2020-06-03T02:59:37.175Z","comments":true,"path":"post/JAVA并发编程-8-阻塞队列/","link":"","permalink":"https://shang.at/post/JAVA并发编程-8-阻塞队列/","excerpt":"","text":"","categories":[{"name":"JAVA并发编程","slug":"JAVA并发编程","permalink":"https://shang.at/categories/JAVA并发编程/"}],"tags":[{"name":"阻塞队列","slug":"阻塞队列","permalink":"https://shang.at/tags/阻塞队列/"}]},{"title":"JAVA并发编程-7-Atomic","slug":"JAVA并发编程-7-Atomic","date":"2020-06-03T02:51:56.000Z","updated":"2020-06-03T02:59:29.958Z","comments":true,"path":"post/JAVA并发编程-7-Atomic/","link":"","permalink":"https://shang.at/post/JAVA并发编程-7-Atomic/","excerpt":"","text":"","categories":[{"name":"JAVA并发编程","slug":"JAVA并发编程","permalink":"https://shang.at/categories/JAVA并发编程/"}],"tags":[{"name":"Atomic","slug":"Atomic","permalink":"https://shang.at/tags/Atomic/"}]},{"title":"JAVA并发编程-6-并发集合","slug":"JAVA并发编程-6-并发集合","date":"2020-06-03T02:51:41.000Z","updated":"2020-06-03T02:59:22.524Z","comments":true,"path":"post/JAVA并发编程-6-并发集合/","link":"","permalink":"https://shang.at/post/JAVA并发编程-6-并发集合/","excerpt":"","text":"","categories":[{"name":"JAVA并发编程","slug":"JAVA并发编程","permalink":"https://shang.at/categories/JAVA并发编程/"}],"tags":[{"name":"并发集合","slug":"并发集合","permalink":"https://shang.at/tags/并发集合/"}]},{"title":"JAVA并发编程-5-其他","slug":"JAVA并发编程-5-其他","date":"2020-06-03T02:51:25.000Z","updated":"2020-06-03T02:59:14.785Z","comments":true,"path":"post/JAVA并发编程-5-其他/","link":"","permalink":"https://shang.at/post/JAVA并发编程-5-其他/","excerpt":"","text":"","categories":[{"name":"JAVA并发编程","slug":"JAVA并发编程","permalink":"https://shang.at/categories/JAVA并发编程/"}],"tags":[{"name":"ThreadLocal-Fork&Join","slug":"ThreadLocal-Fork-Join","permalink":"https://shang.at/tags/ThreadLocal-Fork-Join/"}]},{"title":"JAVA并发编程-4-并发工具类","slug":"JAVA并发编程-4-并发工具类","date":"2020-06-03T02:51:14.000Z","updated":"2020-06-03T02:58:42.622Z","comments":true,"path":"post/JAVA并发编程-4-并发工具类/","link":"","permalink":"https://shang.at/post/JAVA并发编程-4-并发工具类/","excerpt":"","text":"","categories":[{"name":"JAVA并发编程","slug":"JAVA并发编程","permalink":"https://shang.at/categories/JAVA并发编程/"}],"tags":[{"name":"并发工具类","slug":"并发工具类","permalink":"https://shang.at/tags/并发工具类/"}]},{"title":"JAVA并发编程-3-锁","slug":"JAVA并发编程-3-锁","date":"2020-06-03T02:51:06.000Z","updated":"2020-06-03T02:58:32.008Z","comments":true,"path":"post/JAVA并发编程-3-锁/","link":"","permalink":"https://shang.at/post/JAVA并发编程-3-锁/","excerpt":"","text":"","categories":[{"name":"JAVA并发编程","slug":"JAVA并发编程","permalink":"https://shang.at/categories/JAVA并发编程/"}],"tags":[{"name":"锁","slug":"锁","permalink":"https://shang.at/tags/锁/"}]},{"title":"JAVA并发编程-2-并发基础","slug":"JAVA并发编程-2-并发基础","date":"2020-06-03T02:50:55.000Z","updated":"2020-06-03T02:58:23.366Z","comments":true,"path":"post/JAVA并发编程-2-并发基础/","link":"","permalink":"https://shang.at/post/JAVA并发编程-2-并发基础/","excerpt":"","text":"","categories":[{"name":"JAVA并发编程","slug":"JAVA并发编程","permalink":"https://shang.at/categories/JAVA并发编程/"}],"tags":[{"name":"并发基础","slug":"并发基础","permalink":"https://shang.at/tags/并发基础/"}]},{"title":"JAVA并发编程-1-内存模型JMM","slug":"JAVA并发编程-1-内存模型JMM","date":"2020-06-03T02:50:11.000Z","updated":"2020-12-11T08:17:49.358Z","comments":true,"path":"post/JAVA并发编程-1-内存模型JMM/","link":"","permalink":"https://shang.at/post/JAVA并发编程-1-内存模型JMM/","excerpt":"","text":"硬件层的并发优化基础 为了解决CPU速度和内存、磁盘的速度的差异(从上图可以看出，差异是很大的)带来的资源应用不均衡的问题，在硬件层是做了很多优化的，其中一个就是增加多级缓存机制，该机制会利用局部性原理(访问某个数据，那么它附近的数据将很有可能被访问)，将最近被访问的数据做一层缓存，那么下次再次访问该数据的时候，速度会快很多。 其中L1和L2级缓存是与CPU核绑定的，那么这就有可能导致一个问题：在并发环境下不同的核在做L1和L2缓存的时候，可能会导致两者缓存的数据不一致的问题，即Core1和Core2同时缓存了一个变量x，但是在Core1把它修改了，这个时候Core2的缓存实际桑上就不对了。那么如何进行同步呢？ 硬件层缓存一致性 总线锁：老的CPU 数据一致性协议 MESI Cache一致性协议 是一种缓存锁 协议很多 intel 用MESI，给每个缓存的内容作了一个标记 M: 被修改（Modified) 该缓存行只被缓存在该CPU的缓存中，并且是被修改过的（dirty),即与主存中的数据不一致，该缓存行中的内存需要在未来的某个时间点（允许其它CPU读取请主存中相应内存之前）写回（write back）主存。 当被写回主存之后，该缓存行的状态会变成独享（exclusive)状态。 E: 独享的（Exclusive) 该缓存行只被缓存在该CPU的缓存中，它是未被修改过的（clean)，与主存中数据一致。该状态可以在任何时刻当有其它CPU读取该内存时变成共享状态（shared)。 同样地，当CPU修改该缓存行中内容时，该状态可以变成Modified状态。 S: 共享的（Shared) 该状态意味着该缓存行可能被多个CPU缓存，并且各个缓存中的数据与主存数据一致（clean)，当有一个CPU修改该缓存行中，其它CPU中该缓存行可以被作废（变成无效状态（Invalid））。 I: 无效的（Invalid） 该缓存是无效的（可能有其它CPU修改了该缓存行）。 https://www.cnblogs.com/z00377750/p/9180644.html 现代CPU的数据一致性实现 = 缓存锁(MESI …) + 总线锁 缓存行读取缓存以cache line为基本单位，目前64bytes 位于同一缓存行的两个不同数据，被两个不同CPU锁定，产生互相影响的伪共享问题 伪共享问题：JUC/c_028_FalseSharing 使用缓存行的对齐能够提高效率 乱序执行CPU为了提高指令执行效率，会在一条指令执行过程中（比如去内存读取数据（慢100倍）），他会去同时执行另一条指令，前提是，两条指令没有依赖关系 https://www.cnblogs.com/liushaodong/p/4777308.html 写操作也可以合并： 合并写：WC Buffer，Write Combiner Buffer。一般只有4Bytes，位于比L1缓存还高的级别 12345JUC/029_WriteCombining乱序执行的证明：JVM/jmm/Disorder.java原始参考：https://preshing.com/20120515/memory-reordering-caught-in-the-act/ 如何保证特定情况下 不乱序硬件级别的内存屏障：Intel的CPU的汇编指令 sfence： save fence lfence： load fence mfence： modify fence 原子指令：如x86上的lock ...指令是一个FullBarrier，执行时会锁住内存子系统来确保执行顺序，甚至跨多个CPU。Sofeware Locks通常是使用了内存屏障或原子指令来实现变量的可见性和保持程序的顺序 JVM级别的内存屏障 volatile的实现细节 字节码层面 ​ 在编译成字节码的时候，被标记字段的accessflags为0x0040，即加了一个volatile的标记 JVM层面 在volatile变量的读写操作前后会加入不同的内存屏障，保证volatile变量读写操作不会发生乱序 OS和硬件层面 https://blog.csdn.net/qq_26222859/article/details/52235930 hsdis - HotSpot Dis Assembler windows lock 指令实现 | MESI实现 synchronized实现细节 字节码层面ACC_SYNCHRONIZEDmonitorenter monitorexit JVM层面C C++ 调用了操作系统提供的同步机制 OS和硬件层面X86 : lock cmpxchg / xxxhttps://blog.csdn.net/21aspnet/article/details/88571740 java并发内存模型 Happened-Before原则 as-is-serial","categories":[{"name":"JAVA并发编程","slug":"JAVA并发编程","permalink":"https://shang.at/categories/JAVA并发编程/"}],"tags":[{"name":"内存模型JMM","slug":"内存模型JMM","permalink":"https://shang.at/tags/内存模型JMM/"}]},{"title":"Spark应用-Scheduler","slug":"Spark应用-Scheduler","date":"2020-06-01T06:22:50.000Z","updated":"2021-05-07T01:51:09.179Z","comments":true,"path":"post/Spark应用-Scheduler/","link":"","permalink":"https://shang.at/post/Spark应用-Scheduler/","excerpt":"","text":"Spark任务有四种提交方式： local standalone yarn(这里着重讲) mesos 这里涉及到两层的任务调度： 第一层：schedule across applications，应用间的任务调度Spark的application提交到yarn平台，yarn平台负责Spark application的调度，这里也分为两层： 第一层：Yarn的队列，Spark application和其他运行在Yarn平台上的应用并无二致，都要统一服从yarn平台的安排yarn有三种任务调度模型： FIFO scheduler：先入先出调度器，整个Yarn集群只有一个任务队列，所有提交的任务都要等待上一个任务完全执行完才能执行，除非集群中有空闲的资源 Capacity scheduler：容量调度器，以Capacity为中心，把资源划分到若干个队列中，各个队列内根据自己的逻辑分配资源。例如下图中队列A可以调度的资源可以占80%，队列B占有剩下的20%，各队列接受相应的作业请求，在自己的资源中分配 Fair scheduler：秉承公平性原则，尽可能让各个作业得到的资源平均。先提交的job1马上占满了集群资源，那么作业2提交之后，原本Job1占有的资源拨出一些给作业2，从而达到“公平”(但是要等到job1的某些task执行完毕之后才能把资源让出来) 第二层：Yarn队列内的调度当使用FIFO scheduler，自不必说，它只有一个先进先出的队列，也就是队列内部的任务调度；Capacity scheduler会把 集群分成若干个队列，每个队列内部采用FIFO的策略；Fair scheduler可以通过设置，每个Fair Queue内部使用不同的schedulingPolicy，但是会有一个文档级别的默认策略的配置defaultQueueSchedulingPolicy，如果每个Queue没有自己的设置，那么就用defaultQueueSchedulingPolicy 第二层：schedule within application，同一个SparkContext内的job调度谈Spark下并行执行多个Job的问题 一次 Spark SQL 性能提升10倍的经历 Spark调度（一）：Task调度算法，FIFO还是FAIR 理解YARN Scheduler","categories":[],"tags":[{"name":"Scheduler","slug":"Scheduler","permalink":"https://shang.at/tags/Scheduler/"}]},{"title":"分布式常见思想-Bloomfilter","slug":"分布式常见思想-Bloomfilter","date":"2020-06-01T01:58:27.000Z","updated":"2020-06-01T01:58:58.674Z","comments":true,"path":"post/分布式常见思想-Bloomfilter/","link":"","permalink":"https://shang.at/post/分布式常见思想-Bloomfilter/","excerpt":"","text":"","categories":[{"name":"分布式","slug":"分布式","permalink":"https://shang.at/categories/分布式/"}],"tags":[{"name":"BloomFilter","slug":"BloomFilter","permalink":"https://shang.at/tags/BloomFilter/"}]},{"title":"数据结构与算法学习笔记-内存不足","slug":"数据结构与算法学习笔记-内存不足","date":"2020-05-31T23:35:34.000Z","updated":"2020-07-02T11:07:58.668Z","comments":true,"path":"post/数据结构与算法学习笔记-内存不足/","link":"","permalink":"https://shang.at/post/数据结构与算法学习笔记-内存不足/","excerpt":"","text":"中位数定义：数字排序之后，位于中间的那个数。比如将100亿个数字进行排序，排序之后，位于第50亿个位置的那个数 就是中位数。 ①内存够：内存够还慌什么啊，直接把100亿个全部排序了，你用冒泡都可以…然后找到中间那个就可以了。但是你以为面试官会给你内存？？ ②内存不够：题目说是整数，我们认为是带符号的int,所以4字节，占32位。 假设100亿个数字保存在一个大文件中，依次读一部分文件到内存(不超过内存的限制)，将每个数字用二进制表示，比较二进制的最高位(第32位，符号位，0是正，1是负)，如果数字的最高位为0，则将这个数字写入 file_0文件中；如果最高位为 1，则将该数字写入file_1文件中。 从而将100亿个数字分成了两个文件，假设 file_0文件中有 60亿 个数字，file_1文件中有 40亿 个数字。那么中位数就在 file_0 文件中，并且是 file_0 文件中所有数字排序之后的第 10亿 个数字。（file_1中的数都是负数，file_0中的数都是正数，也即这里一共只有40亿个负数，那么排序之后的第50亿个数一定位于file_0中） 现在，我们只需要处理 file_0 文件了（不需要再考虑file_1文件）。对于 file_0 文件，同样采取上面的措施处理：将file_0文件依次读一部分到内存(不超内存限制)，将每个数字用二进制表示，比较二进制的 次高位（第31位），如果数字的次高位为0，写入file_0_0文件中；如果次高位为1，写入file_0_1文件 中。 现假设 file_0_0文件中有30亿个数字，file_0_1中也有30亿个数字，则中位数就是：file_0_0文件中的数字从小到大排序之后的第10亿个数字。 抛弃file_0_1文件，继续对 file_0_0文件 根据 次次高位(第30位) 划分，假设此次划分的两个文件为：file_0_0_0中有5亿个数字，file_0_0_1中有25亿个数字，那么中位数就是 file_0_0_1文件中的所有数字排序之后的 第 5亿 个数。 按照上述思路，直到划分的文件可直接加载进内存时，就可以直接对数字进行快速排序，找出中位数了。","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"}],"tags":[{"name":"外排-分组归并-桶排序","slug":"外排-分组归并-桶排序","permalink":"https://shang.at/tags/外排-分组归并-桶排序/"}]},{"title":"Java学习-Future","slug":"Java学习-Future","date":"2020-05-31T22:44:35.000Z","updated":"2020-06-03T09:32:37.246Z","comments":true,"path":"post/Java学习-Future/","link":"","permalink":"https://shang.at/post/Java学习-Future/","excerpt":"","text":"","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"并发编程-Future","slug":"并发编程-Future","permalink":"https://shang.at/tags/并发编程-Future/"}]},{"title":"Python学习-一些常见的操作","slug":"Python学习-一些常见的操作","date":"2020-05-29T09:15:13.000Z","updated":"2021-05-19T02:25:10.732Z","comments":true,"path":"post/Python学习-一些常见的操作/","link":"","permalink":"https://shang.at/post/Python学习-一些常见的操作/","excerpt":"","text":"全局变量把global语句放到函数的开头 数组 如何初始化一个一维数组 123n=10l = [0]*nl1 = [0 for _ in range(n)] 如何初始化一个二维数组 123m, n = 10, 7l = [[0]*m]*n # 会有赋值问题：n个[0]*m 实际上都是同一个对象l1 = [[0 for _ in range(m)] for _ in range(n)] # 没有赋值问题 如何初始化一个二维数组并且设置右边界和下边界为1(根据实际情况处理) 12m, n = 10, 7dp = [[1 if i == m - 1 or j == n - 1 else 0 for i in range(m)] for j in range(n)] 正序遍历 123n=10for i in range(n): print(i) 倒序遍历 123n=10for i in range(n-1, -1, -1): print(i) 数组的元素拼接 1[str(x)+y+str(z) for x in (1,2,3) for y in ('a','b','c') for z in ('0','9')] 二维数组一维化 1234567891011121314151617181920212223242526272829303132333435363738394041from itertools import chainb=[[1,2,3], [5,8], [7,8,9]]c=list(chain(*b))print(c)[1, 2, 3, 5, 8, 7, 8, 9]### --------------------------------------------------------------------------------ab = [[1,2,3], [5,8], [7,8,9]]print([i for item in ab for i in item])[1, 2, 3, 5, 8, 7, 8, 9]### --------------------------------------------------------------------------------import operatorfrom functools import reducea = [[1,2,3], [4,6], [7,8,9,8]]print(reduce(operator.add, a))[1, 2, 3, 4, 6, 7, 8, 9, 8]### --------------------------------------------------------------------------------a = [[1,2,3], [5, 8], [7,8,9]]l=[]for m in range(0,3): for i in a[m]: l.append(i)print(l)[1, 2, 3, 5, 8, 7, 8, 9]### --------------------------------------------------------------------------------a=[[1,2,3], [5,8], [7,8,9]]a= eval('['+str(a).replace(' ','').replace('[','').replace(']','')+']')print(a)[1, 2, 3, 5, 8, 7, 8, 9]### --------------------------------------------------------------------------------def flatten(a): if not isinstance(a, (list, )): return [a] else: b = [] for item in a: b += flatten(item)if __name__ == '__main__': a = [[[1,2],3],[4,[5,6]],[7,8,9]] print(flatten(a))[1, 2, 3, 4, 5, 6, 7, 8, 9]### -------------------------------------------------------------------------------- 如何拷贝一个一维数组 12x = [1,2,3,4]y = x[:] 如何拷贝一个二维数组 1234567891011121314151617181920212223### --------------------------------------------------------------------------------x=[[1,2,3], [4,6]]y = [row[:] for row in x]### --------------------------------------------------------------------------------x=[[1,2,3], [4,6]]from copy import copy, deepcopyy = deepcopy(x)### --------------------------------------------------------------------------------old_array = [[2, 3], [4, 5]]# python2.*new_array = map(list, old_array)# python3.*new_array = list(map(list, old_array))### --------------------------------------------------------------------------------arr = [[1,2],[3,4]]deepcopy1d2d = lambda lVals: [x if not isinstance(x, list) else x[:] for x in lVals]dst = deepcopy1d2d(arr)dst[1][1]=150print dstprint arr set的操作1234567a = t | s # t 和 s的并集 b = t &amp; s # t 和 s的交集 c = t – s # 求差集（项在t中，但不在s中） d = t ^ s # 对称差集（项在t或s中，但不会同时出现在二者中） ascii码的转换12345# 获取一个字符的ascii码ord('a')# 将ascii码转换成字符chr(97) 内存12345678910# 内存地址a = 9id(a)# 查看一个对象内存占用大小(变量所占字节的大小)import syssys.getsizeof(a)# 查看变量类型type(a) 进制123456789101112131415161718192021222324252627282930# 二进制Ob1010# 获取一个数字的二进制&gt;&gt;&gt; bin(3)'0b11'&gt;&gt;&gt; bin(-10)'-0b1010'# 八进制0o176# 获取一个数字的二进制&gt;&gt;&gt; oct(3)'0o3'&gt;&gt;&gt; bin(-3)'-0o3'# 十进制43# 十六进制0x12abhex(3)&gt;&gt;&gt; '0x3'# 2 8 16 进制转换成 十进制int('101010', 2) # int('0b101010', 0)int('37621', 8) # int('0o37621', 0)int('23abcf', 16) # int('0x23abcf', 0) 位操作123456&gt;&gt; # 右移：向右移1位可以看成除以2&lt;&lt; # 左移：向左移一位可以看成乘以2&amp; # 与| # 或~ # 取反：效果是对n的内部表示的每一位求补^ # 异或 交换元素12345678910111213# 通常写法a=1b=2a,b = b,a# 数组元素交换l = [1,2,3,4,5]l[3], l[1] = l[1], l[3]# 但是要同时交换数组和index就不行了i=0i, l[i] = l[i], 7 # 失败：这里不能用这种写法，因为修改了i的值之后，等号前面的self.p[i]就会立刻指向修改后的i的位置。这里和普通的a b交换有区别l[i], i = 7, l[i] # 等号后面的l[i]实际上是值传递 整数的最大值和最小值1234567import sysmax_value = sys.maxsizemin_value = -sys.maxsize - 1max_value = float('inf')min_value = float('-inf') string123456789101112131415161718192021222324252627282930313233import string # 导入string这个模块print(string.digits) # 输出包含数字0~9的字符串print(string.ascii_letters) # 包含所有字母(大写或小写)的字符串print(string.ascii_lowercase) # 包含所有小写字母的字符串print(string.ascii_uppercase) # 包含所有大写字母的字符串##############0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZprint([chr(i) for i in range(65, 91)]) # 所有大写字母print([chr(i) for i in range(97, 123)]) # 所有小写字母print([chr(i) for i in range(48, 58)]) # 所有数字####################['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']# 去掉空格的三个函数&gt;&gt;&gt;a=\" gho stwwl \"&gt;&gt;&gt;a.lstrip()'gho stwwl '&gt;&gt;&gt;a.rstrip()' gho stwwl'&gt;&gt;&gt;a.strip()'gho stwwl' f-stringhttps://blog.csdn.net/sunxb10/article/details/81036693 合并字典：https://blog.csdn.net/shanliangliuxing/article/details/7757089","categories":[{"name":"Python","slug":"Python","permalink":"https://shang.at/categories/Python/"}],"tags":[{"name":"常见操作技巧","slug":"常见操作技巧","permalink":"https://shang.at/tags/常见操作技巧/"}]},{"title":"Python学习-lru_cache","slug":"Python学习-lru-cache","date":"2020-05-28T01:48:49.000Z","updated":"2020-05-28T01:49:40.668Z","comments":true,"path":"post/Python学习-lru-cache/","link":"","permalink":"https://shang.at/post/Python学习-lru-cache/","excerpt":"","text":"","categories":[{"name":"Python","slug":"Python","permalink":"https://shang.at/categories/Python/"}],"tags":[{"name":"lru_cache","slug":"lru-cache","permalink":"https://shang.at/tags/lru-cache/"}]},{"title":"Java学习-Thread","slug":"Java学习-Thread","date":"2020-05-24T11:17:33.000Z","updated":"2020-06-12T01:57:34.925Z","comments":true,"path":"post/Java学习-Thread/","link":"","permalink":"https://shang.at/post/Java学习-Thread/","excerpt":"","text":"线程的基本概念进程：每个进程都有独立的代码和数据空间（进程上下文），进程间的切换会有较大的开销，一个进程包含1–n个线程。（进程是资源分配的最小单位）线程：同一类线程共享代码和数据空间，每个线程有独立的运行栈和程序计数器(PC)，线程切换开销小。（线程是cpu调度的最小单位） 线程的状态创建、就绪、运行、阻塞、终止 创建：新创建的一个线程对象 就绪：线程对象创建成功后，其他的线程调用该对象的start方法。该状态的线程位于可运行线程池内，变的可运行，等待获取CPU的使用权呢 运行：就绪状态的线程获取了CPU使用权，执行程序代码 阻塞：阻塞状态是线程因为某种原因放弃了CPU使用权，暂停运行。直到线程再次进入就绪状态，才有机会转到运行状态。阻塞的情况分为以下三种： 等待阻塞：运行的线程执行wait方法，JVM会把该线程放入等待池中(wait会释放持有的锁) 同步阻塞：运行的线程在获取对象同步锁的时，如果该同步锁被别的线程占用，则JVM会把改线程放入锁池中 其他阻塞：运行的线程执行sleep或join方法，或者发出了I/O(文件读写、网络请求)请求，JVM会把该线程置为阻塞状态。当sleep状态超时、join等待线程终止或者超时、或者I/O处理完毕时，线程重新转入就绪状态(sleep不会释放持有的锁) 终止：线程执行完了或者因异常推出了run方法，该线程结束生命周期 线程创建和切换的代价：https://www.jianshu.com/p/ece1bb5fa88b","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"并发编程-Thread","slug":"并发编程-Thread","permalink":"https://shang.at/tags/并发编程-Thread/"}]},{"title":"Java学习-并发编程","slug":"Java学习-并发编程","date":"2020-05-24T10:46:42.000Z","updated":"2021-04-12T09:35:48.272Z","comments":true,"path":"post/Java学习-并发编程/","link":"","permalink":"https://shang.at/post/Java学习-并发编程/","excerpt":"","text":"整体脑图 并发编程领域可以抽象成三个核心问题：分工、同步和互斥 分工不同的线程负责不同的任务，可以并发的执行 同步互斥 JAVA并发编程-1-内存模型 JAVA并发编程-2-并发基础 JAVA并发编程-3-锁 JAVA并发编程-4-并发工具类 JAVA并发编程-5-ThreadLocal&amp;Fork&amp;Join JAVA并发编程-6-并发集合 JAVA并发编程-7-Atomic JAVA并发编程-8-阻塞队列 JAVA并发编程-9-线程池 JAVA并发编程-10-响应式编程-RxJava JAVA并发编程-12-并发框架-Disruptor 疑问秒懂系列","categories":[{"name":"JAVA并发编程","slug":"JAVA并发编程","permalink":"https://shang.at/categories/JAVA并发编程/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://shang.at/tags/并发编程/"}]},{"title":"Java学习-JVM","slug":"Java学习-JVM","date":"2020-05-24T10:42:22.000Z","updated":"2020-12-11T08:23:18.090Z","comments":true,"path":"post/Java学习-JVM/","link":"","permalink":"https://shang.at/post/Java学习-JVM/","excerpt":"","text":"转载地址 : JavaGuide 点击关注公众号及时获取笔主最新更新文章，并可免费领取本文档配套的《Java面试突击》以及Java工程师必备学习资源。 Java 内存区域详解如果没有特殊说明，都是针对的是 HotSpot 虚拟机。 写在前面 (常见面试题)基本问题 介绍下 Java 内存区域（运行时数据区） Java 对象的创建过程（五步，建议能默写出来并且要知道每一步虚拟机做了什么） 对象的访问定位的两种方式（句柄和直接指针两种方式） 拓展问题 String 类和常量池 8 种基本类型的包装类和常量池 一 概述对于 Java 程序员来说，在虚拟机自动内存管理机制下，不再需要像 C/C++程序开发程序员这样为每一个 new 操作去写对应的 delete/free 操作，不容易出现内存泄漏和内存溢出问题。正是因为 Java 程序员把内存控制权利交给 Java 虚拟机，一旦出现内存泄漏和溢出方面的问题，如果不了解虚拟机是怎样使用内存的，那么排查错误将会是一个非常艰巨的任务。 二 运行时数据区域Java 虚拟机在执行 Java 程序的过程中会把它管理的内存划分成若干个不同的数据区域。JDK. 1.8 和之前的版本略有不同，下面会介绍到。 JDK 1.8 之前： JDK 1.8 ： 线程私有的：不会带来并发问题 程序计数器 虚拟机栈 本地方法栈 线程共享的：在并发问题中主要解决的就是内存共享问题，其中最主要的是堆内存的管理 堆 方法区 直接内存 (非运行时数据区的一部分) 疑问：线程私有的内存占用是JVM管理的内存的哪一部分？ Xss参数是设定虚拟机中每个线程占用的栈内存大小，而虚拟机栈可分配的内存又跟物理机的内存大小、Java堆内存、方法区(JDK1.7及以前)等内存大小相关，其它的区分得的内存越大，虚拟机栈能够分得的内存就越小，并发的线程数量也就越小————这句话是对的吗？ 测试：单线程 启动参数 -Xss16m -Xms2m -Xmx2m，stack depth:415910 报了StackOverflowError 2.1 程序计数器程序计数器是一块较小的内存空间，可以看作是当前线程所执行的字节码的行号指示器。字节码解释器工作时通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等功能都需要依赖这个计数器来完成。 另外，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各线程之间计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。 从上面的介绍中我们知道程序计数器主要有两个作用： 字节码解释器通过改变程序计数器来依次读取指令，从而实现代码的流程控制，如：顺序执行、选择、循环、异常处理。 在多线程的情况下，程序计数器用于记录当前线程执行的位置，从而当线程被切换回来的时候能够知道该线程上次运行到哪儿了。 注意：程序计数器是唯一一个不会出现 OutOfMemoryError 的内存区域，它的生命周期随着线程的创建而创建，随着线程的结束而死亡。 2.2 Java 虚拟机栈与程序计数器一样，Java 虚拟机栈也是线程私有的，它的生命周期和线程相同，描述的是 Java 方法执行的内存模型，每次方法调用的数据都是通过栈传递的。 Java 内存可以粗糙的区分为堆内存（Heap）和栈内存 (Stack),其中栈就是现在说的虚拟机栈，或者说是虚拟机栈中局部变量表部分。 （实际上，Java 虚拟机栈是由一个个栈帧组成，而每个栈帧中都拥有：局部变量表、操作数栈、动态链接、方法出口信息。） 局部变量表主要存放了编译期可知的各种数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（reference 类型，它不同于对象本身，可能是一个指向对象起始地址的引用指针，也可能是指向一个代表对象的句柄或其他与此对象相关的位置）。 Java 虚拟机栈会出现两种错误：StackOverFlowError 和 OutOfMemoryError。二者差异 StackOverFlowError： 若 Java 虚拟机栈的内存大小不允许动态扩展，那么当线程请求栈的深度超过当前 Java 虚拟机栈的最大深度的时候，就抛出 StackOverFlowError 错误。 OutOfMemoryError： 若 Java 虚拟机栈的内存大小允许动态扩展，且当线程请求栈时内存用完了，无法再动态扩展了，此时抛出 OutOfMemoryError 错误。 Java 虚拟机栈也是线程私有的，每个线程都有各自的 Java 虚拟机栈，而且随着线程的创建而创建，随着线程的死亡而死亡。 扩展：那么方法/函数如何调用？ Java 栈可用类比数据结构中栈，Java 栈中保存的主要内容是栈帧，每一次函数调用都会有一个对应的栈帧被压入 Java 栈，每一个函数调用结束后，都会有一个栈帧被弹出。 Java 方法有两种返回方式： return 语句。 抛出异常。 不管哪种返回方式都会导致栈帧被弹出。 疑问：虚拟机栈到底是占用的哪块内存呢？JVM直接开辟的系统内存空间，还是会共享heap空间？由于每次创建线程，都会创建其对应的虚拟机栈，那么线程创建太多会带来极大的内存消耗代价(以及线程切换代价) 做了一个实验： ​ -Xss4m -Xms8m -Xmx8m：stack depth:215835 报了java.lang.StackOverflowError ​ -Xss16m -Xms8m -Xmx8m：stack depth:416734 报了java.lang.StackOverflowError ​ -Xss32m -Xms8m -Xmx8m：stack depth:835116 报了java.lang.StackOverflowError 从实验结果来看，限制了内存大小之后，改变虚拟机栈大小，无论把虚拟机栈设置多大，最终都是报的java.lang.StackOverflowError，是不是可以得出这样的结论，hotspot的设计中虚拟机栈使用的是独立于java堆内存的系统空间呢？ 2.3 本地方法栈和虚拟机栈所发挥的作用非常相似，区别是： 虚拟机栈为虚拟机执行 Java 方法 （也就是字节码）服务，而本地方法栈则为虚拟机使用到的 Native 方法服务。 在 HotSpot 虚拟机中和 Java 虚拟机栈合二为一。 本地方法被执行的时候，在本地方法栈也会创建一个栈帧，用于存放该本地方法的局部变量表、操作数栈、动态链接、出口信息。 方法执行完毕后相应的栈帧也会出栈并释放内存空间，也会出现 StackOverFlowError 和 OutOfMemoryError 两种错误。 2.4 堆Java 虚拟机所管理的内存中最大的一块，Java 堆是所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例以及数组都在这里分配内存。 Java世界中“几乎”所有的对象都在堆中分配，但是，随着JIT编译期的发展与逃逸分析技术逐渐成熟，栈上分配、标量替换优化技术将会导致一些微妙的变化，所有的对象都分配到堆上也渐渐变得不那么“绝对”了。从jdk 1.7开始已经默认开启逃逸分析，如果某些方法中的对象引用没有被返回或者未被外面使用（也就是未逃逸出去），那么对象可以直接在栈上分配内存。 Java 堆是垃圾收集器管理的主要区域，因此也被称作GC 堆（Garbage Collected Heap）.从垃圾回收的角度，由于现在收集器基本都采用分代垃圾收集算法，所以 Java 堆还可以细分为：新生代和老年代：再细致一点有：Eden 空间、From Survivor、To Survivor 空间等。进一步划分的目的是更好地回收内存，或者更快地分配内存。 在 JDK 7 版本及JDK 7 版本之前，堆内存被通常被分为下面三部分： 新生代内存(Young Generation) 老生代(Old Generation) 永生代(Permanent Generation) JDK 8 版本之后方法区（HotSpot 的永久代）被彻底移除了（JDK1.7 就已经开始了），取而代之是元空间，元空间使用的是直接内存。 上图所示的 Eden 区、两个 Survivor 区都属于新生代（为了区分，这两个 Survivor 区域按照顺序被命名为 from 和 to），中间一层属于老年代。 大部分情况，对象都会首先在 Eden 区域分配，在一次新生代垃圾回收后，如果对象还存活，则会进入 s0 或者 s1，并且对象的年龄还会加 1(Eden 区-&gt;Survivor 区后对象的初始年龄变为 1)，当它的年龄增加到一定程度（默认为 15 岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数 -XX:MaxTenuringThreshold 来设置。 修正（issue552）：“Hotspot遍历所有对象时，按照年龄从小到大对其所占用的大小进行累积，当累积的某个年龄大小超过了survivor区的一半时，取这个年龄和MaxTenuringThreshold中更小的一个值，作为新的晋升年龄阈值”。 动态年龄计算的代码如下 123456789101112131415&gt; uint ageTable::compute_tenuring_threshold(size_t survivor_capacity) &#123;&gt; //survivor_capacity是survivor空间的大小&gt; size_t desired_survivor_size = (size_t)((((double) survivor_capacity)*TargetSurvivorRatio)/100);&gt; size_t total = 0;&gt; uint age = 1;&gt; while (age &lt; table_size) &#123;&gt; total += sizes[age];//sizes数组是每个年龄段对象大小&gt; if (total &gt; desired_survivor_size) break;&gt; age++;&gt; &#125;&gt; uint result = age &lt; MaxTenuringThreshold ? age : MaxTenuringThreshold;&gt; ...&gt; &#125;&gt; &gt; &gt;&gt; 堆这里最容易出现的就是 OutOfMemoryError 错误，并且出现这种错误之后的表现形式还会有几种，比如： OutOfMemoryError: GC Overhead Limit Exceeded ： 当JVM花太多时间执行垃圾回收并且只能回收很少的堆空间时，就会发生此错误。 java.lang.OutOfMemoryError: Java heap space :假如在创建新的对象时, 堆内存中的空间不足以存放新创建的对象, 就会引发java.lang.OutOfMemoryError: Java heap space 错误。(和本机物理内存无关，和你配置的内存大小有关！) …… 2.5 方法区方法区与 Java 堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。虽然 Java 虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做 Non-Heap（非堆），目的应该是与 Java 堆区分开来。 方法区也被称为永久代。很多人都会分不清方法区和永久代的关系，为此我也查阅了文献。 2.5.1 方法区和永久代的关系 《Java 虚拟机规范》只是规定了有方法区这么个概念和它的作用，并没有规定如何去实现它。那么，在不同的 JVM 上方法区的实现肯定是不同的了。 方法区和永久代的关系很像 Java 中接口和类的关系，类实现了接口，而永久代就是 HotSpot 虚拟机对虚拟机规范中方法区的一种实现方式。 也就是说，永久代是 HotSpot 的概念，方法区是 Java 虚拟机规范中的定义，是一种规范，而永久代是一种实现，一个是标准一个是实现，其他的虚拟机实现并没有永久代这一说法。 2.5.2 常用参数JDK 1.8 之前永久代还没被彻底移除的时候通常通过下面这些参数来调节方法区大小 12-XX:PermSize=N //方法区 (永久代) 初始大小-XX:MaxPermSize=N //方法区 (永久代) 最大大小,超过这个值将会抛出 OutOfMemoryError 异常:java.lang.OutOfMemoryError: PermGen 相对而言，垃圾收集行为在这个区域是比较少出现的，但并非数据进入方法区后就“永久存在”了。 JDK 1.8 的时候，方法区（HotSpot 的永久代）被彻底移除了（JDK1.7 就已经开始了），取而代之是元空间，元空间使用的是直接内存。 下面是一些常用参数： 12-XX:MetaspaceSize=N //设置 Metaspace 的初始（和最小大小）-XX:MaxMetaspaceSize=N //设置 Metaspace 的最大大小 与永久代很大的不同就是，如果不指定大小的话，随着更多类的创建，虚拟机会耗尽所有可用的系统内存。 2.5.3 为什么要将永久代 (PermGen) 替换为元空间 (MetaSpace) 呢? 整个永久代有一个 JVM 本身设置固定大小上限，无法进行调整，而元空间使用的是直接内存，受本机可用内存的限制，虽然元空间仍旧可能溢出，但是比原来出现的几率会更小。 当你元空间溢出时会得到如下错误： java.lang.OutOfMemoryError: MetaSpace 你可以使用 -XX：MaxMetaspaceSize 标志设置最大元空间大小，默认值为 unlimited，这意味着它只受系统内存的限制。-XX：MetaspaceSize 调整标志定义元空间的初始大小如果未指定此标志，则 Metaspace 将根据运行时的应用程序需求动态地重新调整大小。 元空间里面存放的是类的元数据，这样加载多少类的元数据就不由 MaxPermSize 控制了, 而由系统的实际可用空间来控制，这样能加载的类就更多了。 在 JDK8，合并 HotSpot 和 JRockit 的代码时, JRockit 从来没有一个叫永久代的东西, 合并之后就没有必要额外的设置这么一个永久代的地方了。 2.6 运行时常量池运行时常量池是方法区的一部分。Class 文件中除了有类的版本、字段、方法、接口等描述信息外，还有常量池表（用于存放编译期生成的各种字面量和符号引用） 既然运行时常量池是方法区的一部分，自然受到方法区内存的限制，当常量池无法再申请到内存时会抛出 OutOfMemoryError 错误。 JDK1.7 及之后版本的 JVM 已经将运行时常量池从方法区中移了出来，在 Java 堆（Heap）中开辟了一块区域存放运行时常量池。 修正(issue747，reference)： JDK1.7之前运行时常量池逻辑包含字符串常量池存放在方法区, 此时hotspot虚拟机对方法区的实现为永久代 JDK1.7 字符串常量池被从方法区拿到了堆中, 这里没有提到运行时常量池,也就是说字符串常量池被单独拿到堆,运行时常量池剩下的东西还在方法区, 也就是hotspot中的永久代 。 JDK1.8 hotspot移除了永久代用元空间(Metaspace)取而代之, 这时候字符串常量池还在堆, 运行时常量池还在方法区, 只不过方法区的实现从永久代变成了元空间(Metaspace) 实验：以下三组参数分别运行同一段代码 123456789&gt; public static void main(String[] args) throws Throwable &#123;&gt; List&lt;String&gt; list = new ArrayList&lt;String&gt;();&gt; int i=0;&gt; while(true)&#123;&gt; list.add(String.valueOf(i++).intern());&gt; &#125;&gt; &gt; &#125;&gt; &gt; JDK1.6 -XX:PermSize=20M -XX:MaxPermSize=20M设置永久区大小，报OutOfMemoryError： PermGen space，说明字符串常量位于PermGen JDK1.7 -XX:PermSize=20M -XX:MaxPermSize=20M设置永久区大小，OutOfMemoryError： Java heap space，说明字符串常量位于了heap内 JDK1.8 -XX:MetaspaceSize=20M -XX:MaxMetaspaceSize=20M设置永久区大小，OutOfMemoryError： Java heap space，说明字符串常量还在heap内 相关问题：JVM 常量池中存储的是对象还是引用呢？： https://www.zhihu.com/question/57109429/answer/151717241 by RednaxelaFX 2.7 直接内存直接内存并不是虚拟机运行时数据区的一部分，也不是虚拟机规范中定义的内存区域，但是这部分内存也被频繁地使用。而且也可能导致 OutOfMemoryError 错误出现。 JDK1.4 中新加入的 NIO(New Input/Output) 类，引入了一种基于通道（Channel） 与缓存区（Buffer） 的 I/O 方式，它可以直接使用 Native 函数库直接分配堆外内存，然后通过一个存储在 Java 堆中的 DirectByteBuffer 对象作为这块内存的引用进行操作。这样就能在一些场景中显著提高性能，因为避免了在 Java 堆和 Native 堆之间来回复制数据。这就是网络编程中常说的Zero-copy(零拷贝) 本机直接内存的分配不会受到 Java 堆的限制，但是，既然是内存就会受到本机总内存大小以及处理器寻址空间的限制。 三 HotSpot 虚拟机对象探秘通过上面的介绍我们大概知道了虚拟机的内存情况，下面我们来详细的了解一下 HotSpot 虚拟机在 Java 堆中对象分配、布局和访问的全过程。 3.1 对象的创建下图便是 Java 对象的创建过程，我建议最好是能默写出来，并且要掌握每一步在做什么。 Step1:类加载检查 虚拟机遇到一条 new 指令时，首先将去检查这个指令的参数是否能在常量池中定位到这个类的符号引用，并且检查这个符号引用代表的类是否已被加载过、解析和初始化过。如果没有，那必须先执行相应的类加载过程。 Step2:分配内存在类加载检查通过后，接下来虚拟机将为新生对象分配内存。对象所需的内存大小在类加载完成后便可确定，为对象分配空间的任务等同于把一块确定大小的内存从 Java 堆中划分出来。分配方式有 “指针碰撞” 和 “空闲列表” 两种，选择那种分配方式由 Java 堆是否规整决定，而 Java 堆是否规整又由所采用的垃圾收集器是否带有压缩整理功能决定。 内存分配的两种方式：（补充内容，需要掌握） 选择以上两种方式中的哪一种，取决于 Java 堆内存是否规整。而 Java 堆内存是否规整，取决于 GC 收集器的算法是”标记-清除”，还是”标记-整理”（也称作”标记-压缩”），值得注意的是，复制算法内存也是规整的 内存分配并发问题（补充内容，需要掌握） 在创建对象的时候有一个很重要的问题，就是线程安全，因为在实际开发过程中，创建对象是很频繁的事情，作为虚拟机来说，必须要保证线程是安全的，通常来讲，虚拟机采用两种方式来保证线程安全： CAS+失败重试： CAS 是乐观锁的一种实现方式。所谓乐观锁就是，每次不加锁而是假设没有冲突而去完成某项操作，如果因为冲突失败就重试，直到成功为止。虚拟机采用 CAS 配上失败重试的方式保证更新操作的原子性。 TLAB： 为每一个线程预先在 Eden 区分配一块儿内存，JVM 在给线程中的对象分配内存时，首先在 TLAB 分配，当对象大于 TLAB 中的剩余内存或 TLAB 的内存已用尽时，再采用上述的 CAS 进行内存分配 Step3:初始化零值内存分配完成后，虚拟机需要将分配到的内存空间都初始化为零值（不包括对象头），这一步操作保证了对象的实例字段在 Java 代码中可以不赋初始值就直接使用，程序能访问到这些字段的数据类型所对应的零值。 Step4:设置对象头初始化零值完成之后，虚拟机要对对象进行必要的设置，例如这个对象是那个类的实例、如何才能找到类的元数据信息、对象的哈希码、对象的 GC 分代年龄等信息。 这些信息存放在对象头中。 另外，根据虚拟机当前运行状态的不同，如是否启用偏向锁等，对象头会有不同的设置方式。 Step5:执行 init 方法 在上面工作都完成之后，从虚拟机的视角来看，一个新的对象已经产生了，但从 Java 程序的视角来看，对象创建才刚开始，&lt;init&gt; 方法还没有执行，所有的字段都还为零。所以一般来说，执行 new 指令之后会接着执行 &lt;init&gt; 方法，把对象按照程序员的意愿进行初始化，这样一个真正可用的对象才算完全产生出来。 3.2 对象的内存布局在 Hotspot 虚拟机中，对象在内存中的布局可以分为 3 块区域：对象头、实例数据和对齐填充。 Hotspot 虚拟机的对象头包括两部分信息，第一部分用于存储对象自身的运行时数据（哈希码、GC 分代年龄、锁状态标志等等），另一部分是类型指针，即对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是那个类的实例。 实例数据部分是对象真正存储的有效信息，也是在程序中所定义的各种类型的字段内容。 对齐填充部分不是必然存在的，也没有什么特别的含义，仅仅起占位作用。 因为 Hotspot 虚拟机的自动内存管理系统要求对象起始地址必须是 8 字节的整数倍，换句话说就是对象的大小必须是 8 字节的整数倍。而对象头部分正好是 8 字节的倍数（1 倍或 2 倍），因此，当对象实例数据部分没有对齐时，就需要通过对齐填充来补全。 3.3 对象的访问定位建立对象就是为了使用对象，我们的 Java 程序通过栈上的 reference 数据来操作堆上的具体对象。对象的访问方式由虚拟机实现而定，目前主流的访问方式有①使用句柄和②直接指针两种： 句柄： 如果使用句柄的话，那么 Java 堆中将会划分出一块内存来作为句柄池，reference 中存储的就是对象的句柄地址，而句柄中包含了对象实例数据与类型数据各自的具体地址信息； 直接指针： 如果使用直接指针访问，那么 Java 堆对象的布局中就必须考虑如何放置访问类型数据的相关信息，而 reference 中存储的直接就是对象的地址。 这两种对象访问方式各有优势。使用句柄来访问的最大好处是 reference 中存储的是稳定的句柄地址，在对象被移动时只会改变句柄中的实例数据指针，而 reference 本身不需要修改。使用直接指针访问方式最大的好处就是速度快，它节省了一次指针定位的时间开销。 四 重点补充内容4.1 String 类和常量池String 对象的两种创建方式： 12345String str1 = \"abcd\";//先检查字符串常量池中有没有\"abcd\"，如果字符串常量池中没有，则创建一个，然后 str1 指向字符串常量池中的对象，如果有，则直接将 str1 指向\"abcd\"\"；String str2 = new String(\"abcd\");//堆中创建一个新的对象String str3 = new String(\"abcd\");//堆中创建一个新的对象System.out.println(str1==str2);//falseSystem.out.println(str2==str3);//false 这两种不同的创建方法是有差别的。 第一种方式是在常量池中拿对象； 第二种方式是直接在堆内存空间创建一个新的对象。 记住一点：只要使用 new 方法，便需要创建新的对象。 再给大家一个图应该更容易理解，图片来源：https://www.journaldev.com/797/what-is-java-string-pool： String 类型的常量池比较特殊。它的主要使用方法有两种： 直接使用双引号声明出来的 String 对象会直接存储在常量池中。 如果不是用双引号声明的 String 对象，可以使用 String 提供的 intern 方法。String.intern() 是一个 Native 方法，它的作用是：如果运行时常量池中已经包含一个等于此 String 对象内容的字符串，则返回常量池中该字符串的引用；如果没有，JDK1.7之前（不包含1.7）的处理方式是在常量池中创建与此 String 内容相同的字符串，并返回常量池中创建的字符串的引用，JDK1.7以及之后的处理方式是在常量池中记录此字符串的引用，并返回该引用。 123456String s1 = new String(\"计算机\");String s2 = s1.intern();String s3 = \"计算机\";System.out.println(s2);//计算机System.out.println(s1 == s2);//false，因为一个是堆内存中的 String 对象一个是常量池中的 String 对象，System.out.println(s3 == s2);//true，因为两个都是常量池中的 String 对象 字符串拼接: 123456789String str1 = \"str\";String str2 = \"ing\"; String str3 = \"str\" + \"ing\";//常量池中的对象String str4 = str1 + str2; //在堆上创建的新的对象 String str5 = \"string\";//常量池中的对象System.out.println(str3 == str4);//falseSystem.out.println(str3 == str5);//trueSystem.out.println(str4 == str5);//false 尽量避免多个字符串拼接，因为这样会重新创建对象。如果需要改变字符串的话，可以使用 StringBuilder 或者 StringBuffer。 4.2 String s1 = new String(“abc”);这句话创建了几个字符串对象？将创建 1 或 2 个字符串。如果池中已存在字符串常量“abc”，则只会在堆空间创建一个字符串常量“abc”。如果池中没有字符串常量“abc”，那么它将首先在池中创建，然后在堆空间中创建，因此将创建总共 2 个字符串对象。 验证： 1234String s1 = new String(\"abc\");// 堆内存的地址值String s2 = \"abc\";System.out.println(s1 == s2);// 输出 false,因为一个是堆内存，一个是常量池的内存，故两者是不同的。System.out.println(s1.equals(s2));// 输出 true 结果： 12falsetrue 4.3 8 种基本类型的包装类和常量池Java 基本类型的包装类的大部分都实现了常量池技术，即 Byte,Short,Integer,Long,Character,Boolean；前面 4 种包装类默认创建了数值[-128，127] 的相应类型的缓存数据，Character创建了数值在[0,127]范围的缓存数据，Boolean 直接返回True Or False。如果超出对应范围仍然会去创建新的对象。 为啥把缓存设置为[-128，127]区间？（参见issue/461）性能和资源之间的权衡。 123public static Boolean valueOf(boolean b) &#123; return (b ? TRUE : FALSE);&#125; 123456789private static class CharacterCache &#123; private CharacterCache()&#123;&#125; static final Character cache[] = new Character[127 + 1]; static &#123; for (int i = 0; i &lt; cache.length; i++) cache[i] = new Character((char)i); &#125; &#125; 两种浮点数类型的包装类 Float,Double 并没有实现常量池技术。** 123456789Integer i1 = 33;Integer i2 = 33;System.out.println(i1 == i2);// 输出 trueInteger i11 = 333;Integer i22 = 333;System.out.println(i11 == i22);// 输出 falseDouble i3 = 1.2;Double i4 = 1.2;System.out.println(i3 == i4);// 输出 false Integer 缓存源代码： 12345678/***此方法将始终缓存-128 到 127（包括端点）范围内的值，并可以缓存此范围之外的其他值。*/ public static Integer valueOf(int i) &#123; if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high) return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i); &#125; 应用场景： Integer i1=40；Java 在编译的时候会直接将代码封装成 Integer i1=Integer.valueOf(40);，从而使用常量池中的对象。 Integer i1 = new Integer(40);这种情况下会创建新的对象。 123Integer i1 = 40;Integer i2 = new Integer(40);System.out.println(i1==i2);//输出 false Integer 比较更丰富的一个例子: 12345678910111213Integer i1 = 40;Integer i2 = 40;Integer i3 = 0;Integer i4 = new Integer(40);Integer i5 = new Integer(40);Integer i6 = new Integer(0);System.out.println(\"i1=i2 \" + (i1 == i2));System.out.println(\"i1=i2+i3 \" + (i1 == i2 + i3));System.out.println(\"i1=i4 \" + (i1 == i4));System.out.println(\"i4=i5 \" + (i4 == i5));System.out.println(\"i4=i5+i6 \" + (i4 == i5 + i6)); System.out.println(\"40=i5+i6 \" + (40 == i5 + i6)); 结果： 123456i1=i2 truei1=i2+i3 truei1=i4 falsei4=i5 falsei4=i5+i6 true40=i5+i6 true 解释： 语句 i4 == i5 + i6，因为+这个操作符不适用于 Integer 对象，首先 i5 和 i6 进行自动拆箱操作，进行数值相加，即 i4 == 40。然后 Integer 对象无法与数值进行直接比较，所以 i4 自动拆箱转为 int 值 40，最终这条语句转为 40 == 40 进行数值比较。 参考 《深入理解 Java 虚拟机：JVM 高级特性与最佳实践（第二版》 《实战 java 虚拟机》 https://docs.oracle.com/javase/specs/index.html http://www.pointsoftware.ch/en/under-the-hood-runtime-data-areas-javas-memory-model/ https://dzone.com/articles/jvm-permgen-%E2%80%93-where-art-thou https://stackoverflow.com/questions/9095748/method-area-and-permgen 深入解析String#internhttps://tech.meituan.com/2014/03/06/in-depth-understanding-string-intern.html 公众号如果大家想要实时关注我更新的文章以及分享的干货的话，可以关注我的公众号。 《Java面试突击》: 由本文档衍生的专为面试而生的《Java面试突击》V2.0 PDF 版本公众号后台回复 “Java面试突击” 即可免费领取！ Java工程师必备学习资源: 一些Java工程师常用学习资源公众号后台回复关键字 “1” 即可免费无套路获取。","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"https://shang.at/tags/JVM/"}]},{"title":"Java学习-wait&notify","slug":"Java学习-wait-notify","date":"2020-05-22T05:09:37.000Z","updated":"2020-06-03T09:33:41.264Z","comments":true,"path":"post/Java学习-wait-notify/","link":"","permalink":"https://shang.at/post/Java学习-wait-notify/","excerpt":"","text":"","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"wait&notify","slug":"wait-notify","permalink":"https://shang.at/tags/wait-notify/"}]},{"title":"Java学习-Timer","slug":"Java学习-Timer","date":"2020-05-22T02:31:16.000Z","updated":"2020-06-03T09:33:28.680Z","comments":true,"path":"post/Java学习-Timer/","link":"","permalink":"https://shang.at/post/Java学习-Timer/","excerpt":"","text":"","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"Timer","slug":"Timer","permalink":"https://shang.at/tags/Timer/"}]},{"title":"Java学习-NIO","slug":"Java学习-NIO","date":"2020-05-19T18:46:53.000Z","updated":"2020-06-03T09:33:10.621Z","comments":true,"path":"post/Java学习-NIO/","link":"","permalink":"https://shang.at/post/Java学习-NIO/","excerpt":"","text":"Java NIO系列教程（六） 多路复用器Selector ServerSocketChannel：一个面向流的侦听套接字 通道。用于监听是否有新的连接到来，可以调用accept函数获取到来的连接(SocketChannel) SocketChannel：一个面向流的连接套接字 通道。用于从连接中读取数据，和向连接写入数据 Selector：选择器，可以管理一批注册的通道集合的信息和它们的就绪状态(OP_CONNECT|OP_ACCEPT|OP_READ|OP_WRITE) SelectionKey：选择键封装了特定的通道与特定的选择器的注册关系，一个key就代表了一个channel SelectableChannel可以被注册到Selector对象上，然后调用Selector.select()方法可以更新SelectionKeys(即处于某种就绪状态的通道集合)，然后我们就可以遍历这些通过处理相应的事件。比如连接就绪的通道，我们会把他们注册成读状态，等待它读就绪；连接读就绪，我们就可以从中读取数据；连接写就绪，我们就可以向连接写数据","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"NIO","slug":"NIO","permalink":"https://shang.at/tags/NIO/"}]},{"title":"Java学习-线程安全的集合类","slug":"Java学习-线程安全的集合类","date":"2020-05-17T15:16:15.000Z","updated":"2020-05-17T15:16:15.157Z","comments":true,"path":"post/Java学习-线程安全的集合类/","link":"","permalink":"https://shang.at/post/Java学习-线程安全的集合类/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Java学习-Iterator","slug":"Java学习-Iterator","date":"2020-05-17T07:27:13.000Z","updated":"2021-05-13T00:10:50.389Z","comments":true,"path":"post/Java学习-Iterator/","link":"","permalink":"https://shang.at/post/Java学习-Iterator/","excerpt":"","text":"在看Iterator之前，先看一个早期版本的迭代器java.util.Enumeration 1234public interface Enumeration&lt;E&gt; &#123; boolean hasMoreElements(); E nextElement();&#125; 1234* NOTE: The functionality of this interface is duplicated by the Iterator* interface. In addition, Iterator adds an optional remove operation, and* has shorter method names. New implementations should consider using* Iterator in preference to Enumeration. 现在来看Iterator： 12345678910111213141516public interface Iterator&lt;E&gt; &#123; boolean hasNext(); E next(); default void remove() &#123; throw new UnsupportedOperationException(\"remove\"); &#125; default void forEachRemaining(Consumer&lt;? super E&gt; action) &#123; Objects.requireNonNull(action); while (hasNext()) action.accept(next()); &#125;&#125; 注意的点： Iterator在好的设计下可以在遍历的过程中对列表进行增加和删除和修改元素 Iterator在遍历的过程只能进行一遍，即遍历完的对象不能再次遍历， 因为大多数Iterator在实现的过程中都是维护了了cursor指针，这个指针一般只会增加，不会减少 同时大都没有重置cursor指针的接口 关键是看Iterator的设计如何 例如下面的ListIterator 1234567891011public interface ListIterator&lt;E&gt; extends Iterator&lt;E&gt; &#123; boolean hasNext(); E next(); boolean hasPrevious(); E previous(); int nextIndex(); int previousIndex(); void remove(); void set(E e); void add(E e);&#125; ListIterator在原来的Iterator的基础上扩展了，使之可以往前遍历，同时可以修改和增加元素","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"JAVA-Iterator","slug":"JAVA-Iterator","permalink":"https://shang.at/tags/JAVA-Iterator/"}]},{"title":"Java学习-函数式编程","slug":"Java学习-函数式编程","date":"2020-05-17T00:38:33.000Z","updated":"2020-06-03T09:32:49.333Z","comments":true,"path":"post/Java学习-函数式编程/","link":"","permalink":"https://shang.at/post/Java学习-函数式编程/","excerpt":"","text":"Java8之后加入了一种全新的方式来实现方法(功能)作为参数传递的机制：lambda表达式 像python语言，天生就支持将function作为参数传递给函数. 可以想象，既然是一种实现方法作为参数传递的机制，java是一种面向对象的编程语言，也就是说在java中除了原始数据类型之外，都是对象： 1234567891011&gt; @FunctionalInterface&gt; interface CharBinaryOperator &#123;&gt; String applyAsChar(char left, char right);&gt; &#125;&gt; &gt; CharBinaryOperator charBinaryOperator = (char a, char b) -&gt; &#123;&gt; System.out.println(a);&gt; System.out.println(b);&gt; return String.valueOf(a + b);&gt; &#125;;&gt; &gt; charBinaryOperator instanceof CharBinaryOperator charBinaryOperator是CharBinaryOperator的一个实例 CharBinaryOperator.class instanceof Class CharBinaryOperator.class是Class的一个实例 Class.class instanceof Class Class.class同时也是Class的一个实例 在java中，传递的参数要么是原始数据类型，要么是对象(类型也是对象，所以能够传递)，不能是其他的类型。在JDK8之前，要想将一个功能传递到函数内部(这一般会被称为函数回调，是大多数异步编程的常用套路：到达某个时间节点或满足某中情况触发一个操作)，那么就只能显示的先定义一个接口，然后创建一个实现了这个接口的类，然后再实例化这个类得到一个对象，最后将这个对象作为参数传入函数，函数内部调用对象实现的方法，如： 123456789CharBinaryOperator charBinaryOperator1 = new CharBinaryOperator() &#123; @Override public String applyAsChar(char a, char b) &#123; System.out.println(a); System.out.println(b); return String.valueOf(a + b); &#125;&#125;;charBinaryOperator.applyAsChar('a', 'b'); 在JDK8及之后，我们不需要再显示的做这一系列的事情(当然你这么做也不会有问题)。 在JDK8及之后，所有满足条件的interface都会被解释函数式接口，即都可以通过lambda表达式的形式代替上述流程 什么样的interface才算满足条件呢？ 只声明了一个未实现的函数的interface就可以。在JDK8及以后，interface中定义的函数也可以有默认的实现 在声明接口的时候，可以使用java.lang.FunctionalInterface注解，表示该interface是一个函数式接口(当然可以不加，compiler会自动判断) 如果在有多个未实现的函数的interface上加这个注解的时候，编译阶段就会报错：Multiple non-overriding abstract methods found in interface OOXX lambda表达式只不过是为了实现这个机制的一种解决方案，可以提高开发效率，同时隐藏了interface的定义细节，compilier完全是按照参数列表来推断当前的lambda表达式是和哪一个interface绑定的(compilier直接找到接口的定义，不是推断的)。如果没有预定义的，那么就会在编译期间报错，所以在java中lambda表达式的使用是有一定的限制的。 同时，在使用JDK预定义的操作时，在内部是调用了接口内定义的那个具体的函数的，所以对于开发者来说也是透明的，如列表的forEach()函数 12345678910111213&gt; default void forEach(Consumer&lt;? super T&gt; action) &#123;&gt; Objects.requireNonNull(action);&gt; for (T t : this) &#123;&gt; action.accept(t);&gt; &#125;&gt; &#125;&gt; // 我们在使用的时候是这样的&gt; List&lt;Integer&gt; integers = new ArrayList&lt;&gt;();&gt; integers.add(1);&gt; integers.forEach(i -&gt; &#123;&gt; System.out.println(i);&gt; &#125;);&gt; &gt;&gt;&gt; 本质上，还是要先有interface的定义(在JDK中已经预定义了大部分的interface，所以我们才不用自己手动定义，我上面的例子中就是一个没有被预定义的例子)，运行结果也是创建了一个实现了指定接口的对象，然后将对象作为参数传递给函数 如果我们完全脱离了JDK预定义的操作，那么我们就需要自己定义innterface，并且在我们使用该接口的地方显示的声明方法的使用，但是在外层传递方法参数的调用，我们仍可以使用简洁明了的lambda表达式，无论怎么说，lambda表达式的这种机制极大的方便了开发人员 Lambda 表达式和匿名类之间的区别 this 关键字。对于匿名类 this 关键字解析为匿名类，而对于 Lambda 表达式，this 关键字解析为包含写入 Lambda 的类。 JDK中预定义的interface在java.util.function可以看到全部的预定义的interface，以下四种是最有代表性的 Function R apply(T) Function compose(Function&lt;? super V, ? extends T&gt; before) Function andThen(Function&lt;? super R, ? extends V&gt; after) Function identity() Consumer void accept(T) Consumer&lt; T&gt; addThen(Consumer&lt;? super T&gt;) Predicate boolean test() Predicate&lt; T&gt; add(Predicate&lt;? super T&gt;) Predicate&lt; T&gt; negate() Predicate&lt; T&gt; or(Predicate&lt;? super T&gt;) Predicate&lt; T&gt; isEqual(Object) Supplier T get() Lambda 表达式的例子1 线程初始化线程可以初始化如下： 123456789101112// Old waynew Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println(\"Hello world\"); &#125;&#125;).start();// New waynew Thread( () -&gt; System.out.println(\"Hello world\")).start(); 2 事件处理事件处理可以用 Java 8 使用 Lambda 表达式来完成。以下代码显示了将 ActionListener 添加到 UI 组件的新旧方式： 123456789101112// Old waybutton.addActionListener(new ActionListener() &#123; @Override public void actionPerformed(ActionEvent e) &#123; System.out.println(\"Hello world\"); &#125;&#125;);// New waybutton.addActionListener( (e) -&gt; &#123; System.out.println(\"Hello world\");&#125;); 3 遍例输出（方法引用）输出给定数组的所有元素的简单代码。请注意，还有一种使用 Lambda 表达式的方式。 1234567891011// old wayList&lt;Integer&gt; list = Arrays.asList(1, 2, 3, 4, 5, 6, 7);for (Integer n : list) &#123; System.out.println(n);&#125;// 使用 -&gt; 的 Lambda 表达式list.forEach(n -&gt; System.out.println(n));// 使用 :: 的 Lambda 表达式list.forEach(System.out::println); 6.4 逻辑操作输出通过逻辑判断的数据。 123456789101112131415161718192021222324252627282930313233343536package com.wuxianjiezh.demo.lambda;import java.util.Arrays;import java.util.List;import java.util.function.Predicate;public class Main &#123; public static void main(String[] args) &#123; List&lt;Integer&gt; list = Arrays.asList(1, 2, 3, 4, 5, 6, 7); System.out.print(\"输出所有数字：\"); evaluate(list, (n) -&gt; true); System.out.print(\"不输出：\"); evaluate(list, (n) -&gt; false); System.out.print(\"输出偶数：\"); evaluate(list, (n) -&gt; n % 2 == 0); System.out.print(\"输出奇数：\"); evaluate(list, (n) -&gt; n % 2 == 1); System.out.print(\"输出大于 5 的数字：\"); evaluate(list, (n) -&gt; n &gt; 5); &#125; public static void evaluate(List&lt;Integer&gt; list, Predicate&lt;Integer&gt; predicate) &#123; for (Integer n : list) &#123; if (predicate.test(n)) &#123; System.out.print(n + \" \"); &#125; &#125; System.out.println(); &#125;&#125; 运行结果： 12345输出所有数字：1 2 3 4 5 6 7 不输出：输出偶数：2 4 6 输出奇数：1 3 5 7 输出大于 5 的数字：6 7 6.4 Stream API 示例java.util.stream.Stream接口 和 Lambda 表达式一样，都是 Java 8 新引入的。所有 Stream 的操作必须以 Lambda 表达式为参数。Stream 接口中带有大量有用的方法，比如 map() 的作用就是将 input Stream 的每个元素，映射成output Stream 的另外一个元素。 下面的例子，我们将 Lambda 表达式 x -&gt; x*x 传递给 map() 方法，将其应用于流的所有元素。之后，我们使用 forEach 打印列表的所有元素。 12345678910// old wayList&lt;Integer&gt; list = Arrays.asList(1,2,3,4,5,6,7);for(Integer n : list) &#123; int x = n * n; System.out.println(x);&#125;// new wayList&lt;Integer&gt; list = Arrays.asList(1,2,3,4,5,6,7);list.stream().map((x) -&gt; x*x).forEach(System.out::println); 下面的示例中，我们给定一个列表，然后求列表中每个元素的平方和。这个例子中，我们使用了 reduce() 方法，这个方法的主要作用是把 Stream 元素组合起来。 12345678910111213// old wayList&lt;Integer&gt; list = Arrays.asList(1,2,3,4,5,6,7);int sum = 0;for(Integer n : list) &#123; int x = n * n; sum = sum + x;&#125;System.out.println(sum);// new wayList&lt;Integer&gt; list = Arrays.asList(1,2,3,4,5,6,7);int sum = list.stream().map(x -&gt; x*x).reduce((x,y) -&gt; x + y).get();System.out.println(sum);","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"函数式编程","slug":"函数式编程","permalink":"https://shang.at/tags/函数式编程/"}]},{"title":"Java学习-集合类","slug":"Java学习-集合类","date":"2020-05-16T23:27:00.000Z","updated":"2020-06-03T09:32:54.364Z","comments":true,"path":"post/Java学习-集合类/","link":"","permalink":"https://shang.at/post/Java学习-集合类/","excerpt":"","text":"[TOC] 总览 注：这里只列举了单一线线程使用的集合对象(Vector除外) JVAV中列表类集合，按照数据的存储方式可以分为两大类：基于数组和基于链表。两种方式各有好处，需要根据实际业务场景做出选择。 数组 优点 支持随机访问，给定下标的访问是O(1)的时间复杂度 缺点 内存必须是连续的，否则会申请空间失败 查找、插入、扩容、删除都是O(n)的时间复杂度 有容量的限制，增加节点时，可能会因为数组大小不够导致扩容，扩容的时间复杂度是O(n)的 使用注意 最好能够预估数据的最大容量，可以预先设计capacity，尽量避免扩容操作 但是在特别的使用场景下，基于数组的实现效率会更好，比如下面要说的ArrayDeque 具体实现 ArrayList Vector Stack ArrayDeque 链表 优点 内存不用是连续的 插入、删除都是O(1)的时间复杂度 没有容量的限制，按理说限制就是JVAV堆的大小限制 缺点 查找是O(n)的时间复杂度 使用 单独使用链表的时候，还挺少的，毕竟一个没有附加特性的链表结构，仅仅只能够做到新增和删除的时间复杂度为O(1)，但是查询却需要O(n)，并且还需要额外的空间存储链表结构。数组可以通过预估容量的方式尽量减少扩容的操作，对比发现，使用基于数组的集合性价比更高 具体实现 LinkList LinkList在定位低index个元素的时候，有个优化的点可以学习 123456789101112131415Node&lt;E&gt; node(int index) &#123; // assert isElementIndex(index); if (index &lt; (size &gt;&gt; 1)) &#123; Node&lt;E&gt; x = first; for (int i = 0; i &lt; index; i++) x = x.next; return x; &#125; else &#123; Node&lt;E&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prev; return x; &#125;&#125; 总结：可以发现，基于数组和基于链表的集合实现方式，想要从他们中查询到具体的元素，时间复杂度都是O(n)的，这是因为，这里仅仅考虑了数据的存储方式，并没有额外的信息给出来，所以是没有办法加速查询的。要想实现加速查询，那么就必须在这基础上增加新的特性： 数组 有序性：可以借助有序性使用二分查找，把查找时间复杂度降到O(log n) 链表 建立树结构： 二叉搜索树：前序遍历就是正向排序，可以把查询的时间复杂度降到O(log n)，但是要维护二叉搜索，尽量保证他是平衡的(但是这个的时间复杂度是O(1)的) 堆：查找最大(最小)值是O(1)的时间复杂度 升维：比如跳表，就是在有序的链表上建立多级索引来实现加速查询的，可以把查找时间复杂度降到O(log n)，但是在新增和删除节点时需要维护多级索引(但是这个的时间复杂度是O(1)的) List 接口信息如下： Method Return Comment Insert add(E) boolean 向队列加入元素，如果空间不足，会触发扩容 Insert add(int, E) void 向指定位置插入元素，可能会抛IndexOutOfBoundsException Remove remove(Object) boolean 移除指定的元素，没有的话返回false，有的话返回true Remove remove(int) E 移除指定index的元素，可能会抛IndexOutOfBoundsException Examine get(int) E 返回指定index的元素，可能会抛IndexOutOfBoundsException Update set(int, E) E 更新指定index的元素，可能会抛IndexOutOfBoundsException 在List的源码中发现多处这样的代码： 12345678910111213// 只返回第一个遇到的o，当o为null的时候，o.equals会报错public int indexOf(Object o) &#123; if (o == null) &#123; for (int i = 0; i &lt; size; i++) if (elementData[i]==null) return i; &#125; else &#123; for (int i = 0; i &lt; size; i++) if (o.equals(elementData[i])) return i; &#125; return -1;&#125; 详看扩容操作 1234567891011121314151617181920212223242526272829303132333435363738394041424344public boolean add(E e) &#123; ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true;&#125;// DEFAULT_CAPACITY=10 private void ensureCapacityInternal(int minCapacity) &#123; if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); &#125; ensureExplicitCapacity(minCapacity); &#125; private void ensureExplicitCapacity(int minCapacity) &#123; modCount++; // overflow-conscious code if (minCapacity - elementData.length &gt; 0) grow(minCapacity); &#125; private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; // newCapacity = int(1.5*oldCapacity) int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity); &#125; // 整形溢出：Integer.MAX_VALUE + 8&lt;0 private static int hugeCapacity(int minCapacity) &#123; if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); return (minCapacity &gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE; &#125; Queue 设计了一套支持队列操作的接口，如下： Method Return Comment Insert add(E) boolean 向队列添加一个元素，如果没有空间会抛出IllegalStateException Insert offer(E) boolean 向队列添加一个元素，如果没有空间会返回false Remove remove() E 移除并返回头结点，如果队列为空的话，会抛NoSuchElementException Remove poll() E 移除并返回头结点，如果队列为空的话，会返回null Examine element() E 返回头结点，如果队列为空的话，会抛NoSuchElementException Examine peek() E 返回头结点，如果队列为空的话，会返回null Deque Stack 总结：通常使用ArrayDeque来作为先进先出的Queue，后进先出的Stack","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"JAVA集合类-列表","slug":"JAVA集合类-列表","permalink":"https://shang.at/tags/JAVA集合类-列表/"}]},{"title":"分布式服务框架-IPC&RPC","slug":"分布式服务框架-IPC-RPC","date":"2020-05-14T03:38:18.000Z","updated":"2020-06-02T22:35:24.864Z","comments":true,"path":"post/分布式服务框架-IPC-RPC/","link":"","permalink":"https://shang.at/post/分布式服务框架-IPC-RPC/","excerpt":"","text":"","categories":[{"name":"分布式","slug":"分布式","permalink":"https://shang.at/categories/分布式/"}],"tags":[{"name":"IPC&RPC","slug":"IPC-RPC","permalink":"https://shang.at/tags/IPC-RPC/"}]},{"title":"Java学习-动态代理","slug":"Java学习-动态代理","date":"2020-05-14T03:34:00.000Z","updated":"2020-06-03T03:01:02.693Z","comments":true,"path":"post/Java学习-动态代理/","link":"","permalink":"https://shang.at/post/Java学习-动态代理/","excerpt":"","text":"","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"动态代理","slug":"动态代理","permalink":"https://shang.at/tags/动态代理/"}]},{"title":"Mysql学习-事务和隔离级别","slug":"Mysql学习-事务和隔离级别","date":"2020-05-12T17:31:26.000Z","updated":"2020-06-25T01:16:15.635Z","comments":true,"path":"post/Mysql学习-事务和隔离级别/","link":"","permalink":"https://shang.at/post/Mysql学习-事务和隔离级别/","excerpt":"","text":"MYSQL事务和隔离级别一、事务事务是由一组SQL语句组成的逻辑处理单元，是满足 ACID 特性的一组操作，可以通过 Commit 提交一个事务，也可以使用 Rollback 进行回滚。事务具有以下4个属性，通常简称为事务的ACID属性: 原子性（Atomicity）：事务是一个原子操作单元，其对数据的修改，要么全都执行，要么全都不执行。比如在同一个事务中的SQL语句，要么全部执行成功，要么全部执行失败。回滚可以用日志来实现，日志记录着事务所执行的修改操作，在回滚时反向执行这些修改操作即可。 一致性（Consistent）：在事务开始和完成时，数据都必须保持一致状态。这意味着所有相关的数据规则都必须应用于事务的修改，以保持数据的完整性；事务结束时，所有的内部数据结构（如B树索引或双向链表）也都必须是正确的。 以转账为例子，A向B转账，假设转账之前这两个用户的钱加起来总共是2000，那么A向B转账之后，不管这两个账户怎么转，A用户的钱和B用户的钱加起来的总额还是2000，这个就是事务的一致性。 隔离性（Isolation）：数据库系统提供一定的隔离机制，保证事务在不受外部并发操作影响的“独立”环境执行。 隔离性是当多个用户并发访问数据库时，比如操作同一张表时，数据库为每一个用户开启的事务，不能被其他事务的操作所干扰，多个并发事务之间要相互隔离。即要达到这么一种效果：对于任意两个并发的事务 T1 和 T2，在事务 T1 看来，T2 要么在 T1 开始之前就已经结束，要么在 T1 结束之后才开始，这样每个事务都感觉不到有其他事务在并发地执行。 持久性（Durable）：事务完成之后，它对于数据的修改是永久性的，即使出现系统故障也能够保持。 可以通过数据库备份和恢复来实现，在系统发生奔溃时，使用备份的数据库进行数据恢复。 MySQL 默认采用自动提交模式。也就是说，如果不显式使用 START TRANSACTION 语句来开始一个事务，那么每个查询都会被当做一个事务自动提交。 这几个特性不是一种平级关系： 只有满足一致性，事务的执行结果才是正确的。 在无并发的情况下，事务串行执行，隔离性一定能够满足。此时要只要能满足原子性，就一定能满足一致性。 在并发的情况下，多个事务并发执行，事务不仅要满足原子性，还需要满足隔离性，才能满足一致性。 事务满足持久化是为了能应对数据库奔溃的情况。 二、并发一致性问题1、更新丢失(Lost Update)T1 和 T2 两个事务都对一个数据进行修改，T1 先修改，T2 随后修改，T2 的修改覆盖了 T1 的修改。 例如，两个程序员修改同一java文件。每程序员独立地更改其副本，然后保存更改后的副本，这样就覆盖了原始文档。最后保存其更改副本的编辑人员覆盖前一个程序员所做的更改。 如果在一个程序员完成并提交事务之前，另一个程序员不能访问同一文件，则可避免此问题。 2、脏读一句话：事务B读取到了事务A已修改但尚未提交的的数据，还在这个数据基础上做了操作。此时，如果A事务回滚Rollback，B读取的数据无效，不符合一致性要求。 解决办法: 把数据库的事务隔离级别调整到 READ_COMMITTED T1 修改一个数据，T2 随后读取这个数据。如果 T1 撤销了这次修改，那么 T2 读取的数据是脏数据。 3、不可重复读(Non-Repeatable Reads) 在一个事务内，多次读同一个数据。在这个事务还没有结束时，另一个事务也访问该同一数据。那么，在第一个事务的两次读数据之间。由于第二个事务的修改，那么第一个事务读到的数据可能不一样，这样就发生了在一个事务内两次读到的数据是不一样的，因此称为不可重复读，即原始读取不可重复。 一句话：一个事务范围内两个相同的查询却返回了不同数据。 同时操作，事务1分别读取事务2操作时和提交后的数据，读取的记录内容不一致。不可重复读是指在同一个事务内，两个相同的查询返回了不同的结果。 解决办法: 如果只有在修改事务完全提交之后才可以读取数据，则可以避免该问题。把数据库的事务隔离级别调整到REPEATABLE_READ T2 读取一个数据，T1 对该数据做了修改。如果 T2 再次读取这个数据，此时读取的结果和第一次读取的结果不同。 4、幻读一个事务T1按相同的查询条件重新读取以前检索过的数据，却发现其他事务T2插入了满足其查询条件的新数据，这种现象就称为“幻读”。（和可重复读类似，但是事务 T2 的数据操作仅仅是插入和删除，不是修改数据，读取的记录数量前后不一致） 一句话：事务A 读取到了事务B提交的新增数据，不符合隔离性。 解决办法: 如果在操作事务完成数据处理之前，任何其他事务都不可以添加新数据，则可避免该问题。把数据库的事务隔离级别调整到 SERIALIZABLE_READ。 T1 读取某个范围的数据，T2 在这个范围内插入新的数据，T1 再次读取这个范围的数据，此时读取的结果和和第一次读取的结果不同。 三、事务隔离级别“脏读”、”不可重复读”和”幻读”，其实都是数据库读一致性问题，必须由数据库提供一定的事务隔离机制来解决。 数据库的事务隔离越严格，并发副作用越小，但付出的代价也就越大，因为事务隔离实质上就是使事务在一定程度上 “串行化”进行，这显然与“并发”是矛盾的。同时，不同的应用对读一致性和事务隔离程度的要求也是不同的，比如许多应用对“不可重复读”和“幻读”并不敏感，可能更关心数据并发访问的能力。 MYSQL常看当前数据库的事务隔离级别：show variables like &#39;tx_isolation&#39;; 1、读未提交 (Read Uncommitted)最低的隔离等级，允许其他事务看到没有提交的数据，会导致脏读。 2、读已提交 (Read Committed)被读取的数据可以被其他事务修改，这样可能导致不可重复读。也就是说，事务读取的时候获取读锁，但是在读完之后立即释放(不需要等事务结束)，而写锁则是事务提交之后才释放，释放读锁之后，就可能被其他事务修改数据。该等级也是 SQL Server 默认的隔离等级。 3、可重复读(Repeatable Read)所有被 Select 获取的数据都不能被修改，这样就可以避免一个事务前后读取数据不一致的情况。但是却没有办法控制幻读，因为这个时候其他事务不能更改所选的数据，但是可以增加数据，即前一个事务有读锁但是没有范围锁，为什么叫做可重复读等级呢？那是因为该等级解决了下面的不可重复读问题。(引申：现在主流数据库都使用 MVCC 并发控制，使用之后RR（可重复读）隔离级别下是不会出现幻读的现象。) MYSQL默认是REPEATABLE-READ。 4、串行化(Serializable)所有事务一个接着一个的执行，这样可以避免幻读 (phantom read)，对于基于锁来实现并发控制的数据库来说，串行化要求在执行范围查询的时候，需要获取范围锁，如果不是基于锁实现并发控制的数据库，则检查到有违反串行操作的事务时，需回滚该事务。 5、总结 读未提交: 一个事务还没提交时，它做的变更就能被别的事务看到。 读提交: 一个事务提交之后，它做的变更才会被其他事务看到。 可重复读 : 一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。 串行化: 顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。 四个级别逐渐增强，每个级别解决一个问题，事务级别越高，性能越差，大多数环境(Read committed 就可以用了) 隔离级别 读数据一致性 脏读 不可重复读 幻读 未提交读 最低级别 √ √ √ 提交读 语句级 × √ √ 可重复读 事务级 × × √ 可串行化 最高级别,事务级 × × × 参考","categories":[{"name":"数据库","slug":"数据库","permalink":"https://shang.at/categories/数据库/"}],"tags":[{"name":"mysql事务和隔离级别","slug":"mysql事务和隔离级别","permalink":"https://shang.at/tags/mysql事务和隔离级别/"}]},{"title":"Mysql学习-第三范式","slug":"Mysql学习-第三范式","date":"2020-05-12T17:19:15.000Z","updated":"2020-12-11T08:24:18.315Z","comments":true,"path":"post/Mysql学习-第三范式/","link":"","permalink":"https://shang.at/post/Mysql学习-第三范式/","excerpt":"","text":"第一范式(1NF) 数据库表的每一列都是不可分割的基本数据项，同一列中不能有多个值，即实体中的某个属性不能有多个值或者不能有重复的属性 符合1NF的关系中的每个属性都不可再分 有两点要求： schema定义：每个属性不可再分，即字段的含义要明确，同一个字段不应该有多于1个的含义 图中的这种schema在RDBMS中是不可能存在的，也就是无法创建的。可以改成如下的schema: 存储的数据：同一列中不能有多个值 图中的一个字段里面存了多个值，这种情况在RDBMS中是可以存在的，但是该字段是可再分的，应该。可以将数据分成多条存储，如下图 第二范式(2NF) 满足第一范式 没有部分依赖 在同一个表中，不能存在某些字段依赖一些键，而另一些字段依赖另外一些键 员工表的一个候选键是{id，mobile，deptNo}，而deptName依赖于deptNo，同样 name 依赖于 id，因此不是 2NF的。为了满足第二范式的条件，需要将这个表拆分成employee、dept、employee_dept、employee_mobile四个表 不满足2NF的表，可能存在的问题：修改异常、新增异常、删除异常 第三范式(3NF) 满足第二范式 没有传递依赖 在同一个表中，不要存在字段A依赖字段B，同时字段B依赖字段C，推导出来字段A间接依赖字段C的关系。 员工表的province、city、district依赖于zip，而zip依赖于id，换句话说，province、city、district传递依赖于id，违反了 3NF 规则。为了满足第三范式的条件，可以将这个表拆分成employee和zip两个表 但是这种关系也不是一定不能存在，视具体的业务而定吧 示例假设有一个名为employee的员工表，它有九个属性：id(员工编号)、name(员工名称)、mobile(电话)、zip(邮编)、province(省份)、city(城市)、district(区县)、deptNo(所属部门编号)、deptName(所属部门名称)、表总数据如下： id name mobile zip province city district deptNo deptName 101 张三 1391000000113910000002 100001 北京 北京 海淀区 D1 部门1 101 张三 1391000000113910000002 100001 北京 北京 海淀区 D2 部门2 102 李四 13910000003 200001 上海 上海 静安区 D3 部门3 103 王五 13910000004 510001 广东省 广州 白云区 D4 部门4 103 王五 13910000004 510001 广东省 广州 白云区 D5 部门 5 将上表改成满足第1范式，如下： id name mobile zip province city district deptNo deptName 101 张三 13910000001 100001 北京 北京 海淀区 D1 部门1 101 张三 13910000002 100001 北京 北京 海淀区 D1 部门1 101 张三 13910000001 100001 北京 北京 海淀区 D2 部门2 101 张三 13910000002 100001 北京 北京 海淀区 D2 部门2 102 李四 13910000003 200001 上海 上海 静安区 D3 部门3 103 王五 13910000004 510001 广东省 广州 白云区 D4 部门4 103 王五 13910000004 510001 广东省 广州 白云区 D5 部门5 仍存在的问题 修改异常：上表中张三、王五都有多条记录，因为他隶属于两个部门。如果我们要修改王五的地址，必修修改两行记录。假如一个部门得到了王五的新地址并进行了更新，而另一个部门没有，那么此时王五在表中会存在两个不同的地址，导致了数据不一致 新增异常：假如一个新员工假如公司，他正处于入职培训阶段，还没有被正式分配到某个部门，如果deptNo字段不允许为空，我们就无法向employee表中新增该员工的数据。 删除异常：假设公司撤销了D3部门，那么在删除deptNo为D3的行时，会将李四的信息也一并删除。因为他隶属于D3这一部门。 为了解决上面的问题，我们可以将上述表设计成满足3NF 在关系数据库模型设计中，一般需要满足第三范式的要求。如果一个表具有良好的主外键设计，就应该是满足3NF的表。规范化带来的好处是通过减少数据冗余提高更新数据的效率，同时保证数据完整性。然而，我们在实际应用中也要防止过度规范化的问题。规范化程度越高，划分的表就越多，在查询数据时越有可能使用表连接操作。而如果连接的表过多，会影响查询性能。关键的问题是要依据业务需求，仔细权衡数据查询和数据更新关系，指定最合适的规范化程度。不要为了遵循严格的规范化规则而修改业务需求。 参考","categories":[{"name":"数据库","slug":"数据库","permalink":"https://shang.at/categories/数据库/"}],"tags":[{"name":"mysql第三范式","slug":"mysql第三范式","permalink":"https://shang.at/tags/mysql第三范式/"}]},{"title":"Python学习-函数参数传递","slug":"Python学习-函数参数传递","date":"2020-04-19T04:00:49.000Z","updated":"2020-04-19T04:26:43.749Z","comments":true,"path":"post/Python学习-函数参数传递/","link":"","permalink":"https://shang.at/post/Python学习-函数参数传递/","excerpt":"","text":"在Python(估计也适用于其他的语言)中，函数参数的传递分为两类 值传递和引用传递，实际上这两类传递类型都是属于变量传值，即： 值传递：将实际参数值复制一份传递到函数内，这样在函数内对参数进行修改，就不会影响到原参数 引用传递：将实际参数的地址直接传递到函数内，那么在函数内对参数所进行的修改，将可能会影响到原参数 要注意的是，在函数内修改参数，实际上又分为两种情况(仅说引用传递)： 1、对参数(a)重新进行赋值操作(a=new_obj)，此时，实际上修改的已经不是传递给函数的最初的参数(a)了，它已经指向了其他的内存地址，这时再修改a，实际上就和之前的对象没有任何关系了 2、直接对a进行修改，比如说a.name=’sdd’，这时，原始的对象就会发生变化","categories":[{"name":"Python","slug":"Python","permalink":"https://shang.at/categories/Python/"}],"tags":[{"name":"python学习","slug":"python学习","permalink":"https://shang.at/tags/python学习/"}]},{"title":"Python学习-OrderedDict","slug":"Python学习-OrderedDict","date":"2020-04-16T08:50:57.000Z","updated":"2020-07-01T09:20:30.458Z","comments":true,"path":"post/Python学习-OrderedDict/","link":"","permalink":"https://shang.at/post/Python学习-OrderedDict/","excerpt":"","text":"12from collections import OrderedDict # 记录插入顺序的dict，操作方式和dict一样。# 是基于dict和双端队列实现，可以用来实现LRUcache","categories":[{"name":"Python","slug":"Python","permalink":"https://shang.at/categories/Python/"}],"tags":[{"name":"OrderedDict","slug":"OrderedDict","permalink":"https://shang.at/tags/OrderedDict/"}]},{"title":"Python学习-bisect","slug":"Python学习-bisect","date":"2020-04-16T08:47:50.000Z","updated":"2020-07-01T09:20:11.928Z","comments":true,"path":"post/Python学习-bisect/","link":"","permalink":"https://shang.at/post/Python学习-bisect/","excerpt":"","text":"这个模块对有序列表提供了支持，使得他们可以在插入新数据仍然保持有序。对于长列表，如果其包含元素的比较操作十分昂贵的话，这可以是对更常见方法的改进 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192\"\"\"Bisection algorithms.\"\"\"def insort_right(a, x, lo=0, hi=None): \"\"\"Insert item x in list a, and keep it sorted assuming a is sorted. If x is already in a, insert it to the right of the rightmost x. Optional args lo (default 0) and hi (default len(a)) bound the slice of a to be searched. \"\"\" if lo &lt; 0: raise ValueError('lo must be non-negative') if hi is None: hi = len(a) while lo &lt; hi: mid = (lo+hi)//2 if x &lt; a[mid]: hi = mid else: lo = mid+1 a.insert(lo, x)insort = insort_right # backward compatibilitydef bisect_right(a, x, lo=0, hi=None): \"\"\"Return the index where to insert item x in list a, assuming a is sorted. The return value i is such that all e in a[:i] have e &lt;= x, and all e in a[i:] have e &gt; x. So if x already appears in the list, a.insert(x) will insert just after the rightmost x already there. Optional args lo (default 0) and hi (default len(a)) bound the slice of a to be searched. \"\"\" if lo &lt; 0: raise ValueError('lo must be non-negative') if hi is None: hi = len(a) while lo &lt; hi: mid = (lo+hi)//2 if x &lt; a[mid]: hi = mid else: lo = mid+1 return lobisect = bisect_right # backward compatibilitydef insort_left(a, x, lo=0, hi=None): \"\"\"Insert item x in list a, and keep it sorted assuming a is sorted. If x is already in a, insert it to the left of the leftmost x. Optional args lo (default 0) and hi (default len(a)) bound the slice of a to be searched. \"\"\" if lo &lt; 0: raise ValueError('lo must be non-negative') if hi is None: hi = len(a) while lo &lt; hi: mid = (lo+hi)//2 if a[mid] &lt; x: lo = mid+1 else: hi = mid a.insert(lo, x)def bisect_left(a, x, lo=0, hi=None): \"\"\"Return the index where to insert item x in list a, assuming a is sorted. The return value i is such that all e in a[:i] have e &lt; x, and all e in a[i:] have e &gt;= x. So if x already appears in the list, a.insert(x) will insert just before the leftmost x already there. Optional args lo (default 0) and hi (default len(a)) bound the slice of a to be searched. \"\"\" if lo &lt; 0: raise ValueError('lo must be non-negative') if hi is None: hi = len(a) while lo &lt; hi: mid = (lo+hi)//2 if a[mid] &lt; x: lo = mid+1 else: hi = mid return lo# Overwrite above definitions with a fast C implementationtry: from _bisect import *except ImportError: pass","categories":[{"name":"Python","slug":"Python","permalink":"https://shang.at/categories/Python/"}],"tags":[{"name":"bisect","slug":"bisect","permalink":"https://shang.at/tags/bisect/"}]},{"title":"数据结构与算法学习笔记-查找算法","slug":"数据结构与算法学习笔记-查找算法","date":"2020-04-10T00:58:45.000Z","updated":"2020-04-10T01:00:39.177Z","comments":true,"path":"post/数据结构与算法学习笔记-查找算法/","link":"","permalink":"https://shang.at/post/数据结构与算法学习笔记-查找算法/","excerpt":"","text":"总结 查找算法 时间复杂度 二分查找 O(logn) O(logn)","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"}],"tags":[{"name":"查找算法","slug":"查找算法","permalink":"https://shang.at/tags/查找算法/"}]},{"title":"Spark应用之import spark.implicits._","slug":"Spark应用之import-spark-implicits","date":"2020-03-25T09:10:54.000Z","updated":"2021-04-11T04:27:58.654Z","comments":true,"path":"post/Spark应用之import-spark-implicits/","link":"","permalink":"https://shang.at/post/Spark应用之import-spark-implicits/","excerpt":"简介：Spark中隐式转换的解释","text":"简介：Spark中隐式转换的解释 在初期使用spark的时候，大家都会遇见一个很奇怪的写法import spark.implicits._ 这里面包含了四个关键字：import、spark、implicits、_ import和_实际上是Scala中包引入的写法，表示引入指定包内的所有成员 本文主要想记录一下另外两个关键字：spark、implicits 关键字一：sparkspark在这里是这样产生的： 12345val conf: SparkConf = new SparkConf().setMaster(\"local\").setAppName(\"test\")val spark: SparkSession = SparkSession .builder() .config(conf) .getOrCreate() 那么这就会有点让人奇怪的地方了，在scala中import到底是如何工作的呢？为什么可以import一个实例对象呢？ 经过查询发现，scala确实能够导入运行时对象实例的成员，举例如下： 1234567891011121314151617181920scala&gt; case class User(name:String, age:Int)defined class Userscala&gt; val a=User(\"zzz\",39)a: User = User(zzz,39)scala&gt; println(\"%s's age is %d\" format (a.name, a.age))zzz's age is 39scala&gt; import a._import a._scala&gt; println(\"%s's age is %d\" format (name, age))zzz's age is 39scala&gt; nameres3: String = zzzscala&gt; ageres4: Int = 39 定义了一个case class User，然后执行了一次import a._，然后发现可以直接使用a实例对象的属性变量，而不需要加a.的前缀。 那么为什么在spark中需要这么做呢？我们点进去spark.implicits，会发现源码是这样的： 12345class SparkSession &#123; object implicits extends SQLImplicits with Serializable &#123; protected override def _sqlContext: SQLContext = SparkSession.this.sqlContext &#125;&#125; implicits是SparkSession的一个内部单例对象，它有一个_sqlContext的函数，用于获取SQLContext，但是它必须调用当前正在运行的SparkSession实例，通过调用它的sqlContext属性来获取。源码中的SparkSession.this实际上是调用了SparkSession class的伴生对象的.this，这个this实际上就是正在运行的SparkSession实例。 具体的隐式转换都可以在org.apache.spark.sql.SQLImplicits中看到 这里实际上是scala中class与伴生对象的爱恨纠缠以及import关键字的使用技巧 下面给出一个测试案例： 1234567891011121314151617181920212223// TestObject.scalaclass TestObject(name: String, age: Int) &#123; val user: User = new User(name, age) object innerObj &#123; val user1: User = TestObject.this.user def say(name: String) &#123; println(s\"$&#123;name&#125; innerObj say TestObject.this = $&#123;TestObject.this&#125;\") &#125; &#125; def say2(): Unit = &#123; println(s\"TestObject say2 TestObject.this = $&#123;TestObject.this&#125;\") &#125;&#125;object TestObject &#123;&#125; 1234567// User.scalaclass User(name: String, age: Int) &#123; def say(): Unit = &#123; println(\"%s's age is %d\" format(name, age)) &#125;&#125; 123456789101112131415161718192021222324252627// Test.scalaobject Test &#123; def main(args: Array[String]): Unit = &#123; val testObject = new TestObject(\"sfa\", 12) println(s\"testObject = $testObject\") // a testObject.say2() // b println(s\"testObject.user = $&#123;testObject.user&#125;\") // c import testObject.innerObj._ println(s\"user1 = $user1\") // d say(\"testObject\") // e println(\"------------------------------\")// val testObject1 = new TestObject(\"abc\", 24)// println(s\"testObject1 = $testObject1\")// testObject1.say2()// println(s\"testObject1.user = $&#123;testObject1.user&#125;\")// import testObject1.innerObj._//// println(s\"testObject1 user1 = $user1\")// say(\"testObject1\") &#125;&#125; 打印结果如下： 123456testObject = com.msb.bigdata.scala.test_object.TestObject@5702b3b1TestObject say2 TestObject.this = com.msb.bigdata.scala.test_object.TestObject@5702b3b1testObject.user = com.msb.bigdata.scala.test_object.User@192b07fduser1 = com.msb.bigdata.scala.test_object.User@192b07fdtestObject innerObj say TestObject.this = com.msb.bigdata.scala.test_object.TestObject@5702b3b1------------------------------ 通过打印结果可以看到： a、b、e三处打印的是同一个对象实例，所以可以断定在class TestObject的内部中如果调用了TestObject.this实际上是指向的正在运行中的testObject对象 TestObject.this只能在class TestObject的内部调用，不能在外部调用 Test.scala中注掉的那一部分，这一句import testObject1.innerObj._会在编译期间报错，如下 1234567891011Error:(29, 36) reference to user1 is ambiguous;it is imported twice in the same scope byimport testObject1.innerObj._and import testObject.innerObj._ println(s&quot;testObject1 user1 Error:(30, 5) reference to say is ambiguous;it is imported twice in the same scope byimport testObject1.innerObj._and import testObject.innerObj._ say(&quot;testObject1&quot;)= $user1&quot;) import 实例对象的行为不能被多次使用，说的是user1变量和say函数被重复import在同一个作用域内，引发了歧义 在spark那里，不允许以下的写法： 1234567891011121314151617181920212223242526272829303132333435object lesson02_sql_api01_1 &#123; def main(args: Array[String]): Unit = &#123; val conf: SparkConf = new SparkConf().setMaster(\"local\").setAppName(\"test\") val session: SparkSession = SparkSession .builder() .config(conf) .getOrCreate() import session.implicits._ println(session) val session1: SparkSession = SparkSession .builder() .config(conf) .getOrCreate() println(session1) import session1.implicits._ val dataDF: DataFrame = List( \"hello world\", \"hello world\", \"hello msb\", \"hello world\", \"hello world\", \"hello spark\", \"hello world\", \"hello spark\" ).toDF(\"line\") dataDF.show() &#125;&#125;// import session.implicits._ 和 import session1.implicits._可以同时出现，但是如果用到了其中的隐式转换，就会报错，一般也没人会这么写 将Test.scala替换成如下的写法： 1234567891011121314151617181920212223242526272829object Test &#123; def main(args: Array[String]): Unit = &#123; val testObject = new TestObject(\"sfa\", 12) println(s\"testObject = $testObject\") // a testObject.say2() // b println(s\"testObject.user = $&#123;testObject.user&#125;\") // c import testObject.innerObj._ println(s\"user1 = $user1\") // d say(\"testObject\") // e println(\"------------------------------\") val testObject1 = new TestObject(\"abc\", 24) println(s\"testObject1 = $testObject1\") testObject1.say2() println(s\"testObject1.user = $&#123;testObject1.user&#125;\")// import testObject1.innerObj._//// println(s\"testObject1 user1 = $user1\")// say(\"testObject1\") println(s\"testObject.innerObj.user1 = $&#123;testObject.innerObj.user1&#125;\") println(s\"testObject.innerObj.say = $&#123;testObject.innerObj.say(\"testObject1\")&#125;\") &#125;&#125; 打印结果如下： 123456789101112testObject = com.msb.bigdata.scala.test_object.TestObject@5702b3b1TestObject say2 TestObject.this = com.msb.bigdata.scala.test_object.TestObject@5702b3b1testObject.user = com.msb.bigdata.scala.test_object.User@192b07fduser1 = com.msb.bigdata.scala.test_object.User@192b07fdtestObject innerObj say TestObject.this = com.msb.bigdata.scala.test_object.TestObject@5702b3b1------------------------------testObject1 = com.msb.bigdata.scala.test_object.TestObject@64bfbc86TestObject say2 TestObject.this = com.msb.bigdata.scala.test_object.TestObject@64bfbc86testObject1.user = com.msb.bigdata.scala.test_object.User@64bf3bbftestObject1.innerObj.user1 = com.msb.bigdata.scala.test_object.User@64bf3bbftestObject1 innerObj say TestObject.this = com.msb.bigdata.scala.test_object.TestObject@64bfbc86testObject1.innerObj.say = () 我们会发现，就没问题了。而且testObject1的InnerObject调用的TestObject.this实际上是指向的testObject1对象。 关键字二：implicitsimport spark.implicits._中的implicits关键字实际上是SparkSession的内部object，它继承了SQLImplicits 123object implicits extends SQLImplicits with Serializable &#123; protected override def _sqlContext: SQLContext = SparkSession.this.sqlContext&#125; org.apache.spark.sql.SQLImplicits：A collection of implicit methods for converting common Scala objects into Datasets. 从以上的案例中可以得到： SparkSession.this是当前运行的SparkSession实例，我们可以通过实例对象将SparkSession的内部object引入进来。这里为什么一定要使用SparkSession实例的这种方式引入隐式转换呢？根据源码可以知道，在进行隐式转换的时候需要用到sqlContext，它是sparkSession的一个属性，只有在SparkSession创建完成之后，它才有值。 可以看到在org.apache.spark.sql.SQLImplicits中这里用到了_sqlContext对象 12345678910111213141516/** * Creates a [[Dataset]] from an RDD. * * @since 1.6.0 */implicit def rddToDatasetHolder[T : Encoder](rdd: RDD[T]): DatasetHolder[T] = &#123; DatasetHolder(_sqlContext.createDataset(rdd))&#125;/** * Creates a [[Dataset]] from a local Seq. * @since 1.6.0 */implicit def localSeqToDatasetHolder[T : Encoder](s: Seq[T]): DatasetHolder[T] = &#123; DatasetHolder(_sqlContext.createDataset(s))&#125; 从org.apache.spark.sql.SQLImplicits中 我们可以看到几个非常漂亮的隐式转换，如下： 12345678910111213141516/** * Converts $\"col name\" into a [[Column]]. * * @since 2.0.0 */implicit class StringToColumn(val sc: StringContext) &#123; def $(args: Any*): ColumnName = &#123; new ColumnName(sc.s(args: _*)) &#125;&#125;/** * An implicit conversion that turns a Scala `Symbol` into a [[Column]]. * @since 1.3.0 */implicit def symbolToColumn(s: Symbol): ColumnName = new ColumnName(s.name) 这就是我们平时可以直接下如下的脚本的关键所在： 1df.select(\"a\", \"b\", $\"c\", $\"d\"+1, 'e, 'f)","categories":[{"name":"Spark应用","slug":"Spark应用","permalink":"https://shang.at/categories/Spark应用/"}],"tags":[{"name":"Spark中的隐式转换","slug":"Spark中的隐式转换","permalink":"https://shang.at/tags/Spark中的隐式转换/"}]},{"title":"hadoop源码学习","slug":"hadoop源码学习一","date":"2019-07-10T03:03:43.000Z","updated":"2020-12-18T10:02:12.432Z","comments":true,"path":"post/hadoop源码学习一/","link":"","permalink":"https://shang.at/post/hadoop源码学习一/","excerpt":"","text":"先导知识 JAVA-动态代理 IPC/RPC hadoop一共包含以下几个组成部分 hadoop-common fs io ipc：是hadoop的灵魂，连接了集群的每一个节点 hdfs fs NN：NameNode，NN server+filesystem manager DN：DataNode，管理数据block的存储 定时向NN汇报 yarn RM NM tools Hadoop 中的 IPC框架Server 流程图 线程模型 我们使用以下的代码，来创建一个IPC Server 123456 // Get RPC server for server side implementation server = new RPC.Builder(conf).setProtocol(TestRpcService.class) .setNumHandlers(4) // .setnumReaders(4) .setInstance(service).setBindAddress(ADDRESS).setPort(PORT).build();server.start() 启动Server，则会得到如下的线程模型 Client 流程图 线程模型 启动一个hadoop jar job：org.apache.hadoop.util.RunJar","categories":[{"name":"Hadoop学习","slug":"Hadoop学习","permalink":"https://shang.at/categories/Hadoop学习/"}],"tags":[{"name":"Hadoop-IPC","slug":"Hadoop-IPC","permalink":"https://shang.at/tags/Hadoop-IPC/"}]},{"title":"Pandas-学习","slug":"Python数据分析-Pandas学习","date":"2019-06-11T01:53:35.000Z","updated":"2021-05-19T02:24:17.311Z","comments":true,"path":"post/Python数据分析-Pandas学习/","link":"","permalink":"https://shang.at/post/Python数据分析-Pandas学习/","excerpt":"","text":"pandas的DataFrame怎么把几列数据合并成为新的一列: https://blog.csdn.net/gangyin5071/article/details/79601386 https://blog.csdn.net/python_Allen/article/details/106646415 讲的很好：https://blog.csdn.net/W_weiying/article/details/80393524 https://zhuanlan.zhihu.com/p/31974169 pandas dataframe to json: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_json.html python：https://www.cnblogs.com/bigberg/p/6430095.html python中分组排序—groupby(),rank()： https://blog.csdn.net/baidu_38409988/article/details/102668006 【Python】groupby 和 agg 实现string聚合（collect_set）： https://blog.csdn.net/qq_34105362/article/details/84345044 to_dict:https://blog.csdn.net/m0_37804518/article/details/78444110","categories":[{"name":"Python数据分析","slug":"Python数据分析","permalink":"https://shang.at/categories/Python数据分析/"}],"tags":[{"name":"Pandas","slug":"Pandas","permalink":"https://shang.at/tags/Pandas/"}]},{"title":"Python学习-时间处理","slug":"Python学习-时间处理","date":"2019-06-06T08:37:46.000Z","updated":"2019-08-03T02:17:49.584Z","comments":true,"path":"post/Python学习-时间处理/","link":"","permalink":"https://shang.at/post/Python学习-时间处理/","excerpt":"","text":"关于时间戳的几个概念时间戳，根据1970年1月1日00:00:00开始按秒计算的偏移量。时间元组（struct_time），包含9个元素。 1time.struct_time(tm_year=2017, tm_mon=10, tm_mday=1, tm_hour=14, tm_min=21, tm_sec=57, tm_wday=6, tm_yday=274, tm_isdst=0) 时间格式字符串，字符串形式的时间。time模块与时间戳和时间相关的重要函数 12345time.time() # 生成当前的时间戳，格式为10位整数的浮点数。time.strftime() # 根据时间元组生成时间格式化字符串。time.strptime() # 根据时间格式化字符串生成时间元组。time.strptime()与time.strftime()为互操作。time.localtime() # 根据时间戳生成当前时区的时间元组。time.mktime() # 根据时间元组生成时间戳。 示例 1234567891011121314151617181920212223242526272829303132333435363738394041424344import time##生成当前时间的时间戳，只有一个参数即时间戳的位数，默认为10位，输入位数即生成相应位数的时间戳，比如可以生成常用的13位时间戳def now_to_timestamp(digits = 10): time_stamp = time.time() digits = 10 ** (digits -10) time_stamp = int(round(time_stamp*digits)) return time_stamp##将时间戳规范为10位时间戳def timestamp_to_timestamp10(time_stamp): time_stamp = int (time_stamp* (10 ** (10-len(str(time_stamp))))) return time_stamp##将当前时间转换为时间字符串，默认为2017-10-01 13:37:04格式def now_to_date(format_string=\"%Y-%m-%d %H:%M:%S\"): time_stamp = int(time.time()) time_array = time.localtime(time_stamp) str_date = time.strftime(format_string, time_array) return str_date##将10位时间戳转换为时间字符串，默认为2017-10-01 13:37:04格式def timestamp_to_date(time_stamp, format_string=\"%Y-%m-%d %H:%M:%S\"): time_array = time.localtime(time_stamp) str_date = time.strftime(format_string, time_array) return str_date##将时间字符串转换为10位时间戳，时间字符串默认为2017-10-01 13:37:04格式def date_to_timestamp(date, format_string=\"%Y-%m-%d %H:%M:%S\"): time_array = time.strptime(date, format_string) time_stamp = int(time.mktime(time_array)) return time_stamp##不同时间格式字符串的转换def date_style_transfomation(date, format_string1=\"%Y-%m-%d %H:%M:%S\",format_string2=\"%Y-%m-%d %H-%M-%S\"): time_array = time.strptime(date, format_string1) str_date = time.strftime(format_string2, time_array) return str_dateprint(now_to_date())print(timestamp_to_date(1506816572))print(date_to_timestamp('2017-10-01 08:09:32'))print(timestamp_to_timestamp10(1506816572546))print(date_style_transfomation('2017-10-01 08:09:32')) 结果为 1234515068362240002017-10-01 13:37:042017-10-01 08:09:3215068165721506816572","categories":[{"name":"Python","slug":"Python","permalink":"https://shang.at/categories/Python/"}],"tags":[{"name":"python中的时间处理","slug":"python中的时间处理","permalink":"https://shang.at/tags/python中的时间处理/"}]},{"title":"Spark学习笔记-Configuration","slug":"Spark学习笔记-Configuration","date":"2019-06-03T09:32:33.000Z","updated":"2020-07-01T09:10:09.964Z","comments":true,"path":"post/Spark学习笔记-Configuration/","link":"","permalink":"https://shang.at/post/Spark学习笔记-Configuration/","excerpt":"","text":"submit 参数 运行时可配置参数：在代码中使用spark.conf.set(‘’， ‘’)的方式设置。运行时设置的参数不会在WebUI中显示","categories":[{"name":"Spark学习","slug":"Spark学习","permalink":"https://shang.at/categories/Spark学习/"}],"tags":[{"name":"Configuration","slug":"Configuration","permalink":"https://shang.at/tags/Configuration/"}]},{"title":"Python学习-队列","slug":"Python学习-队列","date":"2019-06-03T02:10:48.000Z","updated":"2020-07-01T09:21:20.616Z","comments":true,"path":"post/Python学习-队列/","link":"","permalink":"https://shang.at/post/Python学习-队列/","excerpt":"","text":"队列123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051from queue import Queue #LILO队列q = Queue() #创建队列对象q.put(0) #在队列尾部插入元素q.put(1)q.put(2)print('LILO队列',q.queue) #查看队列中的所有元素print(q.get()) #返回并删除队列头部元素print(q.queue)from queue import LifoQueue #LIFO队列lifoQueue = LifoQueue()lifoQueue.put(1)lifoQueue.put(2)lifoQueue.put(3)print('LIFO队列',lifoQueue.queue)lifoQueue.get() #返回并删除队列尾部元素lifoQueue.get()print(lifoQueue.queue)from queue import PriorityQueue #优先队列priorityQueue = PriorityQueue() #创建优先队列对象priorityQueue.put(3) #插入元素priorityQueue.put(78) #插入元素priorityQueue.put(100) #插入元素print(priorityQueue.queue) #查看优先级队列中的所有元素priorityQueue.put(1) #插入元素priorityQueue.put(2) #插入元素print('优先级队列:',priorityQueue.queue) #查看优先级队列中的所有元素priorityQueue.get() #返回并删除优先级最低的元素print('删除后剩余元素',priorityQueue.queue)priorityQueue.get() #返回并删除优先级最低的元素print('删除后剩余元素',priorityQueue.queue) #删除后剩余元素priorityQueue.get() #返回并删除优先级最低的元素print('删除后剩余元素',priorityQueue.queue) #删除后剩余元素priorityQueue.get() #返回并删除优先级最低的元素print('删除后剩余元素',priorityQueue.queue) #删除后剩余元素priorityQueue.get() #返回并删除优先级最低的元素print('全部被删除后:',priorityQueue.queue) #查看优先级队列中的所有元素from collections import deque #双端队列dequeQueue = deque(['Eric','John','Smith'])print(dequeQueue)dequeQueue.append('Tom') #在右侧插入新元素dequeQueue.appendleft('Terry') #在左侧插入新元素print(dequeQueue)dequeQueue.rotate(2) #循环右移2次print('循环右移2次后的队列',dequeQueue)dequeQueue.popleft() #返回并删除队列最左端元素print('删除最左端元素后的队列：',dequeQueue)dequeQueue.pop() #返回并删除队列最右端元素print('删除最右端元素后的队列：',dequeQueue)","categories":[{"name":"Python","slug":"Python","permalink":"https://shang.at/categories/Python/"}],"tags":[{"name":"队列","slug":"队列","permalink":"https://shang.at/tags/队列/"}]},{"title":"Spark学习笔记-广播变量","slug":"Spark学习笔记-广播变量","date":"2019-05-28T08:19:03.000Z","updated":"2020-07-01T09:09:58.493Z","comments":true,"path":"post/Spark学习笔记-广播变量/","link":"","permalink":"https://shang.at/post/Spark学习笔记-广播变量/","excerpt":"","text":"Shared Variables通常，当在远程集群节点上执行传递给Spark操作（例如mapor reduce）的函数时，它将在函数中使用的所有变量的单独副本上工作。这些变量将复制到每台计算机，并且远程计算机上的变量的更新不会传播回驱动程序。支持跨任务的通用，读写共享变量效率低下。但是，Spark确实为两种常见的使用模式提供了两种有限类型的共享变量：广播变量和累加器。 Broadcast广播变量允许程序员在每台机器上保留一个只读变量，而不是随副本一起发送它的副本。例如，它们可用于以有效的方式为每个节点提供大输入数据集的副本。Spark还尝试使用有效的广播算法来分发广播变量，以降低通信成本。 Spark动作通过一组阶段执行，由分布式“shuffle”操作分隔。Spark自动广播每个阶段中任务所需的公共数据。以这种方式广播的数据以序列化形式缓存并在运行每个任务之前反序列化。这意味着显式创建广播变量仅在跨多个阶段的任务需要相同数据或以反序列化形式缓存数据很重要时才有用。 广播变量是v通过调用从变量创建的SparkContext.broadcast(v)。广播变量是一个包装器v，可以通过调用该value 方法来访问它的值。下面的代码显示了这个： 12345&gt;&gt;&gt; broadcastVar = sc.broadcast([1, 2, 3])&lt;pyspark.broadcast.Broadcast object at 0x102789f10&gt;&gt;&gt;&gt; broadcastVar.value[1, 2, 3] 创建广播变量后，应该使用它来代替v群集上运行的任何函数中的值，这样v就不会多次传送到节点。此外，在v广播之后不应修改对象 ，以确保所有节点获得相同的广播变量值（例如，如果稍后将变量发送到新节点）。 Performance Tuning 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108== Physical Plan ==InMemoryTableScan [bill_create_date#4955, week_last_day#5015, month_last_day#5075, total_outstanding_amount_ex_dp90#6075] +- InMemoryRelation [bill_create_date#4955, week_last_day#5015, month_last_day#5075, total_outstanding_amount_ex_dp90#6075], true, 10000, StorageLevel(disk, 1 replicas) +- *(34) Project [bill_create_date#4955, week_last_day#5015, month_last_day#5075, ((coalesce(nanvl(total_disburse_amount#5203, null), 0.0) - cast(coalesce(total_repay_principal_amount#5974, 0) as double)) - coalesce(nanvl(total_write_off_principal#5986, null), 0.0)) AS total_outstanding_amount_ex_dp90#6075] +- SortMergeJoin [bill_create_date#4955], [write_off_date#4776], LeftOuter :- *(23) Project [bill_create_date#4955, week_last_day#5015, month_last_day#5075, total_disburse_amount#5203, total_repay_principal_amount#5974] : +- SortMergeJoin [bill_create_date#4955], [repay_date#5789], LeftOuter : :- *(6) Sort [bill_create_date#4955 ASC NULLS FIRST], false, 0 : : +- Exchange hashpartitioning(bill_create_date#4955, 200) : : +- *(5) Project [bill_create_date#4955, week_last_day#5015, month_last_day#5075, total_disburse_amount#5203] : : +- Window [sum(disburse_amount#5197) windowspecdefinition(1, bill_create_date#4955 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS total_disburse_amount#5203], [1], [bill_create_date#4955 ASC NULLS FIRST] : : +- *(4) Sort [1 ASC NULLS FIRST, bill_create_date#4955 ASC NULLS FIRST], false, 0 : : +- Exchange hashpartitioning(1, 200) : : +- *(3) HashAggregate(keys=[bill_create_date#4955, week_last_day#5015, month_last_day#5075], functions=[sum(cast(principal#615 as double))]) : : +- Exchange hashpartitioning(bill_create_date#4955, week_last_day#5015, month_last_day#5075, 200) : : +- *(2) HashAggregate(keys=[bill_create_date#4955, week_last_day#5015, month_last_day#5075], functions=[partial_sum(cast(principal#615 as double))]) : : +- *(2) Project [principal#615, cast(from_utc_timestamp(cast(from_unixtime(cast((cast(create_time#632L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp), Asia/Ho_Chi_Minh) as date) AS bill_create_date#4955, next_day(cast(from_utc_timestamp(cast(from_unixtime(cast((cast(create_time#632L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp), Asia/Ho_Chi_Minh) as date), Sun) AS week_last_day#5015, last_day(cast(from_utc_timestamp(cast(from_unixtime(cast((cast(create_time#632L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp), Asia/Ho_Chi_Minh) as date)) AS month_last_day#5075] : : +- *(2) BroadcastHashJoin [id#392], [loan_id#609], Inner, BuildRight : : :- *(2) Project [id#392] : : : +- *(2) Filter (status#397 IN (COMPLETED,CURRENT,LATE) &amp;&amp; isnotnull(id#392)) : : : +- *(2) FileScan parquet [id#392,status#397] Batched: true, Format: Parquet, Location: InMemoryFileIndex[...], PartitionFilters: [], PushedFilters: [In(status, [COMPLETED,CURRENT,LATE]), IsNotNull(id)], ReadSchema: struct&lt;id:string,status:string&gt; : : +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true])) : : +- *(1) Project [loan_id#609, principal#615, create_time#632L] : : +- *(1) Filter ((cast(from_utc_timestamp(cast(from_unixtime(cast((cast(create_time#632L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp), Asia/Ho_Chi_Minh) as date) &lt;= 18043) &amp;&amp; isnotnull(loan_id#609)) : : +- *(1) FileScan parquet [loan_id#609,principal#615,create_time#632L] Batched: true, Format: Parquet, Location: InMemoryFileIndex[...], PartitionFilters: [], PushedFilters: [IsNotNull(loan_id)], ReadSchema: struct&lt;loan_id:string,principal:string,create_time:bigint&gt; : +- *(22) Sort [repay_date#5789 ASC NULLS FIRST], false, 0 : +- Exchange hashpartitioning(repay_date#5789, 200) : +- *(21) Project [repay_date#5789, total_repay_principal_amount#5974] : +- Window [sum(repay_principal_amount#5970) windowspecdefinition(1, repay_date#5789 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS total_repay_principal_amount#5974], [1], [repay_date#5789 ASC NULLS FIRST] : +- *(20) Sort [1 ASC NULLS FIRST, repay_date#5789 ASC NULLS FIRST], false, 0 : +- Exchange hashpartitioning(1, 200) : +- *(19) HashAggregate(keys=[repay_date#5789], functions=[sum(CASE WHEN (isnull(write_off_date#4776) || (write_off_date#4776 &gt; repay_date#5789)) THEN repaid_principal#684 END)]) : +- Exchange hashpartitioning(repay_date#5789, 200) : +- *(18) HashAggregate(keys=[repay_date#5789], functions=[partial_sum(CASE WHEN (isnull(write_off_date#4776) || (write_off_date#4776 &gt; repay_date#5789)) THEN repaid_principal#684 END)]) : +- *(18) Project [repaid_principal#684, write_off_date#4776, cast(from_utc_timestamp(repay_time#692, Asia/Ho_Chi_Minh) as date) AS repay_date#5789] : +- SortMergeJoin [loan_id#609], [loan_id#5672], LeftOuter : :- *(12) Sort [loan_id#609 ASC NULLS FIRST], false, 0 : : +- Exchange hashpartitioning(loan_id#609, 200) : : +- *(11) Project [loan_id#609, repaid_principal#684, repay_time#692] : : +- *(11) BroadcastHashJoin [id#608], [bill_id#670], Inner, BuildRight : : :- *(11) Project [id#608, loan_id#609] : : : +- *(11) BroadcastHashJoin [id#392], [loan_id#609], Inner, BuildRight : : : :- *(11) Project [id#392] : : : : +- *(11) Filter (status#397 IN (COMPLETED,CURRENT,LATE) &amp;&amp; isnotnull(id#392)) : : : : +- *(11) FileScan parquet [id#392,status#397] Batched: true, Format: Parquet, Location: InMemoryFileIndex[...], PartitionFilters: [], PushedFilters: [In(status, [COMPLETED,CURRENT,LATE]), IsNotNull(id)], ReadSchema: struct&lt;id:string,status:string&gt; : : : +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true])) : : : +- *(7) Project [id#608, loan_id#609] : : : +- *(7) Filter (isnotnull(loan_id#609) &amp;&amp; isnotnull(id#608)) : : : +- *(7) FileScan parquet [id#608,loan_id#609] Batched: true, Format: Parquet, Location: InMemoryFileIndex[...], PartitionFilters: [], PushedFilters: [IsNotNull(loan_id), IsNotNull(id)], ReadSchema: struct&lt;id:string,loan_id:string&gt; : : +- BroadcastExchange HashedRelationBroadcastMode(ArrayBuffer(input[0, string, true])) : : +- *(10) Project [bill_id#670, repaid_principal#684, repay_time#692] : : +- *(10) Filter ((isnotnull(rn#5382) &amp;&amp; (rn#5382 = 1)) &amp;&amp; (cast(from_utc_timestamp(repay_time#692, Asia/Ho_Chi_Minh) as date) &lt;= 18043)) : : +- Window [row_number() windowspecdefinition(bill_id#670, repay_time#692 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rn#5382], [bill_id#670], [repay_time#692 DESC NULLS LAST] : : +- *(9) Sort [bill_id#670 ASC NULLS FIRST, repay_time#692 DESC NULLS LAST], false, 0 : : +- Exchange hashpartitioning(bill_id#670, 200) : : +- *(8) Project [bill_id#670, repaid_principal#684, repay_time#692] : : +- *(8) Filter isnotnull(bill_id#670) : : +- *(8) FileScan parquet [bill_id#670,repaid_principal#684,repay_time#692] Batched: true, Format: Parquet, Location: InMemoryFileIndex[...], PartitionFilters: [], PushedFilters: [IsNotNull(bill_id)], ReadSchema: struct&lt;bill_id:string,repaid_principal:decimal(20,0),repay_time:timestamp&gt; : +- *(17) Sort [loan_id#5672 ASC NULLS FIRST], false, 0 : +- *(17) HashAggregate(keys=[loan_id#5672], functions=[first(write_off_date#4705, true)]) : +- *(17) HashAggregate(keys=[loan_id#5672], functions=[partial_first(write_off_date#4705, true)]) : +- *(17) Project [loan_id#5672, write_off_date#4705] : +- *(17) BroadcastHashJoin [bill_id#4714], [bill_id#670], LeftOuter, BuildRight : :- *(17) Project [loan_id#5672, write_off_date#4705, bill_id#4714] : : +- *(17) BroadcastHashJoin [loan_id#5672], [loan_id#4718], LeftOuter, BuildRight : : :- *(17) Project [loan_id#5672, write_off_date#4705] : : : +- *(17) BroadcastHashJoin [loan_id#5672], [loan_id#4708], LeftOuter, BuildRight : : : :- *(17) HashAggregate(keys=[loan_id#5672], functions=[min(CASE WHEN is_write_off_bill#4594 THEN write_off_date#4630 END)]) : : : : +- Exchange hashpartitioning(loan_id#5672, 200) : : : : +- *(13) HashAggregate(keys=[loan_id#5672], functions=[partial_min(CASE WHEN is_write_off_bill#4594 THEN write_off_date#4630 END)]) : : : : +- *(13) Project [loan_id#5672, (CASE WHEN isnotnull(CASE WHEN (status#5676 = OVERDUE) THEN datediff(18044, cast(due_date#5686 as date)) END) THEN CASE WHEN (status#5676 = OVERDUE) THEN datediff(18044, cast(due_date#5686 as date)) END WHEN isnotnull(CASE WHEN (status#5676 IN (REBALANCED,REPAID) &amp;&amp; (cast(cast(from_utc_timestamp(cast(from_unixtime(cast((cast(repay_time#5694L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp), Asia/Ho_Chi_Minh) as date) as string) &gt; due_date#5686)) THEN datediff(cast(from_utc_timestamp(cast(from_unixtime(cast((cast(repay_time#5694L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp), Asia/Ho_Chi_Minh) as date), cast(due_date#5686 as date)) END) THEN CASE WHEN (status#5676 IN (REBALANCED,REPAID) &amp;&amp; (cast(cast(from_utc_timestamp(cast(from_unixtime(cast((cast(repay_time#5694L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp), Asia/Ho_Chi_Minh) as date) as string) &gt; due_date#5686)) THEN datediff(cast(from_utc_timestamp(cast(from_unixtime(cast((cast(repay_time#5694L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp), Asia/Ho_Chi_Minh) as date), cast(due_date#5686 as date)) END ELSE 0 END &gt;= 91) AS is_write_off_bill#4594, date_add(cast(due_date#5686 as date), 91) AS write_off_date#4630] : : : : +- *(13) Filter (CASE WHEN isnotnull(CASE WHEN (status#5676 = OVERDUE) THEN datediff(18044, cast(due_date#5686 as date)) END) THEN CASE WHEN (status#5676 = OVERDUE) THEN datediff(18044, cast(due_date#5686 as date)) END WHEN isnotnull(CASE WHEN (status#5676 IN (REBALANCED,REPAID) &amp;&amp; (cast(cast(from_utc_timestamp(cast(from_unixtime(cast((cast(repay_time#5694L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp), Asia/Ho_Chi_Minh) as date) as string) &gt; due_date#5686)) THEN datediff(cast(from_utc_timestamp(cast(from_unixtime(cast((cast(repay_time#5694L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp), Asia/Ho_Chi_Minh) as date), cast(due_date#5686 as date)) END) THEN CASE WHEN (status#5676 IN (REBALANCED,REPAID) &amp;&amp; (cast(cast(from_utc_timestamp(cast(from_unixtime(cast((cast(repay_time#5694L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp), Asia/Ho_Chi_Minh) as date) as string) &gt; due_date#5686)) THEN datediff(cast(from_utc_timestamp(cast(from_unixtime(cast((cast(repay_time#5694L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp), Asia/Ho_Chi_Minh) as date), cast(due_date#5686 as date)) END ELSE 0 END &gt;= 91) : : : : +- *(13) FileScan parquet [loan_id#5672,status#5676,due_date#5686,repay_time#5694L] Batched: true, Format: Parquet, Location: InMemoryFileIndex[...], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;loan_id:string,status:string,due_date:string,repay_time:bigint&gt; : : : +- BroadcastExchange HashedRelationBroadcastMode(ArrayBuffer(input[0, string, true])) : : : +- *(14) Project [id#392 AS loan_id#4708] : : : +- *(14) FileScan parquet [id#392] Batched: true, Format: Parquet, Location: InMemoryFileIndex[...], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;id:string&gt; : : +- BroadcastExchange HashedRelationBroadcastMode(ArrayBuffer(input[1, string, true])) : : +- *(15) Project [id#4717 AS bill_id#4714, loan_id#4718] : : +- *(15) FileScan parquet [id#4717,loan_id#4718] Batched: true, Format: Parquet, Location: InMemoryFileIndex[...], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;id:string,loan_id:string&gt; : +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true])) : +- *(16) FileScan parquet [bill_id#670] Batched: true, Format: Parquet, Location: InMemoryFileIndex[...], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;bill_id:string&gt; +- *(33) Sort [write_off_date#4776 ASC NULLS FIRST], false, 0 +- Exchange hashpartitioning(write_off_date#4776, 200) +- *(32) Project [write_off_date#4776, total_write_off_principal#5986] +- Window [sum(write_off_principal#5982) windowspecdefinition(1, write_off_date#4776 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS total_write_off_principal#5986], [1], [write_off_date#4776 ASC NULLS FIRST] +- *(31) Sort [1 ASC NULLS FIRST, write_off_date#4776 ASC NULLS FIRST], false, 0 +- Exchange hashpartitioning(1, 200) +- *(30) HashAggregate(keys=[write_off_date#4776], functions=[sum(write_off_principal#4779)]) +- Exchange hashpartitioning(write_off_date#4776, 200) +- *(29) HashAggregate(keys=[write_off_date#4776], functions=[partial_sum(write_off_principal#4779)]) +- SortAggregate(key=[loan_id#609], functions=[first(write_off_date#4705, true), first(amount#398, true), sum(CASE WHEN (isnotnull(repayment_date#4760) &amp;&amp; (repayment_date#4760 &lt; write_off_date#4705)) THEN repaid_principal#684 ELSE 0 END)]) +- SortAggregate(key=[loan_id#609], functions=[partial_first(write_off_date#4705, true), partial_first(amount#398, true), partial_sum(CASE WHEN (isnotnull(repayment_date#4760) &amp;&amp; (repayment_date#4760 &lt; write_off_date#4705)) THEN repaid_principal#684 ELSE 0 END)]) +- *(28) Sort [loan_id#609 ASC NULLS FIRST], false, 0 +- *(28) Project [loan_id#609, write_off_date#4705, amount#398, repaid_principal#684, cast(from_utc_timestamp(repay_time#692, Asia/Ho_Chi_Minh) as date) AS repayment_date#4760] +- *(28) BroadcastHashJoin [bill_id#4714], [bill_id#670], LeftOuter, BuildRight :- *(28) Project [loan_id#609, write_off_date#4705, amount#398, bill_id#4714] : +- *(28) BroadcastHashJoin [loan_id#609], [loan_id#4718], LeftOuter, BuildRight : :- *(28) Project [loan_id#609, write_off_date#4705, amount#398] : : +- *(28) BroadcastHashJoin [loan_id#609], [loan_id#4708], LeftOuter, BuildRight : : :- *(28) HashAggregate(keys=[loan_id#609], functions=[min(CASE WHEN is_write_off_bill#4594 THEN write_off_date#4630 END)]) : : : +- ReusedExchange [loan_id#609, min#6101], Exchange hashpartitioning(loan_id#5672, 200) : : +- BroadcastExchange HashedRelationBroadcastMode(ArrayBuffer(input[0, string, true])) : : +- *(25) Project [id#392 AS loan_id#4708, amount#398] : : +- *(25) FileScan parquet [id#392,amount#398] Batched: true, Format: Parquet, Location: InMemoryFileIndex[...], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;id:string,amount:string&gt; : +- ReusedExchange [bill_id#4714, loan_id#4718], BroadcastExchange HashedRelationBroadcastMode(ArrayBuffer(input[1, string, true])) +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true])) +- *(27) Project [bill_id#670, repay_time#692, repaid_principal#684] +- *(27) FileScan parquet [bill_id#670,repaid_principal#684,repay_time#692] Batched: true, Format: Parquet, Location: InMemoryFileIndex[...], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;bill_id:string,repaid_principal:decimal(20,0),repay_time:timestamp&gt;","categories":[{"name":"Spark学习","slug":"Spark学习","permalink":"https://shang.at/categories/Spark学习/"}],"tags":[{"name":"广播变量","slug":"广播变量","permalink":"https://shang.at/tags/广播变量/"}]},{"title":"数据结构学习笔记二-算法","slug":"数据结构学习笔记二-算法","date":"2019-05-16T09:54:29.000Z","updated":"2019-05-16T14:39:58.762Z","comments":true,"path":"post/数据结构学习笔记二-算法/","link":"","permalink":"https://shang.at/post/数据结构学习笔记二-算法/","excerpt":"","text":"递归二分查找哈希算法堆排序深度和广度优先搜索字符串匹配贪心算法分治算法回溯算法动态规划拓扑排序最短路径并行算法","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"}],"tags":[{"name":"算法","slug":"算法","permalink":"https://shang.at/tags/算法/"}]},{"title":"Spark学习笔记-pivot透视图","slug":"Spark学习笔记-pivot透视图","date":"2019-05-09T02:52:07.000Z","updated":"2021-02-25T03:53:22.926Z","comments":true,"path":"post/Spark学习笔记-pivot透视图/","link":"","permalink":"https://shang.at/post/Spark学习笔记-pivot透视图/","excerpt":"","text":"12345678910df = spark.createDataFrame([ ('2018-01','项目1',100, 'xm'), ('2018-01','项目1',100, 'xl'), ('2018-01','项目1',100, 'xp'), ('2018-01','项目2',200, 'ch'), ('2018-01','项目3',300, 'xl'), ('2018-02','项目1',1000, 'xp'), ('2018-02','项目2',2000, 'xl'), ('2018-03','项目x',999, 'xm')], ['date','project','income', 'saler']) 1df.toPandas() date project income saler 0 2018-01 项目1 100 xm 1 2018-01 项目1 100 xl 2 2018-01 项目1 100 xp 3 2018-01 项目2 200 ch 4 2018-01 项目3 300 xl 5 2018-02 项目1 1000 xp 6 2018-02 项目2 2000 xl 7 2018-03 项目x 999 xm pivot1234567df_pivot = df.groupBy('date').pivot( 'project', ['项目1', '项目2', '项目3', '项目x']).agg( sum('income')).na.fill(0)df_pivot.toPandas() date 项目1 项目2 项目3 项目x 0 2018-03 0 0 0 999 1 2018-02 1000 2000 0 0 2 2018-01 300 200 300 0 12345df.groupBy('project').pivot( 'date').agg( sum('income')).na.fill(0).toPandas() project 2018-01 2018-02 2018-03 0 项目2 200 2000 0 1 项目x 0 0 999 2 项目1 300 1000 0 3 项目3 300 0 0 unpivot12345df_pivot.selectExpr(\"date\", \"stack(4, '项目11', `项目1`, '项目22', `项目2`, '项目33', `项目3`, '项目xx', `项目x`) as (`project`,`income`)\")\\ .filter(\"income &gt; 0 \")\\ .orderBy([\"date\", \"project\"])\\ .toPandas() date project income 0 2018-01 项目11 300 1 2018-01 项目22 200 2 2018-01 项目33 300 3 2018-02 项目11 1000 4 2018-02 项目22 2000 5 2018-03 项目xx 999 1stack(n, expr1, ..., exprk) 将k个[expr1, ..., exprk]拆解成n rows pandas实现pivot","categories":[{"name":"Spark学习","slug":"Spark学习","permalink":"https://shang.at/categories/Spark学习/"}],"tags":[{"name":"pivot透视图","slug":"pivot透视图","permalink":"https://shang.at/tags/pivot透视图/"}]},{"title":"Spark学习笔记-tips","slug":"Spark学习笔记-tips","date":"2019-04-15T05:50:00.000Z","updated":"2020-08-31T02:19:01.831Z","comments":true,"path":"post/Spark学习笔记-tips/","link":"","permalink":"https://shang.at/post/Spark学习笔记-tips/","excerpt":"","text":"使用jdbc保存数据的时候，需要注意一个重要的问题：如果是基于原始的数据更改一些内容，然后覆盖原来的表，那么原始的表数据会被 truncate， 所以在同一个spark session中你如果先调用了读取和overwrite操作，那么请注意了，在真正读取数据之前，数据就会被truncate掉，所以原始数据无法被保存。 原因是spark的rdd是不保存数据的，自由真正调用action算子的时候，才会触发计算，但是如果选择了overwritemode，就会把数据truncate掉了，因为这是在Driver端执行的，此时还没有发生计算 如何解决： 必须要用mysql表进行数据存储，那就考虑不要使用spark的jdbc API了 如果必须要用spark的jdbc api，是没办法做到的。因为truncate操作是在action算子之前就已经完成的。 如何屏蔽Spark的日志： 12345678// 获取SparkContext对象val conf: SparkConf = new SparkConf().setMaster(\"local\").setAppName(\"test01\")val sc = new SparkContext(conf)sc.setLogLevel(\"ERROR\")// 或者通过SparkSession也可以获取：val sc = sparkSession.sparkContextsc.setLogLevel(\"ERROR\") 写spark dataframe的时候，最好用哪些字段就取哪些字段，否则spark会默认把所有字段都读进内存，如果进行cache操作，就会无故占用大量内存 尽量不要使用json类型的字段存储数据，因为json字符串会存储大量的无用数据，字段名，最好设计有效的struct结构体来存储 没有被明确select的字段依然可以作为filter的条件 获取周的第一天日期和当前日期位于周的第几天，周的第一天定义不同 周日 周一 Spark Shuffle spill (Memory) and (Disk) on SPARK UI? What do they mean? https://community.hortonworks.com/questions/202809/spark-shuffle-spill-memory.html 窗口函数会引起重分区吗？分区数(200)是固定的吗？ 1234567891011test_df = kreditpintar.spark.range(0, end=100, numPartitions=5).toDF(&apos;input&apos;)test_df.rdd.getNumPartitions() # 5test_1_df = test_df.withColumn(&apos;id&apos;, row_number().over(Window.partitionBy(lit(1)).orderBy(&apos;input&apos;)))test_1_df.rdd.getNumPartitions() # 200test_2_df = test_df.withColumn(&apos;id&apos;, monotonically_increasing_id())test_2_df.rdd.getNumPartitions() # 5 通过withColumn(‘group’, lit(‘aaaabbb’))添加的新列，不能最为后续的join操作的condition expression？ groupBy 和 窗口函数的实现原理 哪一个效率更高 groupby 、窗口函数、distinct三种方式去重 哪个效率高 1distinct&gt;groupby&gt;窗口函数 循环的去跑脚本，然后union每次循环的结果。 这样的使用 task可能会失败，需要优化 转化long列类型到时间戳，保留毫秒信息 12345a_df = spark.createDataFrame([[1556613225852]], ['a'])a_df.select((col('a')/1000.0).cast('timestamp')).toPandas()#CAST((a / 1000.0) AS TIMESTAMP)#0 2019-04-30 08:33:45.852 spark进行计算的过程中间检查数据没有问题，但是执行collect后出现数据不一致的情况(丢失数据和union后的数据重复)","categories":[{"name":"Spark学习","slug":"Spark学习","permalink":"https://shang.at/categories/Spark学习/"}],"tags":[{"name":"Spark-tips","slug":"Spark-tips","permalink":"https://shang.at/tags/Spark-tips/"}]},{"title":"BI工具使用之Tableau一","slug":"BI工具使用之Tableau一","date":"2019-04-11T07:08:39.000Z","updated":"2019-05-12T00:22:00.389Z","comments":true,"path":"post/BI工具使用之Tableau一/","link":"","permalink":"https://shang.at/post/BI工具使用之Tableau一/","excerpt":"","text":"","categories":[{"name":"BI","slug":"BI","permalink":"https://shang.at/categories/BI/"}],"tags":[{"name":"Tableau","slug":"Tableau","permalink":"https://shang.at/tags/Tableau/"}]},{"title":"Spark学习笔记-DSL语法","slug":"Spark学习笔记-DSL语法","date":"2019-03-31T02:20:06.000Z","updated":"2020-07-01T09:10:14.978Z","comments":true,"path":"post/Spark学习笔记-DSL语法/","link":"","permalink":"https://shang.at/post/Spark学习笔记-DSL语法/","excerpt":"","text":"","categories":[{"name":"Spark学习","slug":"Spark学习","permalink":"https://shang.at/categories/Spark学习/"}],"tags":[{"name":"sparkSql-DSL语法","slug":"sparkSql-DSL语法","permalink":"https://shang.at/tags/sparkSql-DSL语法/"}]},{"title":"数据结构与算法学习笔记-排序算法","slug":"数据结构与算法学习笔记-排序算法","date":"2019-03-29T00:49:58.000Z","updated":"2020-06-29T17:51:40.125Z","comments":true,"path":"post/数据结构与算法学习笔记-排序算法/","link":"","permalink":"https://shang.at/post/数据结构与算法学习笔记-排序算法/","excerpt":"","text":"算法分类十种常见排序算法可以分为两大类： 比较类排序：通过比较来决定元素间的相对次序，由于其时间复杂度不能突破O(nlogn)，因此也称为非线性时间比较类排序。 非比较类排序：不通过比较来决定元素间的相对次序，它可以突破基于比较排序的时间下界，以线性时间运行，因此也称为线性时间非比较类排序。 算法复杂度 相关概念 稳定：如果a原本在b前面，而a=b，排序之后a仍然在b的前面。 不稳定：如果a原本在b的前面，而a=b，排序之后 a 可能会出现在 b 的后面。 时间复杂度：对排序数据的总的操作次数。反映当n变化时，操作次数呈现什么规律。 空间复杂度：是指算法在计算机内执行时所需存储空间的度量，它也是数据规模n的函数 参考 O($n^2$)冒泡排序(Bubble Sort) 算法描述 冒泡排序只会操作相邻的两个数据。每次冒泡操作都会对相邻的两个元素进行比较，看是否符合大小关系要求。如果不满足就互换位置。一次冒泡至少会让一个元素移动到它应该在的位置，重复n次，就完成了n个元素的排序工作。 算法实现 12345678910111213141516171819202122232425262728def bubble_sort(nums): \"\"\" 冒泡排序：从小到大 :param nums: :return: \"\"\" if len(nums) &lt;= 1: return nums for i in range(len(nums) - 1): for j in range(i + 1, len(nums)): if nums[i] &gt; nums[j]: nums[i], nums[j] = nums[j], nums[i] return nums def bubble_sort1(nums): if len(nums) &lt;= 1: return nums for i in range(len(nums) - 1): for j in range(len(nums) - 1 - i): if nums[j] &gt; nums[j + i]: nums[j], nums[j + 1] = nums[j + 1], nums[j] return numsif __name__ == '__main__': nums = [-23, 0, 6, -4, 34] print(bubble_sort(nums)) 插入排序 算法描述 将一个元素插入一个已经有序的序列，使其依然有序。首先，将原始的序列分为两个子序列，有序的和无序的，然后，从无序的序列中依次拿出一个元素，插入到有序的序列的合适位置，并保持有序的序列依然有序，直到无序的序列中没有元素了。 算法实现 1234567891011121314151617def insert_sort(nums): if len(nums) &lt;= 1: return nums for i in range(1, len(nums)): # 遍历无序数组的每一个元素 tmp = nums[i] # 待插入元素 j = i - 1 # 待插入子数组 for j in range(i - 1, -1, -1): # 从后往前遍历待插入子数组 if tmp &gt;= nums[j]: break # tmp大于等于当前元素，停止遍历 # 相等元素不会改变其相对位置，故是稳定的 nums[j + 1] = nums[j] # 将nums[j]后移1个位置 nums[j + 1] = tmp # 插入待插入元素 tmp return nums if __name__ == '__main__': nums = [-23, 0, 6, -4, 34, 2] print(insert_sort(nums)) 选择排序 算法描述 选择排序是选择无序序列中的最小的元素放到有序序列的末尾，直到无序序列没有元素。 算法实现 123456789101112131415161718def selection_sort(nums): if len(nums) &lt;= 1: return nums for i in range(len(nums) - 1): # 遍历无序数组的每一个元素 # i和nums[i] min_val = nums[i] min_j = i for j in range(i + 1, len(nums)): # 寻找剩余待排数组的最小元素 if min_val &gt; nums[j]: min_val = nums[j] min_j = j nums[i], nums[min_j] = nums[min_j], nums[i] # 交换最小元素和 return numsif __name__ == '__main__': nums = [-23, 0, 6, -4, 34, 2] print(selection_sort(nums)) 希尔排序 算法描述 123456希尔排序是对插入排序的优化。希尔排序，通过将原始序列按照一定的步长划分为多个子序列 将原始的一维数组映射成二维数组， 然后按列进行插入排序，这样的话，可以让一个元素在一次比较中跨越较大的区间，随后算法在使用较小的步长，一直到步长为1(已知当对有序度较高数组进行排序时，插入排序的时间复杂度接近O(N)，因此可以大幅度提高插入排序的效率)。 维基百科：https://zh.wikipedia.org/wiki/%E5%B8%8C%E5%B0%94%E6%8E%92%E5%BA%8F 算法实现 12345678910111213141516171819202122232425262728293031323334353637383940414243def shell_sort(list): n = len(list) # 初始步长 gap = n // 2 while gap &gt; 0: print(gap) for i in range(gap, n): # 每个步长进行插入排序 temp = list[i] j = i # 插入排序 while j &gt;= gap and list[j - gap] &gt; temp: list[j] = list[j - gap] j -= gap print('inner=', list) list[j] = temp print(list) # 得到新的步长 gap = gap // 2 return listdef shell_sort1(collection): # Marcin Ciura's gap sequence gaps = [701, 301, 132, 57, 23, 10, 4, 1] for gap in gaps: i = gap while i &lt; len(collection): temp = collection[i] j = i while j &gt;= gap and collection[j - gap] &gt; temp: collection[j] = collection[j - gap] j -= gap collection[j] = temp i += 1 return collectionif __name__ == '__main__': nums = [-23, 0, 6, -4, 34, 2] print('\\n', shell_sort1(nums)) O($nlogn$)归并排序 算法描述 将数组分为两部分，分别排序，最后将两部分排好序的数组合并成一个有序的数组。利用递归的方式，重复上述过程。 归并排序为什么会高效：合并两个有序数组的时间复杂度是O(N)的，将一个完整的数组递归的一分为二，那么分解到数组中只有一个元素时，分解的次数为logn，故整体的时间复杂度为O(nlogn)。 算法实现 1234567891011121314151617181920212223242526272829303132333435363738def merge_sort(nums): print('before=', nums) length = len(nums) if length &gt; 1: midpoint = length // 2 left_half = merge_sort(nums[:midpoint]) right_half = merge_sort(nums[midpoint:]) i = 0 j = 0 k = 0 left_length = len(left_half) right_length = len(right_half) while i &lt; left_length and j &lt; right_length: if left_half[i] &lt; right_half[j]: nums[k] = left_half[i] i += 1 else: nums[k] = right_half[j] j += 1 k += 1 while i &lt; left_length: nums[k] = left_half[i] i += 1 k += 1 while j &lt; right_length: nums[k] = right_half[j] j += 1 k += 1 print('after=', nums) return numsif __name__ == '__main__': nums = [-23, 0, 6, -4, 34, 2] print('\\n', merge_sort(nums)) 1234567891011121314151617181920212223def merge_sort(nums, left, right): if left &gt;= right: return mid = (left + right) &gt;&gt; 1 # 二分 merge_sort(nums, left, mid) # 排列左半部分 merge_sort(nums, mid + 1, right) # 排列右半部分 merge(nums, left, mid, right) # 合并左右两个已经有序的数组def merge(nums, left, mid, right): tmp = [0] * (right - left + 1) i, j, k = left, mid + 1, 0 while i &lt;= mid and j &lt;= right: if nums[i] &lt; nums[j]: tmp[k], i = nums[i], i + 1 else: tmp[k], j = nums[j], j + 1 k += 1 while i &lt;= mid: tmp[k], k, i = nums[i], k + 1, i + 1 while j &lt;= right: tmp[k], k, j = nums[j], k + 1, j + 1 nums[left:right+1] = tmp[:]nums = [-23, 0, 6, -4, 34, 2]merge_sort(nums, 0, len(nums) - 1)print(nums) 12345678910111213141516171819202122232425def merge_sort(nums, left, right): if left &gt;= right: return mid = (left + right) &gt;&gt; 1 # 二分 merge_sort(nums, left, mid) # 排列左半部分 merge_sort(nums, mid + 1, right) # 排列右半部分 merge(nums, left, mid, right) # 合并左右两个已经有序的数组def merge(nums, left, mid, right): tmp = [] i, j = left, mid + 1 while i &lt;= mid and j &lt;= right: if nums[i] &lt; nums[j]: tmp.append(nums[i]) i += 1 else: tmp.append(nums[j]) j += 1 tmp.extend(nums[i:mid + 1] if i &lt;= mid else nums[j:right + 1]) nums[left:right + 1] = tmp[:]nums = [-23, 88, 4, 2, 99, 12, 0, 6, -4, 34, 2]merge_sort(nums, 0, len(nums) - 1)print(nums) 123456789101112131415161718192021222324252627282930313233343536373839404142// Javapublic static void mergeSort(int[] array, int left, int right) &#123; if (right &lt;= left) return; int mid = (left + right) &gt;&gt; 1; // (left + right) / 2 mergeSort(array, left, mid); mergeSort(array, mid + 1, right); merge(array, left, mid, right);&#125;public static void merge(int[] arr, int left, int mid, int right) &#123; int[] temp = new int[right - left + 1]; // 中间数组 int i = left, j = mid + 1, k = 0; while (i &lt;= mid &amp;&amp; j &lt;= right) &#123; temp[k++] = arr[i] &lt;= arr[j] ? arr[i++] : arr[j++]; &#125; while (i &lt;= mid) temp[k++] = arr[i++]; while (j &lt;= right) temp[k++] = arr[j++]; for (int p = 0; p &lt; temp.length; p++) &#123; arr[left + p] = temp[p]; &#125; // 也可以用 System.arraycopy(a, start1, b, start2, length)&#125;public static void merge(int[] arr, int left, int mid, int right) &#123; int[] temp = new int[right - left + 1]; // 中间数组 int i = left, j = mid + 1, k = 0; while (i &lt;= mid &amp;&amp; j &lt;= right) &#123; temp[k++] = arr[i] &lt;= arr[j] ? arr[i++] : arr[j++]; &#125; if (i&lt;=mid) System.arraycopy(arr, i, temp, k, mid-i+1); if (j&lt;=right) System.arraycopy(arr, j, temp, k, right-j+1); System.arraycopy(temp, 0, arr, left, temp.length);&#125; 快速排序 算法描述 随机选择一个pivot节点，然后将数组中的数据分成大于pivot和小于pivot的两部分，然后递归地将大于pivot和小于pivot的部分再按照相同的思路处理，直到每个pivot两端的部分都只有最多一个元素 算法实现 123456789def quick_sort(collection): length = len(collection) if length &lt;= 1: return collection else: pivot = collection[0] greater = [element for element in collection[1:] if element &gt; pivot] lesser = [element for element in collection[1:] if element &lt;= pivot] return quick_sort(lesser) + [pivot] + quick_sort(greater) O(n) 时间复杂度内求无序数组中的第 K 大元素 1234567891011121314151617# 选择数组的最后一个元素，作为pivot，然后将数组的所有元素分为大于pivot和小于pivot的两部分，# 如果 len(lesser) == k - 1，则返回pivot# 如果 len(lesser) &gt;= k，则说明要查找的元素在小于pivot的部分，那么继续在lesser中查找# 否则的话，说明要查找的元素在大于pivot的部分，那么继续在greater中查找def find_k_max(nums, k): length = len(nums) if length &lt; k: return None pivot = nums[length - 1] greater = [element for element in nums[:length - 1] if element &gt; pivot] lesser = [element for element in nums[:length - 1] if element &lt;= pivot] if len(lesser) == k - 1: return pivot elif len(lesser) &gt;= k: return find_k_max(lesser, k) else: return find_k_max(greater, k - len(lesser) - 1) In-place算法实现 1234567891011121314151617181920212223def quick_sort(nums, begin, end): if begin&gt;=end: return pivot = partition(nums, begin, end) # 找到[begin,end]的pivot点，pivot点已经固定了，是不需要对它进行排序的 quick_sort(nums, begin, pivot-1) # 递归调用nums[begin, pivot-1] quick_sort(nums, pivot+1, end) # 递归调用nums[pivot+1, end] return numsdef partition(nums, begin, end): # pivot是作为对比的值 # last_smaller是指向小于pivot的子数组的下一个位置的指针 类似于leetcode [移动零] 那一题的解法 pivot, last_smaller = end, begin for i in range(begin, end): # 从前往后遍历待排数组 if nums[i]&lt;nums[pivot]: # 如果当前元素小于pivot，那么调换当前元素和last_smaller位置的元素 nums[i], nums[last_smaller] = nums[last_smaller], nums[i] last_smaller+=1 # last_smaller 向后移动一位 # 把pivot的值移动到last_smaller位置 nums[last_smaller], nums[pivot] = nums[pivot], nums[last_smaller] return last_smaller nums = []quick_sort(nums, 0, len(nums)-1) 123456789101112131415161718192021// Javapublic static void quickSort(int[] array, int begin, int end) &#123; if (end &lt;= begin) return; int pivot = partition(array, begin, end); quickSort(array, begin, pivot - 1); quickSort(array, pivot + 1, end);&#125;static int partition(int[] a, int begin, int end) &#123; // pivot: 标杆位置 // last_smaller是指向小于pivot的子数组的下一个位置的指针 类似于leetcode [移动零] 那一题的解法 int pivot = end, last_smaller = begin; for (int i = begin; i &lt; end; i++) &#123; if (a[i] &lt; a[pivot]) &#123; int temp = a[last_smaller]; a[last_smaller] = a[i]; a[i] = temp; last_smaller++; &#125; &#125; int temp = a[pivot]; a[pivot] = a[last_smaller]; a[last_smaller] = temp; return counter;&#125; 堆排序 算法描述 使用大顶堆和小顶堆的数据结构，进行排序：大顶堆，每次取堆顶元素，遍历完则得到从大到小的序列；小顶堆则相反。 堆有很多种实现，这里只看二叉堆。二叉堆是一种近似完全二叉树，所以可以使用数组存储，每个父亲节点都要比其孩子节点大(大顶堆，小顶堆相反) 利用堆进行排序，包含两个步骤：第一、将数据放入堆中以满足堆的条件；第二、将数据从堆中取出。即为有序数组 算法实现 1234567891011121314151617181920212223242526#Pythondef heapify(parent_index, length, nums): temp = nums[parent_index] child_index = 2*parent_index+1 # 取到左孩子 while child_index &lt; length: # 先检查右孩子，看右孩子是不是比左孩子大，是的话更新孩子节点索引到 右孩子 if child_index+1 &lt; length and nums[child_index+1] &gt; nums[child_index]: child_index = child_index+1 # 如果父结点大于孩子节点，那么停止循环 if temp &gt; nums[child_index]: break # 将父结点更新到新检查的孩子节点，重复上面的操作 nums[parent_index] = nums[child_index] parent_index = child_index child_index = 2*parent_index + 1 nums[parent_index] = temp # 最后，把父结点元素归位def heapsort(nums): ## start from (len(nums)-2)//2:看一下图就知道了 for i in range((len(nums)-2)//2, -1, -1): heapify(i, len(nums), nums) for j in range(len(nums)-1, 0, -1): nums[j], nums[0] = nums[0], nums[j] heapify(0, j, nums) 当len=12，那么从数组下标为5开始调整，图中的6位置 当len=11，那么从数组下标为4开始调整，图中的5位置 依次类推 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374// Javastatic void heapify(int[] array, int length, int i) &#123; int left = 2 * i + 1, right = 2 * i + 2； int largest = i; if (left &lt; length &amp;&amp; array[left] &gt; array[largest]) &#123; largest = left; &#125; if (right &lt; length &amp;&amp; array[right] &gt; array[largest]) &#123; largest = right; &#125; if (largest != i) &#123; int temp = array[i]; array[i] = array[largest]; array[largest] = temp; heapify(array, length, largest); &#125;&#125;public static void heapSort(int[] array) &#123; if (array.length == 0) return; int length = array.length; for (int i = length / 2-1; i &gt;= 0; i--) heapify(array, length, i); for (int i = length - 1; i &gt;= 0; i--) &#123; int temp = array[0]; array[0] = array[i]; array[i] = temp; heapify(array, i, 0); &#125;&#125;// 数组中的第K个最大元素class Solution &#123; public int findKthLargest(int[] nums, int k) &#123; int heapSize = nums.length; buildMaxHeap(nums, heapSize); for (int i = nums.length - 1; i &gt;= nums.length - k + 1; --i) &#123; swap(nums, 0, i); --heapSize; maxHeapify(nums, 0, heapSize); &#125; return nums[0]; &#125; public void buildMaxHeap(int[] a, int heapSize) &#123; for (int i = heapSize / 2; i &gt;= 0; --i) &#123; maxHeapify(a, i, heapSize); &#125; &#125; public void maxHeapify(int[] a, int i, int heapSize) &#123; int l = i * 2 + 1, r = i * 2 + 2, largest = i; if (l &lt; heapSize &amp;&amp; a[l] &gt; a[largest]) &#123; largest = l; &#125; if (r &lt; heapSize &amp;&amp; a[r] &gt; a[largest]) &#123; largest = r; &#125; if (largest != i) &#123; swap(a, i, largest); maxHeapify(a, largest, heapSize); &#125; &#125; public void swap(int[] a, int i, int j) &#123; int temp = a[i]; a[i] = a[j]; a[j] = temp; &#125;&#125; O($n$)计数排序 算法描述 计数排序不是基于比较的排序算法，其核心在于将输入的数据值转化为键存储在额外开辟的数组空间中。 作为一种线性时间复杂度的排序，计数排序要求输入的数据必须是有确定范围的整数。 计数排序是一个稳定的排序算法。当输入的元素是 n 个 0到 k 之间的整数时，时间复杂度是O(n+k)，空间复杂度也是O(n+k)，其排序速度快于任何比较排序算法。当k不是很大并且序列比较集中时，计数排序是一个很有效的排序算法。 步骤 找出待排序的数组中最大和最小的元素； 统计数组中每个值为i的元素出现的次数，存入数组C的第i项； 对所有的计数累加（从C中的第一个元素开始，每一项和前一项相加）； 反向填充目标数组：将每个元素i放在新数组的第C(i)项，每放一个元素就将C(i)减去1。 使用的局限性： 待排序的数组元素只能为有确定范围的整数，因为要使用数组的索引来标识元素的顺序 有时候，小数和负数，可以通过乘以一个倍数或者加上一个正数，来调整成可以使用计数排序的形式 待排序数组的范围不能太大，否则会占用大量的内存，如果数据倾斜严重，可能只会使用到一小部分的位置，会造成大量的内存白白消耗 升级： 可以使用下面要讲的桶排序，来弥补计数排序的两点不足 算法实现 1234567891011121314151617max_value = 1000def count_sort(nums): bucket = [0] * max_value for num in nums: bucket[num] += 1 j = 0 for i in range(max_value): nums[j:j + bucket[i]] = ([i] * bucket[i])[:] j += bucket[i] return numsnums = [9, 2, 3, 1, 12, 3, 0, 3, 3, 3, 3, 3, 3, 4, 4, 4, 5, 56, 6, 7, 7, 78, 8, 8, 8, 8]count_sort(nums)print(nums) 基数排序 算法描述 基数排序是按照低位先排序，然后收集；再按照高位排序，然后再收集；依次类推，直到最高位。有时候有些属性是有优先级顺序的，先按低优先级排序，再按高优先级排序。最后的次序就是高优先级高的在前，高优先级相同的低优先级高的在前。 步骤 取得数组中的最大数，并取得位数； arr为原始数组，从最低位开始取每个位组成radix数组； 对radix进行计数排序（利用计数排序适用于小范围数的特点）； 基数排序基于分别排序，分别收集，所以是稳定的。但基数排序的性能比桶排序要略差，每一次关键字的桶分配都需要O(n)的时间复杂度，而且分配之后得到新的关键字序列又需要O(n)的时间复杂度。假如待排数据可以分为d个关键字，则基数排序的时间复杂度将是O(d*2n) ，当然d要远远小于n，因此基本上还是线性级别的。 基数排序的空间复杂度为O(n+k)，其中k为桶的数量。一般来说n&gt;&gt;k，因此额外空间需要大概n个左右。 算法实现 1234567891011121314151617181920def radix_sort(nums, max_digit): \"\"\" max_digit 表示最大的位数 \"\"\" bucket = [[] for _ in range(10)] # 十进制数每位只能有10个选择：0-9 for digit in range(max_digit): # 从低位开始分桶 for num in nums: # 对每个数整除10的倍数 取余，填入对一个的坑位 bucket[(num // (10 ** digit)) % 10].append(num) j = 0 for i in range(10): # 遍历10个坑位，把数据回填入nums nums[j:j + len(bucket[i])] = bucket[i][:] j += len(bucket[i]) bucket = [[] for _ in range(10)] # 一轮结束后，清空原来的bucket return numsres = radix_sort([32, 43, 4, 54, 24, 1, 34, 3, 1, 13, 4, 4, 134, 4, 53, 34, 1, 34, 3, 3, 4, 2, 1, 4, ], 3)print(res) 桶排序 算法描述 桶排序是计数排序的升级版。它利用了函数的映射关系，高效与否的关键就在于这个映射函数的确定。桶排序 (Bucket sort)的工作的原理：假设输入数据服从均匀分布，将数据分到有限数量的桶里，每个桶再分别排序（有可能再使用别的排序算法或是以递归方式继续使用桶排序进行排）。 桶排序最好情况下使用线性时间O(n)，桶排序的时间复杂度，取决与对各个桶之间数据进行排序的时间复杂度，因为其它部分的时间复杂度都为O(n)。很显然，桶划分的越小，各个桶之间的数据越少，排序所用的时间也会越少。但相应的空间消耗就会增大 算法实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344import mathfrom TheAlgorithms.sorts.insert_sort import insert_sort1DEFAULT_BUCKET_SIZE = 5def bucket_sort(my_list, bucket_size=DEFAULT_BUCKET_SIZE): if len(my_list) == 0: print('You don\\'t have any elements in array!') minValue = my_list[0] maxValue = my_list[0] # For finding minimum and maximum values for i in range(0, len(my_list)): if my_list[i] &lt; minValue: minValue = my_list[i] elif my_list[i] &gt; maxValue: maxValue = my_list[i] # Initialize buckets bucketCount = math.floor((maxValue - minValue) / bucket_size) + 1 buckets = [] for i in range(0, bucketCount): buckets.append([]) # For putting values in buckets for i in range(0, len(my_list)): buckets[math.floor((my_list[i] - minValue) / bucket_size)].append(my_list[i]) # Sort buckets and place back into input array sorted_array = [] for i in range(0, len(buckets)): insert_sort1(buckets[i]) for j in range(0, len(buckets[i])): sorted_array.append(buckets[i][j]) return sorted_arrayif __name__ == '__main__': sorted_array = bucket_sort([12, 23, 4, 5, 3, 2, 12, 81, 56, 95]) print(sorted_array)","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"}],"tags":[{"name":"排序算法","slug":"排序算法","permalink":"https://shang.at/tags/排序算法/"}]},{"title":"数据分析-reduce函数引发的","slug":"数据分析-reduce函数引发的","date":"2019-03-28T05:35:25.000Z","updated":"2019-04-10T15:36:43.244Z","comments":true,"path":"post/数据分析-reduce函数引发的/","link":"","permalink":"https://shang.at/post/数据分析-reduce函数引发的/","excerpt":"","text":"reduce in python12345678910111213141516# _functools.reducedef reduce(function, sequence, initial=None): \"\"\" reduce(function, sequence[, initial]) -&gt; value Apply a function of two arguments cumulatively to the items of a sequence, from left to right, so as to reduce the sequence to a single value. For example, reduce(lambda x, y: x+y, [1, 2, 3, 4, 5]) calculates ((((1+2)+3)+4)+5). If initial is present, it is placed before the items of the sequence in the calculation, and serves as a default when the sequence is empty. :param function:给定的一个func，func具有两个参数，参数1是临时聚合值，参数2是序列中下一个待聚合的值 :param sequence:待处理的可迭代的序列 :param initial:聚合数据的初始值 \"\"\" pass 工作原理：reduce函数对给定的序列遍历调用func函数，每次调用返回一个临时聚合值，直到整个序列遍历结束。如果设置了初始值，那么在第一次执行func函数的时候，会将func的参数1设置为初始值。 例子： 1234567&gt;&gt;&gt; from functools import reduce&gt;&gt;&gt; reduce(lambda x, y: x+y, [1, 2, 3, 4, 5])15&gt;&gt;&gt; reduce(lambda x, y: x+y, [1, 2, 3, 4, 5], 100)115&gt;&gt;&gt; reduce(lambda x, y: str(x)+str(y), [1, 2, 3, 4, 5], '')'12345' reduce函数不仅可以完成这种聚合的功能，还可以完成更加复杂的操作， reduce&amp;foldLeft&amp;foldRight&amp;reduce in scala hive的UDAFspark的UDAF","categories":[{"name":"数据分析","slug":"数据分析","permalink":"https://shang.at/categories/数据分析/"}],"tags":[{"name":"reduce","slug":"reduce","permalink":"https://shang.at/tags/reduce/"},{"name":"数据分析技巧","slug":"数据分析技巧","permalink":"https://shang.at/tags/数据分析技巧/"},{"name":"有初始值的聚合操作","slug":"有初始值的聚合操作","permalink":"https://shang.at/tags/有初始值的聚合操作/"}]},{"title":"信贷数据统计的相关指标","slug":"信贷数据统计的相关指标","date":"2019-03-22T11:19:47.000Z","updated":"2019-03-28T14:24:24.845Z","comments":true,"path":"post/信贷数据统计的相关指标/","link":"","permalink":"https://shang.at/post/信贷数据统计的相关指标/","excerpt":"","text":"贷款类型 等额本息贷款 根据固定的还款时间，计算出应还的总利息，再加上本金，然后每个月平均等额的还款。 等额本金贷款 等额本金相对来说要简单一些，每月所还的本金是相同的，利息由每个月的剩余本金计算得出。 固定点数贷款 按照定义，我们在首次还款时先按固定的点数还一部分贷款，然后再按较低的利率还完剩余的贷款。 双利率贷款 前x个月以较低的r1利率还款，后m-x个月以较高的r2利率还款（假设还款总月数为m） 相关指标 同比增长 环比增长","categories":[{"name":"数据分析","slug":"数据分析","permalink":"https://shang.at/categories/数据分析/"}],"tags":[{"name":"分析指标","slug":"分析指标","permalink":"https://shang.at/tags/分析指标/"}]},{"title":"airflow安装","slug":"airflow安装","date":"2019-03-19T11:24:02.000Z","updated":"2019-03-24T01:33:47.230Z","comments":true,"path":"post/airflow安装/","link":"","permalink":"https://shang.at/post/airflow安装/","excerpt":"","text":"123456789101112131415161718# airflow needs a home, ~/airflow is the default,# but you can lay foundation somewhere else if you prefer# (optional)export AIRFLOW_HOME=~/airflow# install from pypi using pippip install apache-airflow# initialize the databaseairflow initdb# start the web server, default port is 8080airflow webserver -p 8080# start the schedulerairflow scheduler# visit localhost:8080 in the browser and enable the example dag in the home page USE Mysql123vim $AIRFLOW_HOME/airflow.cfg sql_alchemy_conn = mysql+pymysql://root:123456@localhost:3306/airflow 需要pip install pymysql 启动失败1234567891011121314151617181920ERROR [airflow.models.DagBag] Failed to import: /anaconda3/lib/python3.7/site-packages/airflow/example_dags/example_http_operator.pyTraceback (most recent call last): File &quot;/anaconda3/lib/python3.7/site-packages/airflow/models.py&quot;, line 374, in process_file m = imp.load_source(mod_name, filepath) File &quot;/anaconda3/lib/python3.7/imp.py&quot;, line 171, in load_source module = _load(spec) File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 696, in _load File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed File &quot;/anaconda3/lib/python3.7/site-packages/airflow/example_dags/example_http_operator.py&quot;, line 27, in &lt;module&gt; from airflow.operators.http_operator import SimpleHttpOperator File &quot;/anaconda3/lib/python3.7/site-packages/airflow/operators/http_operator.py&quot;, line 21, in &lt;module&gt; from airflow.hooks.http_hook import HttpHook File &quot;/anaconda3/lib/python3.7/site-packages/airflow/hooks/http_hook.py&quot;, line 23, in &lt;module&gt; import tenacity File &quot;/anaconda3/lib/python3.7/site-packages/tenacity/__init__.py&quot;, line 352 from tenacity.async import AsyncRetrying ^SyntaxError: invalid syntax 修复方式：修改from tenacity.async import AsyncRetrying为from tenacity.async_a import AsyncRetrying，同时tenacity包下的async文件名为async_a","categories":[],"tags":[]},{"title":"Spark学习笔记-抽样方法和自增ID","slug":"Spark学习笔记-抽样方法和自增ID","date":"2019-03-19T08:33:24.000Z","updated":"2020-07-01T09:09:37.773Z","comments":true,"path":"post/Spark学习笔记-抽样方法和自增ID/","link":"","permalink":"https://shang.at/post/Spark学习笔记-抽样方法和自增ID/","excerpt":"","text":"抽样方法sample(withReplacement=None, fraction=None, seed=None) Returns a sampled subset of this DataFrame. withReplacement – Sample with replacement or not (default False). true时会将抽样的数据放回数据集，导致抽样数据有重复的 false时不会放回 fraction – Fraction of rows to generate, range [0.0, 1.0]. 表示子集占数据集的占比 seed – Seed for sampling (default a random seed). fraction并不能保证完全按照占比抽样数据 自增IDmonotonically_increasing_id() 每个分区分别排序生成一个64位的整数，但不是连续的。会将分区值放到高31位，然后将每条记录的序列放到低33位。限制：分区数不能大于10亿，每个分区的数据量不能大于80亿。","categories":[{"name":"Spark学习","slug":"Spark学习","permalink":"https://shang.at/categories/Spark学习/"}],"tags":[{"name":"抽样方法和自增ID","slug":"抽样方法和自增ID","permalink":"https://shang.at/tags/抽样方法和自增ID/"}]},{"title":"Spark学习笔记-SparkSQL内置函数","slug":"Spark学习笔记-SparkSQL内置函数","date":"2019-03-19T01:27:01.000Z","updated":"2021-02-24T10:55:03.002Z","comments":true,"path":"post/Spark学习笔记-SparkSQL内置函数/","link":"","permalink":"https://shang.at/post/Spark学习笔记-SparkSQL内置函数/","excerpt":"","text":"学习SparkSQL中的一些内置函数 日期函数 获取默认时区 123spark.conf.get('spark.sql.session.timeZone')&gt;&gt; 'Asia/Shanghai' 获取当前时间 获取当前日期：current_date() 12345spark.sql(\"\"\" select current_date()\"\"\").toPandas()&gt;&gt; 2019-03-19 获取当前时间：current_timestamp()/now() 12345spark.sql(\"\"\" select current_timestamp()\"\"\").toPandas()&gt;&gt; 2019-03-19 13:54:22.236 从日期中截取字段 截取年月日、时分秒:year,month,day/dayofmonth,hour,minute,second dayofweek ,dayofyear 11 = Sunday, 2 = Monday, ..., 7 = Saturday weekofyear 1Extract the week number of a given date as integer. trunc截取某部分的日期，其他部分默认为01 123Returns date truncated to the unit specified by the format.Parameters: format – ‘year’, ‘yyyy’, ‘yy’ or ‘month’, ‘mon’, ‘mm’ date_trunc [“YEAR”, “YYYY”, “YY”, “MON”, “MONTH”, “MM”, “DAY”, “DD”, “HOUR”, “MINUTE”, “SECOND”, “WEEK”, “QUARTER”] 123Returns timestamp truncated to the unit specified by the format.Parameters: format – ‘year’, ‘yyyy’, ‘yy’, ‘month’, ‘mon’, ‘mm’, ‘day’, ‘dd’, ‘hour’, ‘minute’, ‘second’, ‘week’, ‘quarter’ date_format将时间转化为某种格式的字符串 123Converts a date/timestamp/string to a value of string in the format specified by the date format given by the second argument.A pattern could be for instance dd.MM.yyyy and could return a string like ‘18.03.1993’. All pattern letters of the Java class java.text.SimpleDateFormat can be used. 日期时间转换 unix_timestamp返回当前时间的unix时间戳 123Convert time string with given pattern (‘yyyy-MM-dd HH:mm:ss’, by default) to Unix time stamp (in seconds), using the default timezone and the default locale, return null if fail.if timestamp is None, then it returns current timestamp. from_unixtime将时间戳换算成当前时间，to_unix_timestamp将时间转化为时间戳 1Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string representing the timestamp of that moment in the current system time zone in the given format. to_date/date将字符串转化为日期格式，to_timestamp（Since: 2.2.0） 123Converts a Column of pyspark.sql.types.StringType or pyspark.sql.types.TimestampType into pyspark.sql.types.DateType using the optionally specified format. Specify formats according to SimpleDateFormats. By default, it follows casting rules to pyspark.sql.types.DateType if the format is omitted (equivalent to col.cast(&quot;date&quot;)).Converts a Column of pyspark.sql.types.StringType or pyspark.sql.types.TimestampType into pyspark.sql.types.DateType using the optionally specified format. Specify formats according to SimpleDateFormats. By default, it follows casting rules to pyspark.sql.types.TimestampType if the format is omitted (equivalent to col.cast(&quot;timestamp&quot;)). quarter 将1年4等分(range 1 to 4) 1Extract the quarter of a given date as integer. 日期、时间计算 months_between两个日期之间的月数 add_months返回日期后n个月后的日期 last_day(date),next_day(start_date, day_of_week) date_add,date_sub(减) datediff（两个日期间的天数） utc 在集群中对于时间戳的转换，如果不指定时区，默认会采用集群配置的时区，集群默认时区可以通过如下方式获取：spark.conf.get(‘spark.sql.session.timeZone’)。一般而言，这个值应该是集群统一设置，独立提交job的时候，不需要设置。 to_utc_timestamp(timestamp, tz) 1将timestamp按照给定的tz解释，返回utc timestamp from_utc_timestamp(timestamp, tz) 1将timestamp按照utc解释，返回给定tz的timestamp 对于有时区相关的数据统计时，需要注意。比如：集群默认时区设置为UTC，一般将数据存到集群中的时候会将时间戳转为utc timestamp以便后续的操作。此时如果有一个需求是统计北京时间的当天的数据，那么第一个想到的方式是使用current_date()获取当前日期，然后将数据中的时间戳使用to_date(from_utc_timestamp(from_unixtime(ts), ‘Asia/Beijing’))，然后进行比较。但是current_date()获取的日期，是根据集群默认时区得来的，因此会有时区的不同导致的数据统计错误，因此，这种情况不能直接使用current_date()，正确的使用方式是：to_date(from_utc_timestamp(current_timestamp(), ‘Asia/Beijing’))，然后在进行比较。 表关联 Join(other, on=None, how=None) on：a string for the join column name, a list of column names, a join expression (Column), or a list of Columns. If on is a string or a list of strings indicating the name of the join column(s), the column(s) must exist on both sides, and this performs an equi-join. how：str, default inner. Must be one of: inner, cross, outer, full, full_outer, left, left_outer, right, right_outer, left_semi, and left_anti inner:内连，返回joinDF1和joinDF2合并的rows，如果joinDF2中有多条记录对应于joinDF1的同一条记录，那么返回的row number会大于joinDF1的row number outer,full,full_outer：全连 left, left_outer：左连 right，right_outer:右连 left_semi：过滤出joinDF1中和joinDF2共有的部分，只返回joinDF1中的rows left_anti：过滤出joinDF1中joinDF2没有的部分，只返回joinDF1中的rows crossJoin(other) 返回两个DF的笛卡尔积：笛卡尔积会极大的降低任务执行的效率 join优化，spark会默认将小表广播，按照如下的参数设置，满足条件就会广播 参数优化： spark.sql.broadcastTimeout：broadcast的加大超时的时间限制 spark.sql.autoBroadcastJoinThreshold：默认是10M，大小低于该参数设置的阈值时，会被广播，但是默认的BroadCastJoin会将小表的内容，全部收集到Driver中，导致Driver压力变大 spark.sql.bigdata.useExecutorBroadcast：设置为true时，使用Executor广播，将表数据缓存在Executor中，而不是放在Driver之中，减少Spark Driver内存的压力。 在join中使用or连接关键字，会导致笛卡尔积的产生CartesianProduct(当join多个表)|BroadcastNestedLoopJoin(只join一个table)，故不建议这样做，建议使用多次join，取不为空的结果，比如： 12345678910111213141516-- 错误的示范select e.device_id , e,user_id , ab.branchfrom app_event eleft join ab_test ab on e.device_id=ab.admin_id or e.user_id=ab.admin_id-- 正确的写法select e.device_id , e,user_id , COALESCE(ab.branch, ab1.branch) branchfrom app_event eleft join ab_test ab on e.device_id=ab.admin_idleft join ab_test ab1 on e.user_id=ab.admin_id Parses the expression expr 将字符串表示的表达式，翻译成DSL 123expr(\"length(name)\")expr(\"array_contains(user_id_set, user_id)\")","categories":[{"name":"Spark学习","slug":"Spark学习","permalink":"https://shang.at/categories/Spark学习/"}],"tags":[{"name":"sparkSql内置函数","slug":"sparkSql内置函数","permalink":"https://shang.at/tags/sparkSql内置函数/"}]},{"title":"数据分析小知识点","slug":"数据分析小知识点","date":"2019-03-19T01:25:56.000Z","updated":"2021-02-26T09:10:16.159Z","comments":true,"path":"post/数据分析小知识点/","link":"","permalink":"https://shang.at/post/数据分析小知识点/","excerpt":"","text":"总结一下在数据分析中需要注意的一些tips，持续更新 Tip1 时区在进行跨境业务处理的时候，时区的控制是十分必要的。平时对于国内的业务，部署在国内的服务器，使用的时区一般都是北京时间(北京时间是UTC+8:00时区的时间，而UTC时间指UTC+0:00时区的时间)，在数据库中一般存储相对于unix epoch (1970-01-01 00:00:00 UTC)的毫秒时间戳，做某个地区的数据统计时，需要将时间戳转换成当地的时间(即加一个时区的属性) https://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000/001431937554888869fb52b812243dda6103214cd61d0c2000 mysql数据库中 datetime类型的字段是没有时区信息的，只是一个日期时间的字符串，如果使用者提前不知道存入的数据是什么时区，那么这个字段就没有任何意义。 Tip 2 Excel函数 去重计数：SUMPRODUCT(1/COUNTIF(A2:A20,A2:A20)) VLOOKUP(要查找的值,查找返回,返回查找到的第几列,是否精确查找[1]) 概念落地页，也称：着陆页、引导页，是指访问者在其他地方看到发出的某个具有明确主题的特定营销活动——通过Email、社交媒体或广告发布的诱人优惠信息等，点击后被链接到你网站上的第一个页面 PRD：产品需求文档，产品需求文档是将商业需求文档（BRD）和市场需求文档（MRD）用更加专业的语言进行描述","categories":[{"name":"数据分析","slug":"数据分析","permalink":"https://shang.at/categories/数据分析/"}],"tags":[{"name":"数据分析Tips","slug":"数据分析Tips","permalink":"https://shang.at/tags/数据分析Tips/"}]},{"title":"Spark学习笔记-窗口函数","slug":"Spark学习笔记-窗口函数","date":"2019-03-12T02:15:03.000Z","updated":"2021-06-01T11:13:10.117Z","comments":true,"path":"post/Spark学习笔记-窗口函数/","link":"","permalink":"https://shang.at/post/Spark学习笔记-窗口函数/","excerpt":"","text":"初始化环境1234567891011from pyspark.sql import SparkSessionfrom pyspark.sql.functions import *from pyspark.sql import Windowfrom pyspark.sql.types import StructType, StringType, StructField, IntegerTypeschema = StructType([ StructField('shop_id', StringType()), StructField('date', StringType()), StructField('amount', IntegerType())]) 12345spark = SparkSession \\ .builder \\ .master('local[*]') \\ .enableHiveSupport() \\ .getOrCreate() 123456data = [ &#123;'shop_id': '10006', 'date': '201501120030', 'amount': 2313&#125;, &#123;'shop_id': '10006', 'date': '201501120100', 'amount': 23112&#125;, &#123;'shop_id': '10006', 'date': '201501120130', 'amount': 23112&#125;, &#123;'shop_id': '10006', 'date': '201501120200', 'amount': 24234&#125;, ] 1df = spark.createDataFrame(data, schema) 1df.printSchema() 1234root |-- shop_id: string (nullable = true) |-- date: string (nullable = true) |-- amount: integer (nullable = true) 关于子窗口子窗口需要指定一个边界，有以下两种方式： ROWS between CURRENT ROW | UNBOUNDED PRECEDING | [num] PRECEDING AND UNBOUNDED FOLLOWING | [num] FOLLOWING| CURRENT ROW RANGE between [num] PRECEDING AND [num] FOLLOWING 窗口的含义 ROWS是物理窗口，从行数上控制窗口的尺寸的；RANGE是逻辑窗口，从列值上控制窗口的尺寸 通常会结合order by子句使用，如果在order by子句后面没有指定窗口子句，则默认为：rows between unbounded preceding and current row 12345678910111213141516171819202122spark中关于Window函数的学习在spark中涉及Window函数的主要有以下两个类和一个Column的方法pyspark.sql.column.Column#over 在窗口上应用某一种分析函数pyspark.sql.window.Window 创建WindowSpec的工具类 pyspark.sql.window.Window.unboundedPreceding pyspark.sql.window.Window.unboundedFollowing pyspark.sql.window.Window.currentRow pyspark.sql.window.Window#partitionBy pyspark.sql.window.Window#orderBy pyspark.sql.window.Window#rowsBetween(start, end) pyspark.sql.window.Window#rangeBetween(start, end)pyspark.sql.window.WindowSpec 窗口的规范pyspark.sql.window.Window#rowsBetween(start, end)定义窗口的边界，[start, end]，在边界处是闭区间start和end都是相对于当前row的相对位置，例如：- 0：当前row- -1：当前行的前1row- 5：当前行的后5row- (-1, 5)：窗口的范围为，当前row+当前行的前1row+当前行的后5row = 7rows 使用场景统计截止到当前时间段的店铺累计销售金额123456df.withColumn( 't_amount', sum('amount').over(Window.partitionBy('shop_id').orderBy(asc('date')))).select( 'shop_id', 'date', 't_amount').show(50, truncate=False) 分析：根据shop_id分组，根据date正序排列，由于orderBy后面没有追加rowsBetween()，则默认的rowsBetween为：[Window.unboundedPreceding，Window.currentRow]。即会统计根据date排序后，从第一行计算到当前行，从而达到了统计截止到当前时间段的店铺累计销售金额的效果 统计每个时间段的销售占比123456df.withColumn( 't_amount', col('amount')/sum('amount').over(Window.partitionBy('shop_id'))).select( 'shop_id', 'date', 'amount','t_amount').show(50, truncate=False) 分析：根据shop_id分组，不排序，窗口大小默认就是整个分组。 找出2点的销售金额及前半小时的销售金额和后1个小时的销售金额1234567891011121314151617df.withColumn( 'pre_half_hour', lag('date', 1).over(Window.partitionBy('shop_id').orderBy(asc('date')))).withColumn( 'pre_half_hour_amount', lag('amount', 1).over(Window.partitionBy('shop_id').orderBy(asc('date')))).withColumn( 'follow_one_hour', lead('date', 2).over(Window.partitionBy('shop_id').orderBy(asc('date')))).withColumn( 'follow_one_hour_amount', lead('amount', 2).over(Window.partitionBy('shop_id').orderBy(asc('date')))).filter( col('date') == '201501120200').select( 'shop_id', 'date', 'amount','pre_half_hour', 'pre_half_hour_amount', 'follow_one_hour', 'follow_one_hour_amount').show(truncate=False) 12345+-------+------------+------+-------------+--------------------+---------------+----------------------+|shop_id|date |amount|pre_half_hour|pre_half_hour_amount|follow_one_hour|follow_one_hour_amount|+-------+------------+------+-------------+--------------------+---------------+----------------------+|10006 |201501120200|24234 |201501120130 |2342 |201501120300 |31232 |+-------+------------+------+-------------+--------------------+---------------+----------------------+ 123456分析：pyspark.sql.functions.lag(col, count=1, default=none)是取前N行的值pyspark.sql.functions.lead(col, count=1, default=none)是取后N行的值。 按照销售金额进行排名，金额最大的排最前（limit可以取topn的数）123456df.withColumn( 'rn', dense_rank().over(Window.partitionBy('shop_id').orderBy(desc('amount')))).select( 'shop_id', 'date', 'amount', 'rn' ).show(50, truncate=False) 123456df.withColumn( 'rn', rank().over(Window.partitionBy('shop_id').orderBy(desc('amount')))).select( 'shop_id', 'date', 'amount', 'rn' ).show(50, truncate=False) 123456df.withColumn( 'rn', row_number().over(Window.partitionBy('shop_id').orderBy(desc('amount')))).select( 'shop_id', 'date', 'amount', 'rn' ).show(50, truncate=False) 分析：dense_rank和rank都是排名函数，区别在于dense_rank是连续排名，rank遇到排名并列时，下一列排名跳空。row_number是加行号，次序是连续的，不会存在重复的行号 按销售金额排序，取出前20%的时间段和相应金额123456df.withColumn( 'tile', ntile(5).over(Window.partitionBy('shop_id').orderBy(desc('amount')))).select( 'shop_id', 'date', 'amount', 'tile' ).show(50, truncate=False) 分析： NTILE就是把有序分区中的行分发到指定数据的组中，各个组有编号，编号从1开始，对于每一行，NTILE返回此行所属的组的编号 设置n=5，那么ntile就会把排好序的数据均分成n个组，ntile函数会返回每条数据所在组的组编号，从而可以达到取前百分比的数据 写在后面思考：在使用row_number函数的时候，并没有指定rowsBetween，那么默认应该是rows between unbounded preceding and current row。但是，结果却是把组内的所有元素都进行了标号 rowsBetween应该是针对于具有聚合性质的函数起作用","categories":[{"name":"Spark学习","slug":"Spark学习","permalink":"https://shang.at/categories/Spark学习/"}],"tags":[{"name":"窗口函数","slug":"窗口函数","permalink":"https://shang.at/tags/窗口函数/"}]},{"title":"Spark学习笔记-union方法","slug":"Spark学习笔记-union方法","date":"2019-03-12T01:53:56.000Z","updated":"2020-07-01T09:10:57.726Z","comments":true,"path":"post/Spark学习笔记-union方法/","link":"","permalink":"https://shang.at/post/Spark学习笔记-union方法/","excerpt":"","text":"方法简介pyspark.sql.dataframe.DataFrame#union(other) 1union两个df，效果相当于union all(pyspark.sql.dataframe.DataFrame#unionAll在2.0以后就被Deprecated了)。 union方法的特点是： schema会使用前面df的schema， 只有两个有相同数量列的df才能进行union， union的时候会根据列的顺序进行union，与属性名无关 案例测试123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778from pyspark.sql import SparkSessionfrom pyspark.sql.functions import *spark = SparkSession \\ .builder \\ .master('local[*]') \\ .enableHiveSupport() \\ .getOrCreate() df1 = spark.createDataFrame([&#123;'name':'cc', 'age':24&#125;, &#123;'name':'aa', 'age':25&#125;])df1.printSchema() root |-- age: long (nullable = true) |-- name: string (nullable = true)df2 = spark.createDataFrame([&#123;'name1':'bb', 'age1':2&#125;, &#123;'name1':'dd', 'age1':3&#125;])df2.printSchema() root |-- age1: long (nullable = true) |-- name1: string (nullable = true)df1.union(df2) DataFrame[age: bigint, name: string]union_df = df1.union(df2)union_df.printSchema() root |-- age: long (nullable = true) |-- name: string (nullable = true)union_df.show() +---+----+ |age|name| +---+----+ | 24| cc| | 25| aa| | 2| bb| | 3| dd| +---+----+select_union_df = df1.select('name').union(df2.select('age1'))select_union_df.printSchema() root |-- name: string (nullable = true)select_union_df.show() +----+ |name| +----+ | cc| | aa| | 2| | 3| +----+select_union_df = df1.select('name', 'age').union(df2.select('age1')) pyspark.sql.utils.AnalysisException: \"Union can only be performed on tables with the same number of columns, but the first table has 2 columns and the second table has 1 columns;;\\n'Union\\n:- Project [name#1, age#0L]\\n: +- LogicalRDD [age#0L, name#1], false\\n+- Project [age1#4L]\\n +- LogicalRDD [age1#4L, name1#5], false\\n\"select_union_df = df1.select('age').union(df2.select('name1'))select_union_df.show() +---+ |age| +---+ | 24| | 25| | bb| | dd| +---+select_union_df.printSchema() root |-- age: string (nullable = true)select_union_df = df1.select('name', 'age').union(df2.select('age1', 'name1'))select_union_df.printSchema() root |-- name: string (nullable = true) |-- age: string (nullable = true)select_union_df.show() +----+---+ |name|age| +----+---+ | cc| 24| | aa| 25| | 2| bb| | 3| dd| +----+---+ 其他案例要想实现sql中union的效果，需要结合distinct()来使用: 12345678910111213141516171819df1 = spark.createDataFrame([&#123;'name':'cc', 'age':24&#125;, &#123;'name':'aa', 'age':25&#125;])df2 = spark.createDataFrame([&#123;'name1':'cc', 'age1':24&#125;, &#123;'name1':'dd', 'age1':3&#125;])df1.union(df2).show() +---+----+ |age|name| +---+----+ | 24| cc| | 25| aa| | 24| cc| | 3| dd| +---+----+df1.union(df2).distinct().show() +---+----+ |age|name| +---+----+ | 24| cc| | 3| dd| | 25| aa| +---+----+","categories":[{"name":"Spark学习","slug":"Spark学习","permalink":"https://shang.at/categories/Spark学习/"}],"tags":[{"name":"union","slug":"union","permalink":"https://shang.at/tags/union/"}]},{"title":"Spark学习笔记一","slug":"Spark学习笔记一","date":"2019-03-10T10:45:03.000Z","updated":"2019-03-24T01:33:47.230Z","comments":true,"path":"post/Spark学习笔记一/","link":"","permalink":"https://shang.at/post/Spark学习笔记一/","excerpt":"","text":"版本：pyspark 2.4.0 主要包 pyspark pyspark.sql module pyspark.streaming module pyspark.ml package pyspark.mllib package pyspark pyspark.SparkConf(loadDefaults=True, _jvm=None, _jconf=None) 是spark应用的配置类，默认loadDefaults=True，会自动加载java系统参数中的spark.*的参数，_jconf是一个已经存在的sparkConf句柄 主要api： setMaster() 设置应用的提交类型：local|local[n]|local[*] or 不填，本地测试时可以填local系列，提交到集群运行时可以不用填，提交任务的时候会根据集群的配置，自动选择提交的类型：standalone或者yarn模式 setAppName() 设置应用的名称 pyspark.SparkContext(master=None, appName=None, sparkHome=None, pyFiles=None, environment=None, batchSize=0, serializer=PickleSerializer(), conf=None, gateway=None, jsc=None, profiler_cls=) spark应用上下文，是spark应用的主要入口。代表了与spark cluster的链接，可以用来在集群中创建RDD和广播变量。 主要api讲解： addFile(self, path, recursive=False) 为spark job添加一个可下载文件，spark的每一个node都会下载一份，可以是local file、hdfs file、http file、https file或ftp file。可以使用SparkFiles通过文件名来读取设置的文件， 注意，每个应用中，每个文件名只能设置一次。recursive设置为True时，传递的path可以是目录，但是目前只支持hdfs file的场景 12345678910111213141516171819202122232425262728import osfrom tempfile import gettempdirfrom pyspark import SparkConffrom pyspark import SparkContextif __name__ == '__main__': from pyspark import SparkFiles conf = SparkConf() conf.setMaster(\"local\").setAppName(\"My app\") sc = SparkContext(conf=conf) path = os.path.join(gettempdir(), \"test.txt\") print(path) with open(path, \"w\") as testFile: _ = testFile.write(\"100\") sc.addFile(path) def func(iterator): with open(SparkFiles.get('test.txt')) as testFile: fileVal = int(testFile.readline()) print(fileVal) return [x * fileVal for x in iterator] result = sc.parallelize([1, 2, 3, 4]).mapPartitions(func).collect() print(result) accumulator(value, accum_param=None) 创建一个累加器。一个全局共享的可以进行累加的变量，只能在worker上进行update操作，在driver上获取结果值得操作。值类型默认是int和float类型，也可以使用accum_param参数设置为自定义的数据类型 1# 代码后加 broadcast(value) 在集群中广播一个只读的值，返回一个Broadcast对象，以便在分布式方法中调用。被广播的变量只会被发送到集群的各个node上一次","categories":[{"name":"Spark","slug":"Spark","permalink":"https://shang.at/categories/Spark/"}],"tags":[{"name":"pyspark官方文档学习","slug":"pyspark官方文档学习","permalink":"https://shang.at/tags/pyspark官方文档学习/"}]},{"title":"数据仓库学习笔记二-建模流程","slug":"数据仓库学习笔记二-建模流程","date":"2019-03-03T11:36:50.000Z","updated":"2020-12-11T08:13:52.504Z","comments":true,"path":"post/数据仓库学习笔记二-建模流程/","link":"","permalink":"https://shang.at/post/数据仓库学习笔记二-建模流程/","excerpt":"","text":"数据建模的基本流程在建模的不同阶段，将数据模型分为三个层次，每层的作用各不相同。 概念模型:确定系统的核心以及划清系统范围和 边界 逻辑模型:梳理业务规则以及对概念模型的求精 物理模型:从性能、访问、开发等多方面考虑， 做系统的实现 概念模型概念建模小贴士1 注重全局的理解而非细节 在概念模型阶段，即需要对整体架构做思考 概念模型通常是自上而下的模式，通过会议等模式反复沟通，澄清需求 在此阶段，应粗略地估算出整个项目需要的时间以及项目计划草案 根据计划粗略地估算出项目的费用 是数据模型工程师与客户沟通的破冰之旅，使他们在此期间达成共识并奠定未来良好的沟通基础以及私人关系 出品的概念模型可以帮助划定系统边界以及避免方向性的错误 商业主导，相比技术专家而言，更需要商业专家 是未来逻辑模型的沟通基础，以及逐步求精的依据 概念模型交付品通常具备如下特点: 与客户一致的商业语言 尽量一页纸描述清楚整个模型 通常用实体关系型图表示，但不需添加实体的属性 允许多对多的关系存在 逻辑模型逻辑建模小提示1 应更精确估算出整个项目需要的时间以及项目计划草案 并且根据计划更精确地估算出项目的费用 当实体数量超过100时，需要定义术语表 规范化 先规范化再逆规范化，不可一步到位 不可缺少约束的定义 使用CASE工具做逻辑模型 多对多关系需要解决 需要同级评审(Peer Review) 确定可信赖数据源，关键属性需用真实数据验证 应用成熟的建模模式(Pattern) 一定程度的抽象化，决定了未来模型的弹性 高质量的模型定义 重要关联关系需要强制建立 与概念模型保持一致 注意模型的版本管理 非常非常注意细节 数据库专家深度介入 占据整个数据建模80%以上时间 不要忽视属性的长度定义和约束定义 不要忽视属性的默认值(Default Value) 使用控制数据范围的域(Domain) 逻辑建模交付品的特点 要像一本书，而非一页纸 所有实体属性均需添加 实体间关系要清晰描述 使用术语表 遵循命名规范 采用CASE工具创建项目文件 对各个实体必须有清晰描述 对关键属性必须有清晰描述 物理模型物理建模小贴士1 使用CASE工具由逻辑模型自动生成 应用术语表自动转换生成字段名称 对表空间、索引、视图、物化视图、主键、外键等都有命名规则 逆规范化在逻辑层完成，而非本层 数据库DBA深度介入，需要DBA的评审(Peer Review) 和数据库的DDL保持一致 注意版本管理 注意开发、测试、生产三个不同版本的模型管理 注意性能 估算数据规模 考虑数据归档 充分考虑未来使用数据库的优点和缺点 物理建模交付品的特点 自动生成基础库表结构，之后适度手动调整 与未来要使用的数据库类型息息相关 生成数据字典并发布 可直接用于生成DDL DDL中注意注释的生成 如何进行高质量数据建模什么样的模型算是高质量数据模型? 对真实世界的抽象正确而完整 用建模语言表达清晰而准确 框架稳定且灵活，满足当下的需求并能够一定程度容纳未来的变化 根据需求尽可能减少数据冗余 充分考虑潜在的性能问题 从企业全局的视角出发构筑模型","categories":[{"name":"数据仓库","slug":"数据仓库","permalink":"https://shang.at/categories/数据仓库/"}],"tags":[]},{"title":"数据仓库学习笔记二-重要意义","slug":"数据仓库学习笔记二-重要意义","date":"2019-03-03T10:55:58.000Z","updated":"2020-12-11T08:14:13.139Z","comments":true,"path":"post/数据仓库学习笔记二-重要意义/","link":"","permalink":"https://shang.at/post/数据仓库学习笔记二-重要意义/","excerpt":"","text":"数据时代的演化DIKWdata数据 + information信息 + knowledge知识 + wisdom智慧 描述数据的数据被称为元数据(metadata) 信息（information）= 元数据（metadata）+数据（data） 什么是数据模型数据模型实际上就是为了装载数据，用元数据搭建起来的框子 数据模型是将数据元素以标准化的模式组织起来, 用来模拟现实世界的信息框架蓝图。 数据模型的要求: 直观地模拟世界 容易为人所理解 便于计算机实现 数据模型是整个数据应用的基石，牵一发而动全身。数据模型的小小改动将会导致上层数据应用的大幅度变化 建设高质量数据模型的意义低质量数据模型的十宗罪 没有准确的捕获到需求 数据模型不完整 各层模型与其扮演角色不匹配 数据结构不合理 抽象化不够，造成模型不灵活 没有或者不遵循命名规范 缺少数据模型的定义和描述 数据模型可读性差 元数据与数据不匹配 数据模型与企业标准不一致 低质量数据模型的影响 大量修改和重做 重复建设 知识丢失 下游开发困难 高成本 数据质量低下 新业务无法展开 意义","categories":[{"name":"数据仓库","slug":"数据仓库","permalink":"https://shang.at/categories/数据仓库/"}],"tags":[]},{"title":"数据仓库学习笔记一","slug":"数据仓库学习笔记一","date":"2019-03-03T10:24:23.000Z","updated":"2020-12-11T08:14:35.221Z","comments":true,"path":"post/数据仓库学习笔记一/","link":"","permalink":"https://shang.at/post/数据仓库学习笔记一/","excerpt":"","text":"学习路线：","categories":[{"name":"数据仓库","slug":"数据仓库","permalink":"https://shang.at/categories/数据仓库/"}],"tags":[]},{"title":"数据结构学习笔记二-复杂度分析","slug":"数据结构学习笔记二-复杂度分析","date":"2019-03-03T10:22:47.000Z","updated":"2020-04-10T00:41:06.625Z","comments":true,"path":"post/数据结构学习笔记二-复杂度分析/","link":"","permalink":"https://shang.at/post/数据结构学习笔记二-复杂度分析/","excerpt":"","text":"时间复杂度 通常使用大O时间复杂度表示法：它表示了代码执行时间随数据规模增长的变化趋势， 亦可称为 渐进式时间复杂度 几种常见时间复杂度实例分析 各种时间复杂度执行时间比较 故可以看出来O(logn)的算法要比O(n)的算法效率上快上不少 常见排序算法的时间复杂度 空间复杂度 亦可称为 渐进空间复杂度，表示算法的存储空间与数据规模之间的增长关系 空间复杂度基本就只有O(1)、O(n)、O(n^2) 时间复杂度进阶实际上我们前面说的大O表示法只是一个初级的判断方式，此外还有最好情况时间复杂度、最坏情况时间复杂度、平均时间复杂度、均摊时间复杂度。 在大部分的时间大O表示法已经能够分辨出各种算法的时间复杂度了。只有在特殊情况下（同一块代码(或分析出的相同的大O等级)在不同的情况下，时间复杂度有量级的差距），才需要使用最好、最坏、平均三种方法来区分。均摊的情况就更加特殊了，比如向数组加入一个元素，当数组长度不足时，扩充数组再添加元素。","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"}],"tags":[{"name":"复杂度分析","slug":"复杂度分析","permalink":"https://shang.at/tags/复杂度分析/"}]},{"title":"数据结构学习笔记一","slug":"数据结构学习笔记一","date":"2019-03-03T09:34:30.000Z","updated":"2020-07-01T09:21:51.746Z","comments":true,"path":"post/数据结构学习笔记一/","link":"","permalink":"https://shang.at/post/数据结构学习笔记一/","excerpt":"","text":"基础知识数组为什么从0开始编号 数组的随机访问的寻址公式： a[i]_address = base_address + i * data_type_size 常见数据类型的占用内存大小 类型 字节大小 位数 表示范围 最大存储量 byte 1byte 8bits -2^7~2^7-1 2^8-1=255 int 4bytes 4*8bits -2^{31}~2^{31}-1 short 2bytes 2*8bits -2^{15}~2^{15}-1 long 8bytes 8*8bits -2^{63}~2^{63}-1 float 4bytes 4*8bits double 8bytes 8*8bits char 2bytes 2*bits 二进制-十进制计算机内部的二进制表示法：原码、反码、补码 数组链表栈队列跳表散列表关键点 散列思想 散列表是利用了数组可以根据下标随机访问的特性，而产生的一种高性能的数据结构(查找的时间复杂度为O(1))。散列表其实就是数组的一种拓展，是由数组演化而来。 其关键的概念有 键-key、 将键映射到数组下标的方法-散列函数(哈希函数)、 散列函数计算来的值-散列值(哈希值) 散列函数 散列冲突 开放寻址 链表法 进阶：链表使用红黑树或者跳表实现 具体实现|打造一个工业级散列表 散列函数 装载因子 散列冲突解决方法 二叉树红黑树递归树堆图Trie数AC自动机位图布隆过滤器B+数索引","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"}],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://shang.at/tags/数据结构/"}]},{"title":"Python数据科学学习笔记一","slug":"Python数据科学学习笔记一","date":"2019-03-03T09:19:14.000Z","updated":"2019-08-03T11:51:48.061Z","comments":true,"path":"post/Python数据科学学习笔记一/","link":"","permalink":"https://shang.at/post/Python数据科学学习笔记一/","excerpt":"","text":"描述性统计和探索型数据分析变量类型 名义变量 等级变量 连续变量 描述名义变量的分布两个统计量 频数 百分比 可视化 柱状图","categories":[{"name":"Python数据分析","slug":"Python数据分析","permalink":"https://shang.at/categories/Python数据分析/"}],"tags":[{"name":"描述性统计和探索型数据分析","slug":"描述性统计和探索型数据分析","permalink":"https://shang.at/tags/描述性统计和探索型数据分析/"}]},{"title":"记一次线上JVM问题调试","slug":"记一次线上JVM问题调试","date":"2019-02-27T03:21:32.000Z","updated":"2020-06-03T03:02:27.348Z","comments":true,"path":"post/记一次线上JVM问题调试/","link":"","permalink":"https://shang.at/post/记一次线上JVM问题调试/","excerpt":"发现java进程 使用linux命令：ps 1234ps -ef | grep prcessName[root@YZSJHL81-35 ~]# ps -ef | grep Bootstraproot 118271 1 99 00:10 ? 3-16:08:07 Bootstrap startroot 197682 197016 0 17:08 pts/0 00:00:00 grep Bootstrap 使用java自带的检测工具 jps 123[root@YZSJHL81-35 ~]# jps118271 Bootstrap197101 Jps","text":"发现java进程 使用linux命令：ps 1234ps -ef | grep prcessName[root@YZSJHL81-35 ~]# ps -ef | grep Bootstraproot 118271 1 99 00:10 ? 3-16:08:07 Bootstrap startroot 197682 197016 0 17:08 pts/0 00:00:00 grep Bootstrap 使用java自带的检测工具 jps 123[root@YZSJHL81-35 ~]# jps118271 Bootstrap197101 Jps 检测java进程启动的时间 12ps -p 118271 -o etimeps -p 118271 -o etime= 检测进程打开的文件数 1lsof -n -p 118271 | wc -l 检测系统的进程 top 12345678910top - 17:11:21 up 132 days, 2:14, 1 user, load average: 0.12, 0.21, 0.18Tasks: 298 total, 1 running, 297 sleeping, 0 stopped, 0 zombieCpu(s): 1.0%us, 0.6%sy, 0.0%ni, 98.0%id, 0.3%wa, 0.0%hi, 0.1%si, 0.0%stMem: 99009860k total, 90554384k used, 8455476k free, 333672k buffersSwap: 0k total, 0k used, 0k free, 81820024k cachedPID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMANDq-退出1-显示所有的cpu及其状态 查看指定进程下所有线程的状态 1top -Hp pid 查看JVM的GC状态1jstat -gcutil -t pid 1000 100 ---查看pid进程的GC状态每隔1000ms一次，一共100次 ​ 查看JVM栈信息1234printf '%x\\n' tid -- 打印指定线程id的十六进制表示jstack pid | grep 'tid十六进制' -C20 -color -- 查看进程的栈信息jmap -histo:live pid --查看进程所有存活的实例jmap -dumap:format=b,file=dump.hprof pid --导出JVM的dump文件","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"JVM问题调试","slug":"JVM问题调试","permalink":"https://shang.at/tags/JVM问题调试/"}]}],"categories":[{"name":"Python数据分析","slug":"Python数据分析","permalink":"https://shang.at/categories/Python数据分析/"},{"name":"机器学习","slug":"机器学习","permalink":"https://shang.at/categories/机器学习/"},{"name":"Spark学习","slug":"Spark学习","permalink":"https://shang.at/categories/Spark学习/"},{"name":"操作系统","slug":"操作系统","permalink":"https://shang.at/categories/操作系统/"},{"name":"大数据","slug":"大数据","permalink":"https://shang.at/categories/大数据/"},{"name":"Mysql","slug":"Mysql","permalink":"https://shang.at/categories/Mysql/"},{"name":"正则","slug":"正则","permalink":"https://shang.at/categories/正则/"},{"name":"python-lsi","slug":"python-lsi","permalink":"https://shang.at/categories/python-lsi/"},{"name":"jupyter","slug":"jupyter","permalink":"https://shang.at/categories/jupyter/"},{"name":"idea","slug":"idea","permalink":"https://shang.at/categories/idea/"},{"name":"池化","slug":"池化","permalink":"https://shang.at/categories/池化/"},{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"},{"name":"工具使用","slug":"工具使用","permalink":"https://shang.at/categories/工具使用/"},{"name":"Linux","slug":"Linux","permalink":"https://shang.at/categories/Linux/"},{"name":"IO","slug":"IO","permalink":"https://shang.at/categories/IO/"},{"name":"Scala学习","slug":"Scala学习","permalink":"https://shang.at/categories/Scala学习/"},{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"},{"name":"Python","slug":"Python","permalink":"https://shang.at/categories/Python/"},{"name":"容器","slug":"容器","permalink":"https://shang.at/categories/容器/"},{"name":"Spark应用","slug":"Spark应用","permalink":"https://shang.at/categories/Spark应用/"},{"name":"Hadoop学习","slug":"Hadoop学习","permalink":"https://shang.at/categories/Hadoop学习/"},{"name":"Hive学习","slug":"Hive学习","permalink":"https://shang.at/categories/Hive学习/"},{"name":"数据库","slug":"数据库","permalink":"https://shang.at/categories/数据库/"},{"name":"Shell编程","slug":"Shell编程","permalink":"https://shang.at/categories/Shell编程/"},{"name":"大数据生态","slug":"大数据生态","permalink":"https://shang.at/categories/大数据生态/"},{"name":"虚拟机","slug":"虚拟机","permalink":"https://shang.at/categories/虚拟机/"},{"name":"JAVA并发编程","slug":"JAVA并发编程","permalink":"https://shang.at/categories/JAVA并发编程/"},{"name":"分布式","slug":"分布式","permalink":"https://shang.at/categories/分布式/"},{"name":"BI","slug":"BI","permalink":"https://shang.at/categories/BI/"},{"name":"数据分析","slug":"数据分析","permalink":"https://shang.at/categories/数据分析/"},{"name":"Spark","slug":"Spark","permalink":"https://shang.at/categories/Spark/"},{"name":"数据仓库","slug":"数据仓库","permalink":"https://shang.at/categories/数据仓库/"}],"tags":[{"name":"Matplotlib","slug":"Matplotlib","permalink":"https://shang.at/tags/Matplotlib/"},{"name":"Numpy","slug":"Numpy","permalink":"https://shang.at/tags/Numpy/"},{"name":"机器学习","slug":"机器学习","permalink":"https://shang.at/tags/机器学习/"},{"name":"json数据操作","slug":"json数据操作","permalink":"https://shang.at/tags/json数据操作/"},{"name":"操作系统","slug":"操作系统","permalink":"https://shang.at/tags/操作系统/"},{"name":"zookeeper","slug":"zookeeper","permalink":"https://shang.at/tags/zookeeper/"},{"name":"mysql优化","slug":"mysql优化","permalink":"https://shang.at/tags/mysql优化/"},{"name":"索引","slug":"索引","permalink":"https://shang.at/tags/索引/"},{"name":"正则","slug":"正则","permalink":"https://shang.at/tags/正则/"},{"name":"python-lsi","slug":"python-lsi","permalink":"https://shang.at/tags/python-lsi/"},{"name":"flink","slug":"flink","permalink":"https://shang.at/tags/flink/"},{"name":"工具使用","slug":"工具使用","permalink":"https://shang.at/tags/工具使用/"},{"name":"工具","slug":"工具","permalink":"https://shang.at/tags/工具/"},{"name":"程序设计","slug":"程序设计","permalink":"https://shang.at/tags/程序设计/"},{"name":"对象内存布局","slug":"对象内存布局","permalink":"https://shang.at/tags/对象内存布局/"},{"name":"gitlab","slug":"gitlab","permalink":"https://shang.at/tags/gitlab/"},{"name":"Linux","slug":"Linux","permalink":"https://shang.at/tags/Linux/"},{"name":"Class的加载过程","slug":"Class的加载过程","permalink":"https://shang.at/tags/Class的加载过程/"},{"name":"IO","slug":"IO","permalink":"https://shang.at/tags/IO/"},{"name":"JAVA-IO","slug":"JAVA-IO","permalink":"https://shang.at/tags/JAVA-IO/"},{"name":"Epoll","slug":"Epoll","permalink":"https://shang.at/tags/Epoll/"},{"name":"Scala泛型","slug":"Scala泛型","permalink":"https://shang.at/tags/Scala泛型/"},{"name":"Scala-import","slug":"Scala-import","permalink":"https://shang.at/tags/Scala-import/"},{"name":"Encoder","slug":"Encoder","permalink":"https://shang.at/tags/Encoder/"},{"name":"String","slug":"String","permalink":"https://shang.at/tags/String/"},{"name":"函数式编程","slug":"函数式编程","permalink":"https://shang.at/tags/函数式编程/"},{"name":"Scala编程Tips","slug":"Scala编程Tips","permalink":"https://shang.at/tags/Scala编程Tips/"},{"name":"Scala-Iterator设计模式","slug":"Scala-Iterator设计模式","permalink":"https://shang.at/tags/Scala-Iterator设计模式/"},{"name":"Scala隐式转换","slug":"Scala隐式转换","permalink":"https://shang.at/tags/Scala隐式转换/"},{"name":"Scala偏函数","slug":"Scala偏函数","permalink":"https://shang.at/tags/Scala偏函数/"},{"name":"Scala模式匹配","slug":"Scala模式匹配","permalink":"https://shang.at/tags/Scala模式匹配/"},{"name":"Scala案例类","slug":"Scala案例类","permalink":"https://shang.at/tags/Scala案例类/"},{"name":"Scala特质和抽象类型","slug":"Scala特质和抽象类型","permalink":"https://shang.at/tags/Scala特质和抽象类型/"},{"name":"Scala集合","slug":"Scala集合","permalink":"https://shang.at/tags/Scala集合/"},{"name":"Scala方法","slug":"Scala方法","permalink":"https://shang.at/tags/Scala方法/"},{"name":"Scala-for-yield","slug":"Scala-for-yield","permalink":"https://shang.at/tags/Scala-for-yield/"},{"name":"Scala类型","slug":"Scala类型","permalink":"https://shang.at/tags/Scala类型/"},{"name":"maven","slug":"maven","permalink":"https://shang.at/tags/maven/"},{"name":"左神学习笔记","slug":"左神学习笔记","permalink":"https://shang.at/tags/左神学习笔记/"},{"name":"aws","slug":"aws","permalink":"https://shang.at/tags/aws/"},{"name":"文件操作","slug":"文件操作","permalink":"https://shang.at/tags/文件操作/"},{"name":"网络的配置","slug":"网络的配置","permalink":"https://shang.at/tags/网络的配置/"},{"name":"UML","slug":"UML","permalink":"https://shang.at/tags/UML/"},{"name":"设计模式","slug":"设计模式","permalink":"https://shang.at/tags/设计模式/"},{"name":"Docker","slug":"Docker","permalink":"https://shang.at/tags/Docker/"},{"name":"Centos开发环境","slug":"Centos开发环境","permalink":"https://shang.at/tags/Centos开发环境/"},{"name":"centos7时区设置","slug":"centos7时区设置","permalink":"https://shang.at/tags/centos7时区设置/"},{"name":"ssh","slug":"ssh","permalink":"https://shang.at/tags/ssh/"},{"name":"deque","slug":"deque","permalink":"https://shang.at/tags/deque/"},{"name":"Python标准库","slug":"Python标准库","permalink":"https://shang.at/tags/Python标准库/"},{"name":"itertools","slug":"itertools","permalink":"https://shang.at/tags/itertools/"},{"name":"Spark大数据集防止driver OOM","slug":"Spark大数据集防止driver-OOM","permalink":"https://shang.at/tags/Spark大数据集防止driver-OOM/"},{"name":"Python类的特殊方法","slug":"Python类的特殊方法","permalink":"https://shang.at/tags/Python类的特殊方法/"},{"name":"JAVA-JMH","slug":"JAVA-JMH","permalink":"https://shang.at/tags/JAVA-JMH/"},{"name":"动态规划","slug":"动态规划","permalink":"https://shang.at/tags/动态规划/"},{"name":"python的dict","slug":"python的dict","permalink":"https://shang.at/tags/python的dict/"},{"name":"Hadoop","slug":"Hadoop","permalink":"https://shang.at/tags/Hadoop/"},{"name":"Yarn-Scheduler","slug":"Yarn-Scheduler","permalink":"https://shang.at/tags/Yarn-Scheduler/"},{"name":"Yarn-Container","slug":"Yarn-Container","permalink":"https://shang.at/tags/Yarn-Container/"},{"name":"实现一个在Yarn上的Application","slug":"实现一个在Yarn上的Application","permalink":"https://shang.at/tags/实现一个在Yarn上的Application/"},{"name":"Mapreduce编程模式","slug":"Mapreduce编程模式","permalink":"https://shang.at/tags/Mapreduce编程模式/"},{"name":"Hive安装","slug":"Hive安装","permalink":"https://shang.at/tags/Hive安装/"},{"name":"mysql环境配置","slug":"mysql环境配置","permalink":"https://shang.at/tags/mysql环境配置/"},{"name":"Hadoop-Shell","slug":"Hadoop-Shell","permalink":"https://shang.at/tags/Hadoop-Shell/"},{"name":"iterm2","slug":"iterm2","permalink":"https://shang.at/tags/iterm2/"},{"name":"Hadoop源码编译","slug":"Hadoop源码编译","permalink":"https://shang.at/tags/Hadoop源码编译/"},{"name":"Hadoop集群搭建","slug":"Hadoop集群搭建","permalink":"https://shang.at/tags/Hadoop集群搭建/"},{"name":"Hadoop配置详解","slug":"Hadoop配置详解","permalink":"https://shang.at/tags/Hadoop配置详解/"},{"name":"常见命令","slug":"常见命令","permalink":"https://shang.at/tags/常见命令/"},{"name":"centos7修改hostname","slug":"centos7修改hostname","permalink":"https://shang.at/tags/centos7修改hostname/"},{"name":"vim","slug":"vim","permalink":"https://shang.at/tags/vim/"},{"name":"大数据端口","slug":"大数据端口","permalink":"https://shang.at/tags/大数据端口/"},{"name":"大数据环境","slug":"大数据环境","permalink":"https://shang.at/tags/大数据环境/"},{"name":"Vagrant","slug":"Vagrant","permalink":"https://shang.at/tags/Vagrant/"},{"name":"Kafka","slug":"Kafka","permalink":"https://shang.at/tags/Kafka/"},{"name":"大数据生态","slug":"大数据生态","permalink":"https://shang.at/tags/大数据生态/"},{"name":"java底层","slug":"java底层","permalink":"https://shang.at/tags/java底层/"},{"name":"class文件格式","slug":"class文件格式","permalink":"https://shang.at/tags/class文件格式/"},{"name":"运行时线程","slug":"运行时线程","permalink":"https://shang.at/tags/运行时线程/"},{"name":"JOIN的算法实现","slug":"JOIN的算法实现","permalink":"https://shang.at/tags/JOIN的算法实现/"},{"name":"JDK环境切换","slug":"JDK环境切换","permalink":"https://shang.at/tags/JDK环境切换/"},{"name":"JVM虚拟机栈","slug":"JVM虚拟机栈","permalink":"https://shang.at/tags/JVM虚拟机栈/"},{"name":"JVM疑问","slug":"JVM疑问","permalink":"https://shang.at/tags/JVM疑问/"},{"name":"基本类型和包装类型","slug":"基本类型和包装类型","permalink":"https://shang.at/tags/基本类型和包装类型/"},{"name":"网络编程-零拷贝","slug":"网络编程-零拷贝","permalink":"https://shang.at/tags/网络编程-零拷贝/"},{"name":"JVM参数","slug":"JVM参数","permalink":"https://shang.at/tags/JVM参数/"},{"name":"常见异常","slug":"常见异常","permalink":"https://shang.at/tags/常见异常/"},{"name":"Disruptor","slug":"Disruptor","permalink":"https://shang.at/tags/Disruptor/"},{"name":"RxJava","slug":"RxJava","permalink":"https://shang.at/tags/RxJava/"},{"name":"协程","slug":"协程","permalink":"https://shang.at/tags/协程/"},{"name":"线程池","slug":"线程池","permalink":"https://shang.at/tags/线程池/"},{"name":"阻塞队列","slug":"阻塞队列","permalink":"https://shang.at/tags/阻塞队列/"},{"name":"Atomic","slug":"Atomic","permalink":"https://shang.at/tags/Atomic/"},{"name":"并发集合","slug":"并发集合","permalink":"https://shang.at/tags/并发集合/"},{"name":"ThreadLocal-Fork&Join","slug":"ThreadLocal-Fork-Join","permalink":"https://shang.at/tags/ThreadLocal-Fork-Join/"},{"name":"并发工具类","slug":"并发工具类","permalink":"https://shang.at/tags/并发工具类/"},{"name":"锁","slug":"锁","permalink":"https://shang.at/tags/锁/"},{"name":"并发基础","slug":"并发基础","permalink":"https://shang.at/tags/并发基础/"},{"name":"内存模型JMM","slug":"内存模型JMM","permalink":"https://shang.at/tags/内存模型JMM/"},{"name":"Scheduler","slug":"Scheduler","permalink":"https://shang.at/tags/Scheduler/"},{"name":"BloomFilter","slug":"BloomFilter","permalink":"https://shang.at/tags/BloomFilter/"},{"name":"外排-分组归并-桶排序","slug":"外排-分组归并-桶排序","permalink":"https://shang.at/tags/外排-分组归并-桶排序/"},{"name":"并发编程-Future","slug":"并发编程-Future","permalink":"https://shang.at/tags/并发编程-Future/"},{"name":"常见操作技巧","slug":"常见操作技巧","permalink":"https://shang.at/tags/常见操作技巧/"},{"name":"lru_cache","slug":"lru-cache","permalink":"https://shang.at/tags/lru-cache/"},{"name":"并发编程-Thread","slug":"并发编程-Thread","permalink":"https://shang.at/tags/并发编程-Thread/"},{"name":"并发编程","slug":"并发编程","permalink":"https://shang.at/tags/并发编程/"},{"name":"JVM","slug":"JVM","permalink":"https://shang.at/tags/JVM/"},{"name":"wait&notify","slug":"wait-notify","permalink":"https://shang.at/tags/wait-notify/"},{"name":"Timer","slug":"Timer","permalink":"https://shang.at/tags/Timer/"},{"name":"NIO","slug":"NIO","permalink":"https://shang.at/tags/NIO/"},{"name":"JAVA-Iterator","slug":"JAVA-Iterator","permalink":"https://shang.at/tags/JAVA-Iterator/"},{"name":"JAVA集合类-列表","slug":"JAVA集合类-列表","permalink":"https://shang.at/tags/JAVA集合类-列表/"},{"name":"IPC&RPC","slug":"IPC-RPC","permalink":"https://shang.at/tags/IPC-RPC/"},{"name":"动态代理","slug":"动态代理","permalink":"https://shang.at/tags/动态代理/"},{"name":"mysql事务和隔离级别","slug":"mysql事务和隔离级别","permalink":"https://shang.at/tags/mysql事务和隔离级别/"},{"name":"mysql第三范式","slug":"mysql第三范式","permalink":"https://shang.at/tags/mysql第三范式/"},{"name":"python学习","slug":"python学习","permalink":"https://shang.at/tags/python学习/"},{"name":"OrderedDict","slug":"OrderedDict","permalink":"https://shang.at/tags/OrderedDict/"},{"name":"bisect","slug":"bisect","permalink":"https://shang.at/tags/bisect/"},{"name":"查找算法","slug":"查找算法","permalink":"https://shang.at/tags/查找算法/"},{"name":"Spark中的隐式转换","slug":"Spark中的隐式转换","permalink":"https://shang.at/tags/Spark中的隐式转换/"},{"name":"Hadoop-IPC","slug":"Hadoop-IPC","permalink":"https://shang.at/tags/Hadoop-IPC/"},{"name":"Pandas","slug":"Pandas","permalink":"https://shang.at/tags/Pandas/"},{"name":"python中的时间处理","slug":"python中的时间处理","permalink":"https://shang.at/tags/python中的时间处理/"},{"name":"Configuration","slug":"Configuration","permalink":"https://shang.at/tags/Configuration/"},{"name":"队列","slug":"队列","permalink":"https://shang.at/tags/队列/"},{"name":"广播变量","slug":"广播变量","permalink":"https://shang.at/tags/广播变量/"},{"name":"算法","slug":"算法","permalink":"https://shang.at/tags/算法/"},{"name":"pivot透视图","slug":"pivot透视图","permalink":"https://shang.at/tags/pivot透视图/"},{"name":"Spark-tips","slug":"Spark-tips","permalink":"https://shang.at/tags/Spark-tips/"},{"name":"Tableau","slug":"Tableau","permalink":"https://shang.at/tags/Tableau/"},{"name":"sparkSql-DSL语法","slug":"sparkSql-DSL语法","permalink":"https://shang.at/tags/sparkSql-DSL语法/"},{"name":"排序算法","slug":"排序算法","permalink":"https://shang.at/tags/排序算法/"},{"name":"reduce","slug":"reduce","permalink":"https://shang.at/tags/reduce/"},{"name":"数据分析技巧","slug":"数据分析技巧","permalink":"https://shang.at/tags/数据分析技巧/"},{"name":"有初始值的聚合操作","slug":"有初始值的聚合操作","permalink":"https://shang.at/tags/有初始值的聚合操作/"},{"name":"分析指标","slug":"分析指标","permalink":"https://shang.at/tags/分析指标/"},{"name":"抽样方法和自增ID","slug":"抽样方法和自增ID","permalink":"https://shang.at/tags/抽样方法和自增ID/"},{"name":"sparkSql内置函数","slug":"sparkSql内置函数","permalink":"https://shang.at/tags/sparkSql内置函数/"},{"name":"数据分析Tips","slug":"数据分析Tips","permalink":"https://shang.at/tags/数据分析Tips/"},{"name":"窗口函数","slug":"窗口函数","permalink":"https://shang.at/tags/窗口函数/"},{"name":"union","slug":"union","permalink":"https://shang.at/tags/union/"},{"name":"pyspark官方文档学习","slug":"pyspark官方文档学习","permalink":"https://shang.at/tags/pyspark官方文档学习/"},{"name":"复杂度分析","slug":"复杂度分析","permalink":"https://shang.at/tags/复杂度分析/"},{"name":"数据结构","slug":"数据结构","permalink":"https://shang.at/tags/数据结构/"},{"name":"描述性统计和探索型数据分析","slug":"描述性统计和探索型数据分析","permalink":"https://shang.at/tags/描述性统计和探索型数据分析/"},{"name":"JVM问题调试","slug":"JVM问题调试","permalink":"https://shang.at/tags/JVM问题调试/"}]}